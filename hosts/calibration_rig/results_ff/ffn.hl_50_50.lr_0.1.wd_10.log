[2017-12-15 15:20:50] Experiment ffn.hl_50_50.lr_0.1.wd_10 logging started.
[2017-12-15 15:20:50] 
                       *** Starting Experiment ffn.hl_50_50.lr_0.1.wd_10 ***
                      
[2017-12-15 15:20:50] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 15:20:50] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 1)
                      )
[2017-12-15 15:20:50]  *** Training on GPU ***
[2017-12-15 15:20:58] Epoch 0001 mean train/dev loss: 4080.7964 / 157.6573
[2017-12-15 15:21:07] Epoch 0002 mean train/dev loss: 165.1628 / 1188.8683
[2017-12-15 15:21:15] Epoch 0003 mean train/dev loss: 164.1118 / 159.2125
[2017-12-15 15:21:23] Epoch 0004 mean train/dev loss: 160.4672 / 130.7722
[2017-12-15 15:21:31] Epoch 0005 mean train/dev loss: 140.3949 / 234.7164
[2017-12-15 15:21:39] Epoch 0006 mean train/dev loss: 136.3860 / 148.4283
[2017-12-15 15:21:47] Epoch 0007 mean train/dev loss: 143.5208 / 148.3524
[2017-12-15 15:21:56] Epoch 0008 mean train/dev loss: 139.1278 / 135.6217
[2017-12-15 15:22:04] Epoch 0009 mean train/dev loss: 143.1842 / 145.9033
[2017-12-15 15:22:12] Epoch 0010 mean train/dev loss: 146.4906 / 201.8663
[2017-12-15 15:22:12] Checkpointing model at epoch 10 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:22:12] Model Checkpointing finished.
[2017-12-15 15:22:21] Epoch 0011 mean train/dev loss: 134.0958 / 203.8215
[2017-12-15 15:22:29] Epoch 0012 mean train/dev loss: 134.1840 / 171.4425
[2017-12-15 15:22:36] Epoch 0013 mean train/dev loss: 139.1910 / 131.7220
[2017-12-15 15:22:44] Epoch 0014 mean train/dev loss: 129.6724 / 115.6974
[2017-12-15 15:22:53] Epoch 0015 mean train/dev loss: 147.5816 / 126.6677
[2017-12-15 15:22:53] Learning rate decayed by 0.5000
[2017-12-15 15:23:01] Epoch 0016 mean train/dev loss: 118.7162 / 122.7358
[2017-12-15 15:23:08] Epoch 0017 mean train/dev loss: 122.9565 / 148.5213
[2017-12-15 15:23:16] Epoch 0018 mean train/dev loss: 121.5315 / 141.7191
[2017-12-15 15:23:24] Epoch 0019 mean train/dev loss: 122.3664 / 117.5459
[2017-12-15 15:23:32] Epoch 0020 mean train/dev loss: 122.4472 / 114.6745
[2017-12-15 15:23:32] Checkpointing model at epoch 20 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:23:32] Model Checkpointing finished.
[2017-12-15 15:23:40] Epoch 0021 mean train/dev loss: 124.0308 / 158.4369
[2017-12-15 15:23:48] Epoch 0022 mean train/dev loss: 122.5900 / 142.4521
[2017-12-15 15:23:56] Epoch 0023 mean train/dev loss: 125.2163 / 151.2047
[2017-12-15 15:24:04] Epoch 0024 mean train/dev loss: 122.3920 / 119.5720
[2017-12-15 15:24:12] Epoch 0025 mean train/dev loss: 122.4421 / 121.6660
[2017-12-15 15:24:20] Epoch 0026 mean train/dev loss: 123.2034 / 144.3933
[2017-12-15 15:24:28] Epoch 0027 mean train/dev loss: 122.1033 / 130.3695
[2017-12-15 15:24:35] Epoch 0028 mean train/dev loss: 122.2744 / 140.9386
[2017-12-15 15:24:43] Epoch 0029 mean train/dev loss: 122.0651 / 123.4784
[2017-12-15 15:24:51] Epoch 0030 mean train/dev loss: 122.6963 / 117.1348
[2017-12-15 15:24:51] Learning rate decayed by 0.5000
[2017-12-15 15:24:51] Checkpointing model at epoch 30 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:24:51] Model Checkpointing finished.
[2017-12-15 15:24:59] Epoch 0031 mean train/dev loss: 118.0004 / 114.4132
[2017-12-15 15:25:07] Epoch 0032 mean train/dev loss: 117.8858 / 115.5462
[2017-12-15 15:25:15] Epoch 0033 mean train/dev loss: 118.0674 / 122.4268
[2017-12-15 15:25:23] Epoch 0034 mean train/dev loss: 117.7152 / 120.1360
[2017-12-15 15:25:31] Epoch 0035 mean train/dev loss: 118.3608 / 116.6047
[2017-12-15 15:25:39] Epoch 0036 mean train/dev loss: 118.3190 / 120.0439
[2017-12-15 15:25:47] Epoch 0037 mean train/dev loss: 118.2504 / 120.0030
[2017-12-15 15:25:55] Epoch 0038 mean train/dev loss: 118.3820 / 117.0139
[2017-12-15 15:26:03] Epoch 0039 mean train/dev loss: 118.7450 / 114.3865
[2017-12-15 15:26:11] Epoch 0040 mean train/dev loss: 118.0279 / 111.4926
[2017-12-15 15:26:11] Checkpointing model at epoch 40 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:26:11] Model Checkpointing finished.
[2017-12-15 15:26:19] Epoch 0041 mean train/dev loss: 118.3997 / 114.0755
[2017-12-15 15:26:27] Epoch 0042 mean train/dev loss: 118.5277 / 121.5667
[2017-12-15 15:26:35] Epoch 0043 mean train/dev loss: 118.2253 / 111.1784
[2017-12-15 15:26:43] Epoch 0044 mean train/dev loss: 117.7214 / 122.8424
[2017-12-15 15:26:51] Epoch 0045 mean train/dev loss: 117.8754 / 113.5324
[2017-12-15 15:26:51] Learning rate decayed by 0.5000
[2017-12-15 15:26:59] Epoch 0046 mean train/dev loss: 115.7715 / 117.6291
[2017-12-15 15:27:06] Epoch 0047 mean train/dev loss: 116.0823 / 113.3607
[2017-12-15 15:27:14] Epoch 0048 mean train/dev loss: 115.9977 / 119.6100
[2017-12-15 15:27:22] Epoch 0049 mean train/dev loss: 115.8626 / 111.2892
[2017-12-15 15:27:30] Epoch 0050 mean train/dev loss: 116.2145 / 110.7093
[2017-12-15 15:27:30] Checkpointing model at epoch 50 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:27:31] Model Checkpointing finished.
[2017-12-15 15:27:38] Epoch 0051 mean train/dev loss: 116.1421 / 115.1045
[2017-12-15 15:27:46] Epoch 0052 mean train/dev loss: 116.3059 / 117.7489
[2017-12-15 15:27:54] Epoch 0053 mean train/dev loss: 116.2865 / 110.8490
[2017-12-15 15:28:02] Epoch 0054 mean train/dev loss: 116.0847 / 112.6589
[2017-12-15 15:28:10] Epoch 0055 mean train/dev loss: 116.0610 / 118.7247
[2017-12-15 15:28:18] Epoch 0056 mean train/dev loss: 116.4279 / 111.9707
[2017-12-15 15:28:26] Epoch 0057 mean train/dev loss: 115.9550 / 114.6278
[2017-12-15 15:28:34] Epoch 0058 mean train/dev loss: 116.5407 / 118.7883
[2017-12-15 15:28:42] Epoch 0059 mean train/dev loss: 116.0170 / 114.6189
[2017-12-15 15:28:50] Epoch 0060 mean train/dev loss: 116.5549 / 109.9986
[2017-12-15 15:28:50] Learning rate decayed by 0.5000
[2017-12-15 15:28:50] Checkpointing model at epoch 60 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:28:50] Model Checkpointing finished.
[2017-12-15 15:28:58] Epoch 0061 mean train/dev loss: 114.8477 / 110.1939
[2017-12-15 15:29:06] Epoch 0062 mean train/dev loss: 115.0539 / 116.7978
[2017-12-15 15:29:13] Epoch 0063 mean train/dev loss: 115.2367 / 114.1320
[2017-12-15 15:29:21] Epoch 0064 mean train/dev loss: 115.1985 / 111.4063
[2017-12-15 15:29:29] Epoch 0065 mean train/dev loss: 114.9785 / 112.4698
[2017-12-15 15:29:37] Epoch 0066 mean train/dev loss: 115.2223 / 115.1133
[2017-12-15 15:29:46] Epoch 0067 mean train/dev loss: 115.2778 / 113.3553
[2017-12-15 15:29:53] Epoch 0068 mean train/dev loss: 115.2379 / 109.5637
[2017-12-15 15:30:01] Epoch 0069 mean train/dev loss: 115.3212 / 109.9458
[2017-12-15 15:30:09] Epoch 0070 mean train/dev loss: 115.1480 / 111.3205
[2017-12-15 15:30:09] Checkpointing model at epoch 70 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:30:10] Model Checkpointing finished.
[2017-12-15 15:30:17] Epoch 0071 mean train/dev loss: 115.2194 / 110.5747
[2017-12-15 15:30:25] Epoch 0072 mean train/dev loss: 115.2301 / 115.0845
[2017-12-15 15:30:33] Epoch 0073 mean train/dev loss: 115.0764 / 111.6777
[2017-12-15 15:30:41] Epoch 0074 mean train/dev loss: 115.1484 / 111.2676
[2017-12-15 15:30:49] Epoch 0075 mean train/dev loss: 115.1448 / 111.6227
[2017-12-15 15:30:49] Learning rate decayed by 0.5000
[2017-12-15 15:30:57] Epoch 0076 mean train/dev loss: 114.5105 / 112.0326
[2017-12-15 15:31:05] Epoch 0077 mean train/dev loss: 114.4563 / 110.6885
[2017-12-15 15:31:13] Epoch 0078 mean train/dev loss: 114.6240 / 111.1236
[2017-12-15 15:31:21] Epoch 0079 mean train/dev loss: 114.6420 / 111.1918
[2017-12-15 15:31:29] Epoch 0080 mean train/dev loss: 114.4943 / 113.5989
[2017-12-15 15:31:29] Checkpointing model at epoch 80 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:31:29] Model Checkpointing finished.
[2017-12-15 15:31:37] Epoch 0081 mean train/dev loss: 114.6082 / 112.1247
[2017-12-15 15:31:45] Epoch 0082 mean train/dev loss: 114.4837 / 110.6667
[2017-12-15 15:31:53] Epoch 0083 mean train/dev loss: 114.5388 / 115.1597
[2017-12-15 15:32:00] Epoch 0084 mean train/dev loss: 114.5397 / 112.6513
[2017-12-15 15:32:08] Epoch 0085 mean train/dev loss: 114.4709 / 111.5621
[2017-12-15 15:32:16] Epoch 0086 mean train/dev loss: 114.6089 / 111.1432
[2017-12-15 15:32:24] Epoch 0087 mean train/dev loss: 114.5817 / 112.1819
[2017-12-15 15:32:32] Epoch 0088 mean train/dev loss: 114.5689 / 112.6706
[2017-12-15 15:32:40] Epoch 0089 mean train/dev loss: 114.6045 / 111.3378
[2017-12-15 15:32:48] Epoch 0090 mean train/dev loss: 114.5012 / 109.5439
[2017-12-15 15:32:48] Learning rate decayed by 0.5000
[2017-12-15 15:32:48] Checkpointing model at epoch 90 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:32:48] Model Checkpointing finished.
[2017-12-15 15:32:56] Epoch 0091 mean train/dev loss: 114.2351 / 111.7603
[2017-12-15 15:33:04] Epoch 0092 mean train/dev loss: 114.1703 / 111.5170
[2017-12-15 15:33:12] Epoch 0093 mean train/dev loss: 114.2188 / 110.0795
[2017-12-15 15:33:19] Epoch 0094 mean train/dev loss: 114.1753 / 111.4470
[2017-12-15 15:33:26] Epoch 0095 mean train/dev loss: 114.2633 / 111.1527
[2017-12-15 15:33:32] Epoch 0096 mean train/dev loss: 114.2462 / 110.2673
[2017-12-15 15:33:39] Epoch 0097 mean train/dev loss: 114.2476 / 111.7384
[2017-12-15 15:33:46] Epoch 0098 mean train/dev loss: 114.1879 / 110.8812
[2017-12-15 15:33:52] Epoch 0099 mean train/dev loss: 114.2585 / 112.0161
[2017-12-15 15:33:59] Epoch 0100 mean train/dev loss: 114.1593 / 111.4158
[2017-12-15 15:33:59] Checkpointing model at epoch 100 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:33:59] Model Checkpointing finished.
[2017-12-15 15:34:06] Epoch 0101 mean train/dev loss: 114.2007 / 110.5064
[2017-12-15 15:34:13] Epoch 0102 mean train/dev loss: 114.2150 / 112.9265
[2017-12-15 15:34:20] Epoch 0103 mean train/dev loss: 114.2539 / 111.8256
[2017-12-15 15:34:27] Epoch 0104 mean train/dev loss: 114.2209 / 111.1761
[2017-12-15 15:34:34] Epoch 0105 mean train/dev loss: 114.2179 / 111.2853
[2017-12-15 15:34:34] Learning rate decayed by 0.5000
[2017-12-15 15:34:41] Epoch 0106 mean train/dev loss: 113.9823 / 110.5317
[2017-12-15 15:34:46] Epoch 0107 mean train/dev loss: 113.9926 / 111.2716
[2017-12-15 15:34:51] Epoch 0108 mean train/dev loss: 114.0409 / 110.9967
[2017-12-15 15:34:57] Epoch 0109 mean train/dev loss: 114.0372 / 110.9001
[2017-12-15 15:35:02] Epoch 0110 mean train/dev loss: 114.0324 / 111.4985
[2017-12-15 15:35:02] Checkpointing model at epoch 110 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:35:02] Model Checkpointing finished.
[2017-12-15 15:35:08] Epoch 0111 mean train/dev loss: 114.0358 / 110.7966
[2017-12-15 15:35:13] Epoch 0112 mean train/dev loss: 114.0377 / 110.6913
[2017-12-15 15:35:19] Epoch 0113 mean train/dev loss: 114.0293 / 111.6401
[2017-12-15 15:35:24] Epoch 0114 mean train/dev loss: 114.0373 / 110.8314
[2017-12-15 15:35:29] Epoch 0115 mean train/dev loss: 114.0572 / 110.5131
[2017-12-15 15:35:35] Epoch 0116 mean train/dev loss: 114.0236 / 111.5106
[2017-12-15 15:35:40] Epoch 0117 mean train/dev loss: 114.0340 / 111.9038
[2017-12-15 15:35:45] Epoch 0118 mean train/dev loss: 114.0531 / 110.8689
[2017-12-15 15:35:51] Epoch 0119 mean train/dev loss: 114.0246 / 111.3179
[2017-12-15 15:35:56] Epoch 0120 mean train/dev loss: 114.0696 / 111.5037
[2017-12-15 15:35:56] Learning rate decayed by 0.5000
[2017-12-15 15:35:56] Checkpointing model at epoch 120 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:35:56] Model Checkpointing finished.
[2017-12-15 15:36:02] Epoch 0121 mean train/dev loss: 113.8981 / 111.0900
[2017-12-15 15:36:07] Epoch 0122 mean train/dev loss: 113.9377 / 110.7407
[2017-12-15 15:36:12] Epoch 0123 mean train/dev loss: 113.9176 / 111.7432
[2017-12-15 15:36:18] Epoch 0124 mean train/dev loss: 113.9489 / 110.7932
[2017-12-15 15:36:23] Epoch 0125 mean train/dev loss: 113.9432 / 110.4829
[2017-12-15 15:36:29] Epoch 0126 mean train/dev loss: 113.9690 / 111.3335
[2017-12-15 15:36:34] Epoch 0127 mean train/dev loss: 113.9367 / 111.1240
[2017-12-15 15:36:39] Epoch 0128 mean train/dev loss: 113.8974 / 111.4044
[2017-12-15 15:36:45] Epoch 0129 mean train/dev loss: 113.9445 / 111.0715
[2017-12-15 15:36:50] Epoch 0130 mean train/dev loss: 113.9015 / 110.9160
[2017-12-15 15:36:50] Checkpointing model at epoch 130 for ffn.hl_50_50.lr_0.1.wd_10
[2017-12-15 15:36:50] Model Checkpointing finished.
[2017-12-15 15:36:55] Epoch 0131 mean train/dev loss: 113.9411 / 111.0441
[2017-12-15 15:36:55] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:36:55] 
                       *** Training finished *** 
[2017-12-15 15:36:56] Dev MSE: 111.0441
[2017-12-15 15:37:01] Training MSE: 113.6036
[2017-12-15 15:37:02] Experiment ffn.hl_50_50.lr_0.1.wd_10 logging ended.
