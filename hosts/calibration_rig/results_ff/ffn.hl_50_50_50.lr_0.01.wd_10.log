[2017-12-15 17:51:33] Experiment ffn.hl_50_50_50.lr_0.01.wd_10 logging started.
[2017-12-15 17:51:33] 
                       *** Starting Experiment ffn.hl_50_50_50.lr_0.01.wd_10 ***
                      
[2017-12-15 17:51:33] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 17:51:33] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 50)
                        (relu3): ReLU ()
                        (linear4): Linear (50 -> 1)
                      )
[2017-12-15 17:51:33]  *** Training on GPU ***
[2017-12-15 17:51:42] Epoch 0001 mean train/dev loss: 13393.2610 / 332.2502
[2017-12-15 17:51:51] Epoch 0002 mean train/dev loss: 166.7519 / 198.8318
[2017-12-15 17:52:00] Epoch 0003 mean train/dev loss: 129.1816 / 148.5892
[2017-12-15 17:52:09] Epoch 0004 mean train/dev loss: 121.0935 / 127.0375
[2017-12-15 17:52:17] Epoch 0005 mean train/dev loss: 119.8851 / 112.6599
[2017-12-15 17:52:26] Epoch 0006 mean train/dev loss: 120.6124 / 135.5408
[2017-12-15 17:52:35] Epoch 0007 mean train/dev loss: 118.3608 / 116.7804
[2017-12-15 17:52:43] Epoch 0008 mean train/dev loss: 121.1630 / 126.1707
[2017-12-15 17:52:52] Epoch 0009 mean train/dev loss: 120.2662 / 121.2943
[2017-12-15 17:53:01] Epoch 0010 mean train/dev loss: 118.6008 / 125.4805
[2017-12-15 17:53:01] Checkpointing model at epoch 10 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 17:53:01] Model Checkpointing finished.
[2017-12-15 17:53:09] Epoch 0011 mean train/dev loss: 119.5795 / 126.7130
[2017-12-15 17:53:18] Epoch 0012 mean train/dev loss: 121.0180 / 147.0420
[2017-12-15 17:53:26] Epoch 0013 mean train/dev loss: 121.3134 / 115.3156
[2017-12-15 17:53:35] Epoch 0014 mean train/dev loss: 119.3637 / 123.9177
[2017-12-15 17:53:43] Epoch 0015 mean train/dev loss: 119.7594 / 115.9380
[2017-12-15 17:53:43] Learning rate decayed by 0.5000
[2017-12-15 17:53:52] Epoch 0016 mean train/dev loss: 114.2892 / 110.9947
[2017-12-15 17:54:00] Epoch 0017 mean train/dev loss: 114.5121 / 129.0457
[2017-12-15 17:54:09] Epoch 0018 mean train/dev loss: 115.1066 / 109.2455
[2017-12-15 17:54:18] Epoch 0019 mean train/dev loss: 115.1728 / 116.0175
[2017-12-15 17:54:26] Epoch 0020 mean train/dev loss: 115.4313 / 109.9149
[2017-12-15 17:54:26] Checkpointing model at epoch 20 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 17:54:27] Model Checkpointing finished.
[2017-12-15 17:54:35] Epoch 0021 mean train/dev loss: 115.3406 / 111.9153
[2017-12-15 17:54:44] Epoch 0022 mean train/dev loss: 115.2098 / 111.7570
[2017-12-15 17:54:52] Epoch 0023 mean train/dev loss: 116.0955 / 117.5647
[2017-12-15 17:55:01] Epoch 0024 mean train/dev loss: 115.3534 / 110.1154
[2017-12-15 17:55:09] Epoch 0025 mean train/dev loss: 115.3571 / 117.4799
[2017-12-15 17:55:18] Epoch 0026 mean train/dev loss: 115.3927 / 109.9742
[2017-12-15 17:55:26] Epoch 0027 mean train/dev loss: 115.0088 / 115.4287
[2017-12-15 17:55:35] Epoch 0028 mean train/dev loss: 114.4727 / 108.8304
[2017-12-15 17:55:44] Epoch 0029 mean train/dev loss: 115.1612 / 112.3102
[2017-12-15 17:55:53] Epoch 0030 mean train/dev loss: 115.1430 / 115.3421
[2017-12-15 17:55:53] Learning rate decayed by 0.5000
[2017-12-15 17:55:53] Checkpointing model at epoch 30 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 17:55:53] Model Checkpointing finished.
[2017-12-15 17:56:01] Epoch 0031 mean train/dev loss: 112.8337 / 108.2414
[2017-12-15 17:56:10] Epoch 0032 mean train/dev loss: 113.1299 / 112.1986
[2017-12-15 17:56:18] Epoch 0033 mean train/dev loss: 113.1161 / 111.4286
[2017-12-15 17:56:26] Epoch 0034 mean train/dev loss: 113.4248 / 108.7343
[2017-12-15 17:56:35] Epoch 0035 mean train/dev loss: 113.4524 / 112.0583
[2017-12-15 17:56:43] Epoch 0036 mean train/dev loss: 113.8306 / 117.4619
[2017-12-15 17:56:52] Epoch 0037 mean train/dev loss: 113.3918 / 111.4707
[2017-12-15 17:57:00] Epoch 0038 mean train/dev loss: 112.9799 / 111.9656
[2017-12-15 17:57:09] Epoch 0039 mean train/dev loss: 113.4980 / 109.2655
[2017-12-15 17:57:17] Epoch 0040 mean train/dev loss: 113.1753 / 108.0800
[2017-12-15 17:57:17] Checkpointing model at epoch 40 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 17:57:18] Model Checkpointing finished.
[2017-12-15 17:57:26] Epoch 0041 mean train/dev loss: 113.6914 / 109.2599
[2017-12-15 17:57:34] Epoch 0042 mean train/dev loss: 113.4900 / 109.9566
[2017-12-15 17:57:43] Epoch 0043 mean train/dev loss: 113.6135 / 112.0984
[2017-12-15 17:57:51] Epoch 0044 mean train/dev loss: 113.4654 / 108.9097
[2017-12-15 17:58:00] Epoch 0045 mean train/dev loss: 113.1985 / 110.2484
[2017-12-15 17:58:00] Learning rate decayed by 0.5000
[2017-12-15 17:58:08] Epoch 0046 mean train/dev loss: 112.2047 / 114.7904
[2017-12-15 17:58:17] Epoch 0047 mean train/dev loss: 112.3110 / 108.6425
[2017-12-15 17:58:25] Epoch 0048 mean train/dev loss: 112.3720 / 111.7352
[2017-12-15 17:58:34] Epoch 0049 mean train/dev loss: 112.4154 / 110.4671
[2017-12-15 17:58:42] Epoch 0050 mean train/dev loss: 112.5382 / 108.5830
[2017-12-15 17:58:42] Checkpointing model at epoch 50 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 17:58:43] Model Checkpointing finished.
[2017-12-15 17:58:51] Epoch 0051 mean train/dev loss: 112.5465 / 111.5426
[2017-12-15 17:59:00] Epoch 0052 mean train/dev loss: 112.4345 / 107.3764
[2017-12-15 17:59:08] Epoch 0053 mean train/dev loss: 112.4666 / 110.6051
[2017-12-15 17:59:17] Epoch 0054 mean train/dev loss: 112.4063 / 109.5518
[2017-12-15 17:59:26] Epoch 0055 mean train/dev loss: 112.5274 / 107.2383
[2017-12-15 17:59:33] Epoch 0056 mean train/dev loss: 112.5350 / 114.2473
[2017-12-15 17:59:41] Epoch 0057 mean train/dev loss: 112.3612 / 111.0619
[2017-12-15 17:59:48] Epoch 0058 mean train/dev loss: 112.4107 / 107.7917
[2017-12-15 17:59:55] Epoch 0059 mean train/dev loss: 112.3752 / 108.7894
[2017-12-15 18:00:02] Epoch 0060 mean train/dev loss: 112.5867 / 112.8834
[2017-12-15 18:00:02] Learning rate decayed by 0.5000
[2017-12-15 18:00:02] Checkpointing model at epoch 60 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 18:00:03] Model Checkpointing finished.
[2017-12-15 18:00:11] Epoch 0061 mean train/dev loss: 111.8168 / 108.3242
[2017-12-15 18:00:17] Epoch 0062 mean train/dev loss: 111.8868 / 107.6954
[2017-12-15 18:00:25] Epoch 0063 mean train/dev loss: 111.8890 / 109.1301
[2017-12-15 18:00:31] Epoch 0064 mean train/dev loss: 111.9560 / 107.8352
[2017-12-15 18:00:37] Epoch 0065 mean train/dev loss: 111.8753 / 107.7396
[2017-12-15 18:00:42] Epoch 0066 mean train/dev loss: 111.9416 / 108.6492
[2017-12-15 18:00:48] Epoch 0067 mean train/dev loss: 111.9493 / 109.1269
[2017-12-15 18:00:54] Epoch 0068 mean train/dev loss: 111.8682 / 110.0895
[2017-12-15 18:01:00] Epoch 0069 mean train/dev loss: 111.8698 / 109.4638
[2017-12-15 18:01:05] Epoch 0070 mean train/dev loss: 111.8971 / 107.7152
[2017-12-15 18:01:05] Checkpointing model at epoch 70 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 18:01:06] Model Checkpointing finished.
[2017-12-15 18:01:11] Epoch 0071 mean train/dev loss: 111.8209 / 108.8856
[2017-12-15 18:01:17] Epoch 0072 mean train/dev loss: 111.8840 / 108.9813
[2017-12-15 18:01:23] Epoch 0073 mean train/dev loss: 111.9653 / 109.4897
[2017-12-15 18:01:29] Epoch 0074 mean train/dev loss: 111.9792 / 108.9555
[2017-12-15 18:01:35] Epoch 0075 mean train/dev loss: 111.8404 / 108.8472
[2017-12-15 18:01:35] Learning rate decayed by 0.5000
[2017-12-15 18:01:41] Epoch 0076 mean train/dev loss: 111.5467 / 108.4399
[2017-12-15 18:01:46] Epoch 0077 mean train/dev loss: 111.6528 / 107.7512
[2017-12-15 18:01:52] Epoch 0078 mean train/dev loss: 111.6936 / 109.2020
[2017-12-15 18:01:58] Epoch 0079 mean train/dev loss: 111.6573 / 109.8543
[2017-12-15 18:02:04] Epoch 0080 mean train/dev loss: 111.6113 / 109.5532
[2017-12-15 18:02:04] Checkpointing model at epoch 80 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 18:02:04] Model Checkpointing finished.
[2017-12-15 18:02:10] Epoch 0081 mean train/dev loss: 111.6386 / 108.5349
[2017-12-15 18:02:16] Epoch 0082 mean train/dev loss: 111.6247 / 108.4538
[2017-12-15 18:02:21] Epoch 0083 mean train/dev loss: 111.6140 / 108.0553
[2017-12-15 18:02:27] Epoch 0084 mean train/dev loss: 111.5991 / 109.5807
[2017-12-15 18:02:32] Epoch 0085 mean train/dev loss: 111.6767 / 108.8938
[2017-12-15 18:02:38] Epoch 0086 mean train/dev loss: 111.6221 / 108.0824
[2017-12-15 18:02:44] Epoch 0087 mean train/dev loss: 111.6259 / 108.3570
[2017-12-15 18:02:50] Epoch 0088 mean train/dev loss: 111.6349 / 108.0288
[2017-12-15 18:02:55] Epoch 0089 mean train/dev loss: 111.6263 / 107.4115
[2017-12-15 18:03:01] Epoch 0090 mean train/dev loss: 111.5863 / 108.0597
[2017-12-15 18:03:01] Learning rate decayed by 0.5000
[2017-12-15 18:03:01] Checkpointing model at epoch 90 for ffn.hl_50_50_50.lr_0.01.wd_10
[2017-12-15 18:03:01] Model Checkpointing finished.
[2017-12-15 18:03:07] Epoch 0091 mean train/dev loss: 111.4475 / 108.9204
[2017-12-15 18:03:12] Epoch 0092 mean train/dev loss: 111.4686 / 108.5709
[2017-12-15 18:03:18] Epoch 0093 mean train/dev loss: 111.4812 / 108.8518
[2017-12-15 18:03:24] Epoch 0094 mean train/dev loss: 111.4555 / 108.4953
[2017-12-15 18:03:29] Epoch 0095 mean train/dev loss: 111.4971 / 108.1831
[2017-12-15 18:03:35] Epoch 0096 mean train/dev loss: 111.4073 / 109.3291
[2017-12-15 18:03:35] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:03:35] 
                       *** Training finished *** 
[2017-12-15 18:03:36] Dev MSE: 109.3291
[2017-12-15 18:03:41] Training MSE: 111.8106
[2017-12-15 18:03:42] Experiment ffn.hl_50_50_50.lr_0.01.wd_10 logging ended.
