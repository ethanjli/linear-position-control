[2017-12-15 18:20:01] Experiment ffn.hl_100_100.lr_0.01.wd_10 logging started.
[2017-12-15 18:20:01] 
                       *** Starting Experiment ffn.hl_100_100.lr_0.01.wd_10 ***
                      
[2017-12-15 18:20:01] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100, 100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 18:20:01] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 100)
                        (relu2): ReLU ()
                        (linear3): Linear (100 -> 1)
                      )
[2017-12-15 18:20:01]  *** Training on GPU ***
[2017-12-15 18:20:09] Epoch 0001 mean train/dev loss: 14039.1232 / 332.0145
[2017-12-15 18:20:17] Epoch 0002 mean train/dev loss: 175.5763 / 168.1612
[2017-12-15 18:20:26] Epoch 0003 mean train/dev loss: 133.6340 / 150.5036
[2017-12-15 18:20:33] Epoch 0004 mean train/dev loss: 123.4408 / 117.3568
[2017-12-15 18:20:42] Epoch 0005 mean train/dev loss: 120.9026 / 119.9336
[2017-12-15 18:20:50] Epoch 0006 mean train/dev loss: 119.1587 / 114.5922
[2017-12-15 18:20:58] Epoch 0007 mean train/dev loss: 121.5864 / 155.0443
[2017-12-15 18:21:05] Epoch 0008 mean train/dev loss: 119.6354 / 114.7824
[2017-12-15 18:21:14] Epoch 0009 mean train/dev loss: 121.2613 / 120.8307
[2017-12-15 18:21:22] Epoch 0010 mean train/dev loss: 118.6130 / 133.0216
[2017-12-15 18:21:22] Checkpointing model at epoch 10 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:21:22] Model Checkpointing finished.
[2017-12-15 18:21:30] Epoch 0011 mean train/dev loss: 119.2929 / 120.2862
[2017-12-15 18:21:38] Epoch 0012 mean train/dev loss: 119.3891 / 115.7247
[2017-12-15 18:21:46] Epoch 0013 mean train/dev loss: 119.6154 / 130.7452
[2017-12-15 18:21:54] Epoch 0014 mean train/dev loss: 119.1451 / 114.1801
[2017-12-15 18:22:02] Epoch 0015 mean train/dev loss: 118.7368 / 123.9133
[2017-12-15 18:22:02] Learning rate decayed by 0.5000
[2017-12-15 18:22:10] Epoch 0016 mean train/dev loss: 115.6982 / 111.8275
[2017-12-15 18:22:18] Epoch 0017 mean train/dev loss: 116.2457 / 113.2274
[2017-12-15 18:22:26] Epoch 0018 mean train/dev loss: 116.0989 / 115.2286
[2017-12-15 18:22:34] Epoch 0019 mean train/dev loss: 116.2948 / 114.0466
[2017-12-15 18:22:42] Epoch 0020 mean train/dev loss: 116.6335 / 117.6851
[2017-12-15 18:22:42] Checkpointing model at epoch 20 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:22:42] Model Checkpointing finished.
[2017-12-15 18:22:51] Epoch 0021 mean train/dev loss: 116.4730 / 117.5987
[2017-12-15 18:22:58] Epoch 0022 mean train/dev loss: 116.2717 / 112.1493
[2017-12-15 18:23:07] Epoch 0023 mean train/dev loss: 116.1003 / 117.4105
[2017-12-15 18:23:15] Epoch 0024 mean train/dev loss: 116.3321 / 117.1094
[2017-12-15 18:23:23] Epoch 0025 mean train/dev loss: 116.4035 / 114.3660
[2017-12-15 18:23:30] Epoch 0026 mean train/dev loss: 116.0713 / 112.6715
[2017-12-15 18:23:38] Epoch 0027 mean train/dev loss: 116.4586 / 111.7490
[2017-12-15 18:23:46] Epoch 0028 mean train/dev loss: 116.0443 / 112.3124
[2017-12-15 18:23:54] Epoch 0029 mean train/dev loss: 116.4665 / 114.2059
[2017-12-15 18:24:02] Epoch 0030 mean train/dev loss: 116.0032 / 112.4924
[2017-12-15 18:24:02] Learning rate decayed by 0.5000
[2017-12-15 18:24:02] Checkpointing model at epoch 30 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:24:02] Model Checkpointing finished.
[2017-12-15 18:24:10] Epoch 0031 mean train/dev loss: 114.8824 / 111.5702
[2017-12-15 18:24:18] Epoch 0032 mean train/dev loss: 114.9617 / 117.6133
[2017-12-15 18:24:26] Epoch 0033 mean train/dev loss: 114.9452 / 116.4123
[2017-12-15 18:24:34] Epoch 0034 mean train/dev loss: 115.0682 / 112.7052
[2017-12-15 18:24:42] Epoch 0035 mean train/dev loss: 115.1026 / 112.3458
[2017-12-15 18:24:50] Epoch 0036 mean train/dev loss: 115.0975 / 110.3960
[2017-12-15 18:24:58] Epoch 0037 mean train/dev loss: 114.9747 / 117.5512
[2017-12-15 18:25:06] Epoch 0038 mean train/dev loss: 115.0000 / 115.0526
[2017-12-15 18:25:14] Epoch 0039 mean train/dev loss: 114.9050 / 110.9928
[2017-12-15 18:25:22] Epoch 0040 mean train/dev loss: 115.2409 / 112.5623
[2017-12-15 18:25:22] Checkpointing model at epoch 40 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:25:22] Model Checkpointing finished.
[2017-12-15 18:25:30] Epoch 0041 mean train/dev loss: 114.8517 / 112.0181
[2017-12-15 18:25:38] Epoch 0042 mean train/dev loss: 115.0826 / 112.7646
[2017-12-15 18:25:47] Epoch 0043 mean train/dev loss: 115.0778 / 109.9127
[2017-12-15 18:25:55] Epoch 0044 mean train/dev loss: 115.0947 / 113.9542
[2017-12-15 18:26:03] Epoch 0045 mean train/dev loss: 115.0819 / 116.0748
[2017-12-15 18:26:03] Learning rate decayed by 0.5000
[2017-12-15 18:26:11] Epoch 0046 mean train/dev loss: 114.3594 / 111.1720
[2017-12-15 18:26:19] Epoch 0047 mean train/dev loss: 114.2806 / 110.0759
[2017-12-15 18:26:27] Epoch 0048 mean train/dev loss: 114.4436 / 113.7902
[2017-12-15 18:26:35] Epoch 0049 mean train/dev loss: 114.3585 / 112.6671
[2017-12-15 18:26:43] Epoch 0050 mean train/dev loss: 114.4054 / 112.3894
[2017-12-15 18:26:43] Checkpointing model at epoch 50 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:26:44] Model Checkpointing finished.
[2017-12-15 18:26:52] Epoch 0051 mean train/dev loss: 114.3912 / 113.6400
[2017-12-15 18:27:00] Epoch 0052 mean train/dev loss: 114.3511 / 110.3249
[2017-12-15 18:27:08] Epoch 0053 mean train/dev loss: 114.4137 / 112.9886
[2017-12-15 18:27:16] Epoch 0054 mean train/dev loss: 114.3855 / 112.3191
[2017-12-15 18:27:24] Epoch 0055 mean train/dev loss: 114.5223 / 112.4956
[2017-12-15 18:27:32] Epoch 0056 mean train/dev loss: 114.4334 / 111.7788
[2017-12-15 18:27:40] Epoch 0057 mean train/dev loss: 114.3506 / 111.6873
[2017-12-15 18:27:48] Epoch 0058 mean train/dev loss: 114.3209 / 113.1023
[2017-12-15 18:27:55] Epoch 0059 mean train/dev loss: 114.2710 / 112.3490
[2017-12-15 18:28:03] Epoch 0060 mean train/dev loss: 114.3765 / 113.1503
[2017-12-15 18:28:03] Learning rate decayed by 0.5000
[2017-12-15 18:28:03] Checkpointing model at epoch 60 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:28:04] Model Checkpointing finished.
[2017-12-15 18:28:12] Epoch 0061 mean train/dev loss: 113.9292 / 110.3449
[2017-12-15 18:28:20] Epoch 0062 mean train/dev loss: 113.9847 / 111.9025
[2017-12-15 18:28:28] Epoch 0063 mean train/dev loss: 113.9928 / 110.4608
[2017-12-15 18:28:36] Epoch 0064 mean train/dev loss: 114.0071 / 110.6979
[2017-12-15 18:28:44] Epoch 0065 mean train/dev loss: 114.0045 / 110.7722
[2017-12-15 18:28:52] Epoch 0066 mean train/dev loss: 114.0115 / 111.1512
[2017-12-15 18:29:00] Epoch 0067 mean train/dev loss: 113.9880 / 111.7185
[2017-12-15 18:29:08] Epoch 0068 mean train/dev loss: 113.9737 / 111.5701
[2017-12-15 18:29:16] Epoch 0069 mean train/dev loss: 114.0186 / 109.9318
[2017-12-15 18:29:24] Epoch 0070 mean train/dev loss: 114.0505 / 110.7063
[2017-12-15 18:29:24] Checkpointing model at epoch 70 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:29:25] Model Checkpointing finished.
[2017-12-15 18:29:31] Epoch 0071 mean train/dev loss: 113.9070 / 111.5218
[2017-12-15 18:29:36] Epoch 0072 mean train/dev loss: 113.9926 / 110.0894
[2017-12-15 18:29:42] Epoch 0073 mean train/dev loss: 114.0014 / 110.4003
[2017-12-15 18:29:47] Epoch 0074 mean train/dev loss: 113.9827 / 111.4391
[2017-12-15 18:29:52] Epoch 0075 mean train/dev loss: 113.9356 / 110.9957
[2017-12-15 18:29:52] Learning rate decayed by 0.5000
[2017-12-15 18:29:58] Epoch 0076 mean train/dev loss: 113.8114 / 110.8235
[2017-12-15 18:30:03] Epoch 0077 mean train/dev loss: 113.7969 / 109.9940
[2017-12-15 18:30:09] Epoch 0078 mean train/dev loss: 113.7681 / 110.7292
[2017-12-15 18:30:14] Epoch 0079 mean train/dev loss: 113.7984 / 110.5210
[2017-12-15 18:30:20] Epoch 0080 mean train/dev loss: 113.8136 / 110.0620
[2017-12-15 18:30:20] Checkpointing model at epoch 80 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:30:20] Model Checkpointing finished.
[2017-12-15 18:30:25] Epoch 0081 mean train/dev loss: 113.8032 / 109.6792
[2017-12-15 18:30:30] Epoch 0082 mean train/dev loss: 113.7740 / 111.0923
[2017-12-15 18:30:36] Epoch 0083 mean train/dev loss: 113.8055 / 111.4612
[2017-12-15 18:30:41] Epoch 0084 mean train/dev loss: 113.8326 / 110.8799
[2017-12-15 18:30:47] Epoch 0085 mean train/dev loss: 113.8152 / 111.0555
[2017-12-15 18:30:52] Epoch 0086 mean train/dev loss: 113.8385 / 110.5687
[2017-12-15 18:30:57] Epoch 0087 mean train/dev loss: 113.8021 / 110.3420
[2017-12-15 18:31:03] Epoch 0088 mean train/dev loss: 113.8143 / 110.6003
[2017-12-15 18:31:08] Epoch 0089 mean train/dev loss: 113.8120 / 110.1047
[2017-12-15 18:31:14] Epoch 0090 mean train/dev loss: 113.7906 / 110.7771
[2017-12-15 18:31:14] Learning rate decayed by 0.5000
[2017-12-15 18:31:14] Checkpointing model at epoch 90 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:31:14] Model Checkpointing finished.
[2017-12-15 18:31:20] Epoch 0091 mean train/dev loss: 113.6712 / 110.5857
[2017-12-15 18:31:25] Epoch 0092 mean train/dev loss: 113.6928 / 111.2223
[2017-12-15 18:31:31] Epoch 0093 mean train/dev loss: 113.7157 / 110.6934
[2017-12-15 18:31:36] Epoch 0094 mean train/dev loss: 113.6621 / 111.1506
[2017-12-15 18:31:41] Epoch 0095 mean train/dev loss: 113.7240 / 111.6125
[2017-12-15 18:31:47] Epoch 0096 mean train/dev loss: 113.6972 / 110.9255
[2017-12-15 18:31:52] Epoch 0097 mean train/dev loss: 113.6765 / 111.2240
[2017-12-15 18:31:58] Epoch 0098 mean train/dev loss: 113.6703 / 111.7832
[2017-12-15 18:32:03] Epoch 0099 mean train/dev loss: 113.6916 / 110.7468
[2017-12-15 18:32:09] Epoch 0100 mean train/dev loss: 113.7027 / 110.2408
[2017-12-15 18:32:09] Checkpointing model at epoch 100 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:32:09] Model Checkpointing finished.
[2017-12-15 18:32:14] Epoch 0101 mean train/dev loss: 113.6763 / 110.5252
[2017-12-15 18:32:20] Epoch 0102 mean train/dev loss: 113.6774 / 110.3919
[2017-12-15 18:32:25] Epoch 0103 mean train/dev loss: 113.6848 / 110.8221
[2017-12-15 18:32:31] Epoch 0104 mean train/dev loss: 113.6811 / 110.6143
[2017-12-15 18:32:36] Epoch 0105 mean train/dev loss: 113.6571 / 111.2959
[2017-12-15 18:32:36] Learning rate decayed by 0.5000
[2017-12-15 18:32:42] Epoch 0106 mean train/dev loss: 113.6318 / 110.8091
[2017-12-15 18:32:47] Epoch 0107 mean train/dev loss: 113.6054 / 110.4664
[2017-12-15 18:32:53] Epoch 0108 mean train/dev loss: 113.6194 / 111.1245
[2017-12-15 18:32:58] Epoch 0109 mean train/dev loss: 113.6274 / 110.2526
[2017-12-15 18:33:04] Epoch 0110 mean train/dev loss: 113.6027 / 110.9234
[2017-12-15 18:33:04] Checkpointing model at epoch 110 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:33:04] Model Checkpointing finished.
[2017-12-15 18:33:09] Epoch 0111 mean train/dev loss: 113.6068 / 111.2856
[2017-12-15 18:33:15] Epoch 0112 mean train/dev loss: 113.6142 / 110.5423
[2017-12-15 18:33:20] Epoch 0113 mean train/dev loss: 113.6387 / 110.6974
[2017-12-15 18:33:25] Epoch 0114 mean train/dev loss: 113.6023 / 110.7076
[2017-12-15 18:33:30] Epoch 0115 mean train/dev loss: 113.6136 / 110.5279
[2017-12-15 18:33:36] Epoch 0116 mean train/dev loss: 113.6232 / 110.7911
[2017-12-15 18:33:41] Epoch 0117 mean train/dev loss: 113.5972 / 110.5705
[2017-12-15 18:33:46] Epoch 0118 mean train/dev loss: 113.6275 / 110.7200
[2017-12-15 18:33:51] Epoch 0119 mean train/dev loss: 113.5997 / 111.1902
[2017-12-15 18:33:56] Epoch 0120 mean train/dev loss: 113.6193 / 111.0234
[2017-12-15 18:33:56] Learning rate decayed by 0.5000
[2017-12-15 18:33:56] Checkpointing model at epoch 120 for ffn.hl_100_100.lr_0.01.wd_10
[2017-12-15 18:33:57] Model Checkpointing finished.
[2017-12-15 18:34:02] Epoch 0121 mean train/dev loss: 113.5960 / 110.6772
[2017-12-15 18:34:08] Epoch 0122 mean train/dev loss: 113.5784 / 110.5448
[2017-12-15 18:34:08] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:34:08] 
                       *** Training finished *** 
[2017-12-15 18:34:09] Dev MSE: 110.5448
[2017-12-15 18:34:13] Training MSE: 113.3834
[2017-12-15 18:34:14] Experiment ffn.hl_100_100.lr_0.01.wd_10 logging ended.
