[2017-12-15 18:20:01] Experiment ffn.hl_100_100.lr_0.01.wd_1.0 logging started.
[2017-12-15 18:20:01] 
                       *** Starting Experiment ffn.hl_100_100.lr_0.01.wd_1.0 ***
                      
[2017-12-15 18:20:01] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100, 100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 18:20:01] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 100)
                        (relu2): ReLU ()
                        (linear3): Linear (100 -> 1)
                      )
[2017-12-15 18:20:01]  *** Training on GPU ***
[2017-12-15 18:20:09] Epoch 0001 mean train/dev loss: 14330.7473 / 241.5233
[2017-12-15 18:20:17] Epoch 0002 mean train/dev loss: 131.3830 / 160.5345
[2017-12-15 18:20:25] Epoch 0003 mean train/dev loss: 120.6651 / 140.9221
[2017-12-15 18:20:33] Epoch 0004 mean train/dev loss: 116.5962 / 127.2439
[2017-12-15 18:20:41] Epoch 0005 mean train/dev loss: 115.0584 / 121.4174
[2017-12-15 18:20:49] Epoch 0006 mean train/dev loss: 114.5960 / 136.3134
[2017-12-15 18:20:57] Epoch 0007 mean train/dev loss: 114.4584 / 133.4889
[2017-12-15 18:21:04] Epoch 0008 mean train/dev loss: 114.0200 / 116.4179
[2017-12-15 18:21:12] Epoch 0009 mean train/dev loss: 114.3719 / 137.1203
[2017-12-15 18:21:20] Epoch 0010 mean train/dev loss: 113.1386 / 114.0964
[2017-12-15 18:21:20] Checkpointing model at epoch 10 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:21:20] Model Checkpointing finished.
[2017-12-15 18:21:29] Epoch 0011 mean train/dev loss: 111.6200 / 113.7517
[2017-12-15 18:21:36] Epoch 0012 mean train/dev loss: 111.6804 / 133.0041
[2017-12-15 18:21:45] Epoch 0013 mean train/dev loss: 111.8748 / 141.5902
[2017-12-15 18:21:53] Epoch 0014 mean train/dev loss: 110.7190 / 114.1576
[2017-12-15 18:22:01] Epoch 0015 mean train/dev loss: 111.1705 / 126.2388
[2017-12-15 18:22:01] Learning rate decayed by 0.5000
[2017-12-15 18:22:09] Epoch 0016 mean train/dev loss: 107.5940 / 113.6693
[2017-12-15 18:22:17] Epoch 0017 mean train/dev loss: 108.2134 / 117.1627
[2017-12-15 18:22:25] Epoch 0018 mean train/dev loss: 108.1441 / 122.2958
[2017-12-15 18:22:33] Epoch 0019 mean train/dev loss: 108.4219 / 113.8553
[2017-12-15 18:22:41] Epoch 0020 mean train/dev loss: 107.9028 / 114.9194
[2017-12-15 18:22:41] Checkpointing model at epoch 20 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:22:41] Model Checkpointing finished.
[2017-12-15 18:22:49] Epoch 0021 mean train/dev loss: 108.2439 / 114.0752
[2017-12-15 18:22:57] Epoch 0022 mean train/dev loss: 108.1041 / 112.2047
[2017-12-15 18:23:05] Epoch 0023 mean train/dev loss: 108.5617 / 110.8183
[2017-12-15 18:23:13] Epoch 0024 mean train/dev loss: 107.9084 / 109.8029
[2017-12-15 18:23:21] Epoch 0025 mean train/dev loss: 108.4031 / 114.4747
[2017-12-15 18:23:28] Epoch 0026 mean train/dev loss: 107.9078 / 113.1400
[2017-12-15 18:23:36] Epoch 0027 mean train/dev loss: 107.5733 / 118.0460
[2017-12-15 18:23:45] Epoch 0028 mean train/dev loss: 107.3869 / 110.2683
[2017-12-15 18:23:53] Epoch 0029 mean train/dev loss: 107.2695 / 110.9727
[2017-12-15 18:24:01] Epoch 0030 mean train/dev loss: 107.2248 / 110.6111
[2017-12-15 18:24:01] Learning rate decayed by 0.5000
[2017-12-15 18:24:01] Checkpointing model at epoch 30 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:24:01] Model Checkpointing finished.
[2017-12-15 18:24:09] Epoch 0031 mean train/dev loss: 106.1956 / 114.3544
[2017-12-15 18:24:17] Epoch 0032 mean train/dev loss: 105.8838 / 119.2684
[2017-12-15 18:24:25] Epoch 0033 mean train/dev loss: 105.8218 / 110.7754
[2017-12-15 18:24:33] Epoch 0034 mean train/dev loss: 106.3622 / 111.2454
[2017-12-15 18:24:41] Epoch 0035 mean train/dev loss: 106.0548 / 117.0494
[2017-12-15 18:24:49] Epoch 0036 mean train/dev loss: 106.1665 / 109.3346
[2017-12-15 18:24:57] Epoch 0037 mean train/dev loss: 105.9262 / 109.1871
[2017-12-15 18:25:05] Epoch 0038 mean train/dev loss: 106.1626 / 110.7701
[2017-12-15 18:25:12] Epoch 0039 mean train/dev loss: 105.8488 / 111.8707
[2017-12-15 18:25:20] Epoch 0040 mean train/dev loss: 105.9081 / 112.2711
[2017-12-15 18:25:20] Checkpointing model at epoch 40 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:25:21] Model Checkpointing finished.
[2017-12-15 18:25:29] Epoch 0041 mean train/dev loss: 106.1486 / 114.8909
[2017-12-15 18:25:37] Epoch 0042 mean train/dev loss: 105.9579 / 110.1783
[2017-12-15 18:25:45] Epoch 0043 mean train/dev loss: 106.0179 / 115.8351
[2017-12-15 18:25:53] Epoch 0044 mean train/dev loss: 105.9663 / 108.3891
[2017-12-15 18:26:00] Epoch 0045 mean train/dev loss: 105.9005 / 111.5571
[2017-12-15 18:26:00] Learning rate decayed by 0.5000
[2017-12-15 18:26:09] Epoch 0046 mean train/dev loss: 105.2363 / 110.8704
[2017-12-15 18:26:17] Epoch 0047 mean train/dev loss: 105.3203 / 108.0700
[2017-12-15 18:26:24] Epoch 0048 mean train/dev loss: 105.1971 / 112.0874
[2017-12-15 18:26:32] Epoch 0049 mean train/dev loss: 105.2228 / 113.1622
[2017-12-15 18:26:40] Epoch 0050 mean train/dev loss: 105.2418 / 108.7838
[2017-12-15 18:26:40] Checkpointing model at epoch 50 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:26:41] Model Checkpointing finished.
[2017-12-15 18:26:49] Epoch 0051 mean train/dev loss: 105.2626 / 112.8565
[2017-12-15 18:26:57] Epoch 0052 mean train/dev loss: 105.1412 / 111.3394
[2017-12-15 18:27:05] Epoch 0053 mean train/dev loss: 105.2609 / 109.5540
[2017-12-15 18:27:13] Epoch 0054 mean train/dev loss: 105.2868 / 110.5671
[2017-12-15 18:27:21] Epoch 0055 mean train/dev loss: 105.2365 / 111.7362
[2017-12-15 18:27:29] Epoch 0056 mean train/dev loss: 105.1595 / 113.3218
[2017-12-15 18:27:37] Epoch 0057 mean train/dev loss: 105.2298 / 111.3881
[2017-12-15 18:27:45] Epoch 0058 mean train/dev loss: 105.0799 / 109.5443
[2017-12-15 18:27:53] Epoch 0059 mean train/dev loss: 105.2243 / 110.9084
[2017-12-15 18:28:01] Epoch 0060 mean train/dev loss: 105.2748 / 111.4539
[2017-12-15 18:28:01] Learning rate decayed by 0.5000
[2017-12-15 18:28:01] Checkpointing model at epoch 60 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:28:02] Model Checkpointing finished.
[2017-12-15 18:28:09] Epoch 0061 mean train/dev loss: 104.7932 / 109.2287
[2017-12-15 18:28:18] Epoch 0062 mean train/dev loss: 104.7263 / 107.7045
[2017-12-15 18:28:26] Epoch 0063 mean train/dev loss: 104.6837 / 110.3891
[2017-12-15 18:28:34] Epoch 0064 mean train/dev loss: 104.7308 / 111.9924
[2017-12-15 18:28:41] Epoch 0065 mean train/dev loss: 104.7216 / 108.5287
[2017-12-15 18:28:49] Epoch 0066 mean train/dev loss: 104.7186 / 110.9853
[2017-12-15 18:28:57] Epoch 0067 mean train/dev loss: 104.7366 / 109.2031
[2017-12-15 18:29:05] Epoch 0068 mean train/dev loss: 104.6931 / 113.8425
[2017-12-15 18:29:13] Epoch 0069 mean train/dev loss: 104.7066 / 110.8557
[2017-12-15 18:29:20] Epoch 0070 mean train/dev loss: 104.6848 / 110.3454
[2017-12-15 18:29:20] Checkpointing model at epoch 70 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:29:21] Model Checkpointing finished.
[2017-12-15 18:29:27] Epoch 0071 mean train/dev loss: 104.6971 / 107.1506
[2017-12-15 18:29:32] Epoch 0072 mean train/dev loss: 104.6900 / 110.6184
[2017-12-15 18:29:38] Epoch 0073 mean train/dev loss: 104.7616 / 111.5327
[2017-12-15 18:29:43] Epoch 0074 mean train/dev loss: 104.7593 / 110.5281
[2017-12-15 18:29:48] Epoch 0075 mean train/dev loss: 104.7595 / 108.7233
[2017-12-15 18:29:48] Learning rate decayed by 0.5000
[2017-12-15 18:29:54] Epoch 0076 mean train/dev loss: 104.4645 / 109.5162
[2017-12-15 18:29:59] Epoch 0077 mean train/dev loss: 104.5112 / 108.7718
[2017-12-15 18:30:05] Epoch 0078 mean train/dev loss: 104.5090 / 109.6210
[2017-12-15 18:30:10] Epoch 0079 mean train/dev loss: 104.5062 / 109.9807
[2017-12-15 18:30:15] Epoch 0080 mean train/dev loss: 104.5082 / 109.4105
[2017-12-15 18:30:15] Checkpointing model at epoch 80 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:30:16] Model Checkpointing finished.
[2017-12-15 18:30:21] Epoch 0081 mean train/dev loss: 104.4852 / 110.5347
[2017-12-15 18:30:26] Epoch 0082 mean train/dev loss: 104.4644 / 108.5278
[2017-12-15 18:30:32] Epoch 0083 mean train/dev loss: 104.4918 / 109.0705
[2017-12-15 18:30:37] Epoch 0084 mean train/dev loss: 104.4801 / 109.8201
[2017-12-15 18:30:43] Epoch 0085 mean train/dev loss: 104.4865 / 108.9923
[2017-12-15 18:30:48] Epoch 0086 mean train/dev loss: 104.5186 / 110.4355
[2017-12-15 18:30:53] Epoch 0087 mean train/dev loss: 104.4584 / 108.8171
[2017-12-15 18:30:59] Epoch 0088 mean train/dev loss: 104.4773 / 109.6907
[2017-12-15 18:31:04] Epoch 0089 mean train/dev loss: 104.5051 / 109.0674
[2017-12-15 18:31:10] Epoch 0090 mean train/dev loss: 104.4439 / 109.1314
[2017-12-15 18:31:10] Learning rate decayed by 0.5000
[2017-12-15 18:31:10] Checkpointing model at epoch 90 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:31:10] Model Checkpointing finished.
[2017-12-15 18:31:15] Epoch 0091 mean train/dev loss: 104.3475 / 109.1178
[2017-12-15 18:31:21] Epoch 0092 mean train/dev loss: 104.3407 / 108.8475
[2017-12-15 18:31:27] Epoch 0093 mean train/dev loss: 104.3548 / 109.6539
[2017-12-15 18:31:32] Epoch 0094 mean train/dev loss: 104.3383 / 109.2630
[2017-12-15 18:31:38] Epoch 0095 mean train/dev loss: 104.3337 / 109.4128
[2017-12-15 18:31:43] Epoch 0096 mean train/dev loss: 104.3582 / 109.1640
[2017-12-15 18:31:49] Epoch 0097 mean train/dev loss: 104.3568 / 109.3913
[2017-12-15 18:31:54] Epoch 0098 mean train/dev loss: 104.3334 / 109.1728
[2017-12-15 18:32:00] Epoch 0099 mean train/dev loss: 104.3473 / 109.2583
[2017-12-15 18:32:05] Epoch 0100 mean train/dev loss: 104.3556 / 108.5073
[2017-12-15 18:32:05] Checkpointing model at epoch 100 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:32:06] Model Checkpointing finished.
[2017-12-15 18:32:11] Epoch 0101 mean train/dev loss: 104.3400 / 108.9255
[2017-12-15 18:32:17] Epoch 0102 mean train/dev loss: 104.3731 / 108.2393
[2017-12-15 18:32:22] Epoch 0103 mean train/dev loss: 104.3353 / 109.4159
[2017-12-15 18:32:27] Epoch 0104 mean train/dev loss: 104.3724 / 108.4681
[2017-12-15 18:32:33] Epoch 0105 mean train/dev loss: 104.3378 / 108.3919
[2017-12-15 18:32:33] Learning rate decayed by 0.5000
[2017-12-15 18:32:38] Epoch 0106 mean train/dev loss: 104.2928 / 109.3376
[2017-12-15 18:32:43] Epoch 0107 mean train/dev loss: 104.2770 / 109.1033
[2017-12-15 18:32:49] Epoch 0108 mean train/dev loss: 104.2849 / 108.4872
[2017-12-15 18:32:55] Epoch 0109 mean train/dev loss: 104.2665 / 109.1041
[2017-12-15 18:33:00] Epoch 0110 mean train/dev loss: 104.2725 / 109.3170
[2017-12-15 18:33:00] Checkpointing model at epoch 110 for ffn.hl_100_100.lr_0.01.wd_1.0
[2017-12-15 18:33:00] Model Checkpointing finished.
[2017-12-15 18:33:06] Epoch 0111 mean train/dev loss: 104.2670 / 110.0797
[2017-12-15 18:33:11] Epoch 0112 mean train/dev loss: 104.2826 / 109.3619
[2017-12-15 18:33:11] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:33:11] 
                       *** Training finished *** 
[2017-12-15 18:33:12] Dev MSE: 109.3619
[2017-12-15 18:33:17] Training MSE: 104.2313
[2017-12-15 18:33:18] Experiment ffn.hl_100_100.lr_0.01.wd_1.0 logging ended.
