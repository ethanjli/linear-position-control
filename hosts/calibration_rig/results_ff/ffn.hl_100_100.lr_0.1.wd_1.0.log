[2017-12-15 16:13:34] Experiment ffn.hl_100_100.lr_0.1.wd_1.0 logging started.
[2017-12-15 16:13:34] 
                       *** Starting Experiment ffn.hl_100_100.lr_0.1.wd_1.0 ***
                      
[2017-12-15 16:13:34] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100, 100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 16:13:34] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 100)
                        (relu2): ReLU ()
                        (linear3): Linear (100 -> 1)
                      )
[2017-12-15 16:13:34]  *** Training on GPU ***
[2017-12-15 16:13:42] Epoch 0001 mean train/dev loss: 3337.1112 / 191.5291
[2017-12-15 16:13:50] Epoch 0002 mean train/dev loss: 216.6678 / 550.4568
[2017-12-15 16:13:58] Epoch 0003 mean train/dev loss: 210.5986 / 308.4973
[2017-12-15 16:14:06] Epoch 0004 mean train/dev loss: 177.4371 / 127.3324
[2017-12-15 16:14:14] Epoch 0005 mean train/dev loss: 151.4343 / 123.4162
[2017-12-15 16:14:22] Epoch 0006 mean train/dev loss: 146.7278 / 133.6542
[2017-12-15 16:14:30] Epoch 0007 mean train/dev loss: 133.4633 / 132.7147
[2017-12-15 16:14:38] Epoch 0008 mean train/dev loss: 126.5023 / 152.6071
[2017-12-15 16:14:47] Epoch 0009 mean train/dev loss: 133.6837 / 177.8558
[2017-12-15 16:14:55] Epoch 0010 mean train/dev loss: 129.8386 / 158.6688
[2017-12-15 16:14:55] Checkpointing model at epoch 10 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:14:55] Model Checkpointing finished.
[2017-12-15 16:15:03] Epoch 0011 mean train/dev loss: 129.6066 / 136.2152
[2017-12-15 16:15:11] Epoch 0012 mean train/dev loss: 126.4157 / 124.7513
[2017-12-15 16:15:19] Epoch 0013 mean train/dev loss: 129.9098 / 140.1348
[2017-12-15 16:15:27] Epoch 0014 mean train/dev loss: 203.7974 / 120.4957
[2017-12-15 16:15:35] Epoch 0015 mean train/dev loss: 118.2154 / 193.3411
[2017-12-15 16:15:35] Learning rate decayed by 0.5000
[2017-12-15 16:15:43] Epoch 0016 mean train/dev loss: 113.0501 / 116.9674
[2017-12-15 16:15:50] Epoch 0017 mean train/dev loss: 113.5034 / 124.7708
[2017-12-15 16:15:58] Epoch 0018 mean train/dev loss: 114.5958 / 122.8242
[2017-12-15 16:16:06] Epoch 0019 mean train/dev loss: 115.5623 / 138.7521
[2017-12-15 16:16:14] Epoch 0020 mean train/dev loss: 114.8739 / 117.1810
[2017-12-15 16:16:14] Checkpointing model at epoch 20 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:16:15] Model Checkpointing finished.
[2017-12-15 16:16:23] Epoch 0021 mean train/dev loss: 115.4855 / 135.0478
[2017-12-15 16:16:31] Epoch 0022 mean train/dev loss: 115.0403 / 127.6102
[2017-12-15 16:16:39] Epoch 0023 mean train/dev loss: 114.9628 / 127.2319
[2017-12-15 16:16:47] Epoch 0024 mean train/dev loss: 114.7284 / 120.0527
[2017-12-15 16:16:56] Epoch 0025 mean train/dev loss: 115.3405 / 122.2119
[2017-12-15 16:17:04] Epoch 0026 mean train/dev loss: 114.2369 / 123.7169
[2017-12-15 16:17:12] Epoch 0027 mean train/dev loss: 116.9150 / 138.7692
[2017-12-15 16:17:20] Epoch 0028 mean train/dev loss: 114.1446 / 134.2280
[2017-12-15 16:17:28] Epoch 0029 mean train/dev loss: 114.6631 / 133.7081
[2017-12-15 16:17:36] Epoch 0030 mean train/dev loss: 115.1408 / 142.8929
[2017-12-15 16:17:36] Learning rate decayed by 0.5000
[2017-12-15 16:17:36] Checkpointing model at epoch 30 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:17:37] Model Checkpointing finished.
[2017-12-15 16:17:45] Epoch 0031 mean train/dev loss: 111.6281 / 119.6997
[2017-12-15 16:17:53] Epoch 0032 mean train/dev loss: 111.4668 / 113.0661
[2017-12-15 16:18:01] Epoch 0033 mean train/dev loss: 111.6719 / 124.0191
[2017-12-15 16:18:09] Epoch 0034 mean train/dev loss: 111.2306 / 116.8898
[2017-12-15 16:18:16] Epoch 0035 mean train/dev loss: 111.9327 / 133.7178
[2017-12-15 16:18:25] Epoch 0036 mean train/dev loss: 111.4946 / 122.9632
[2017-12-15 16:18:33] Epoch 0037 mean train/dev loss: 111.4757 / 128.1254
[2017-12-15 16:18:41] Epoch 0038 mean train/dev loss: 111.4385 / 118.7647
[2017-12-15 16:18:49] Epoch 0039 mean train/dev loss: 111.7569 / 119.6994
[2017-12-15 16:18:57] Epoch 0040 mean train/dev loss: 111.4606 / 121.1323
[2017-12-15 16:18:57] Checkpointing model at epoch 40 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:18:57] Model Checkpointing finished.
[2017-12-15 16:19:05] Epoch 0041 mean train/dev loss: 111.6094 / 119.0426
[2017-12-15 16:19:13] Epoch 0042 mean train/dev loss: 111.8835 / 117.5604
[2017-12-15 16:19:21] Epoch 0043 mean train/dev loss: 111.2717 / 121.4529
[2017-12-15 16:19:29] Epoch 0044 mean train/dev loss: 111.4743 / 112.5808
[2017-12-15 16:19:38] Epoch 0045 mean train/dev loss: 111.5945 / 125.5567
[2017-12-15 16:19:38] Learning rate decayed by 0.5000
[2017-12-15 16:19:46] Epoch 0046 mean train/dev loss: 109.8641 / 118.9540
[2017-12-15 16:19:54] Epoch 0047 mean train/dev loss: 110.0608 / 119.5423
[2017-12-15 16:20:02] Epoch 0048 mean train/dev loss: 109.9353 / 121.0896
[2017-12-15 16:20:10] Epoch 0049 mean train/dev loss: 109.9726 / 123.6798
[2017-12-15 16:20:18] Epoch 0050 mean train/dev loss: 110.0899 / 122.3926
[2017-12-15 16:20:18] Checkpointing model at epoch 50 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:20:18] Model Checkpointing finished.
[2017-12-15 16:20:27] Epoch 0051 mean train/dev loss: 110.2526 / 120.2429
[2017-12-15 16:20:35] Epoch 0052 mean train/dev loss: 109.9301 / 121.6129
[2017-12-15 16:20:43] Epoch 0053 mean train/dev loss: 109.9791 / 121.6176
[2017-12-15 16:20:52] Epoch 0054 mean train/dev loss: 109.8853 / 117.6717
[2017-12-15 16:21:00] Epoch 0055 mean train/dev loss: 109.9551 / 119.8623
[2017-12-15 16:21:08] Epoch 0056 mean train/dev loss: 110.3481 / 119.3899
[2017-12-15 16:21:16] Epoch 0057 mean train/dev loss: 109.9209 / 123.6466
[2017-12-15 16:21:24] Epoch 0058 mean train/dev loss: 109.8955 / 117.9550
[2017-12-15 16:21:32] Epoch 0059 mean train/dev loss: 109.8015 / 119.1334
[2017-12-15 16:21:40] Epoch 0060 mean train/dev loss: 109.5189 / 118.4540
[2017-12-15 16:21:40] Learning rate decayed by 0.5000
[2017-12-15 16:21:40] Checkpointing model at epoch 60 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:21:40] Model Checkpointing finished.
[2017-12-15 16:21:49] Epoch 0061 mean train/dev loss: 108.4420 / 121.0131
[2017-12-15 16:21:57] Epoch 0062 mean train/dev loss: 108.4503 / 117.8646
[2017-12-15 16:22:05] Epoch 0063 mean train/dev loss: 108.3955 / 117.2010
[2017-12-15 16:22:13] Epoch 0064 mean train/dev loss: 108.3093 / 123.8626
[2017-12-15 16:22:21] Epoch 0065 mean train/dev loss: 108.3481 / 112.2898
[2017-12-15 16:22:29] Epoch 0066 mean train/dev loss: 108.2803 / 116.7902
[2017-12-15 16:22:37] Epoch 0067 mean train/dev loss: 108.3600 / 118.7033
[2017-12-15 16:22:45] Epoch 0068 mean train/dev loss: 108.4066 / 117.8684
[2017-12-15 16:22:53] Epoch 0069 mean train/dev loss: 108.3935 / 119.9621
[2017-12-15 16:23:01] Epoch 0070 mean train/dev loss: 108.4091 / 119.6623
[2017-12-15 16:23:01] Checkpointing model at epoch 70 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:23:01] Model Checkpointing finished.
[2017-12-15 16:23:09] Epoch 0071 mean train/dev loss: 108.5286 / 115.6786
[2017-12-15 16:23:17] Epoch 0072 mean train/dev loss: 108.2776 / 114.6693
[2017-12-15 16:23:25] Epoch 0073 mean train/dev loss: 108.5032 / 115.1346
[2017-12-15 16:23:33] Epoch 0074 mean train/dev loss: 108.3250 / 115.5068
[2017-12-15 16:23:42] Epoch 0075 mean train/dev loss: 108.4092 / 117.3544
[2017-12-15 16:23:42] Learning rate decayed by 0.5000
[2017-12-15 16:23:50] Epoch 0076 mean train/dev loss: 107.8693 / 115.6739
[2017-12-15 16:23:58] Epoch 0077 mean train/dev loss: 107.8539 / 117.5197
[2017-12-15 16:24:06] Epoch 0078 mean train/dev loss: 107.8165 / 117.7460
[2017-12-15 16:24:14] Epoch 0079 mean train/dev loss: 107.7868 / 116.9246
[2017-12-15 16:24:22] Epoch 0080 mean train/dev loss: 107.8307 / 116.3092
[2017-12-15 16:24:22] Checkpointing model at epoch 80 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:24:22] Model Checkpointing finished.
[2017-12-15 16:24:30] Epoch 0081 mean train/dev loss: 107.7435 / 114.4458
[2017-12-15 16:24:38] Epoch 0082 mean train/dev loss: 107.7980 / 117.9643
[2017-12-15 16:24:47] Epoch 0083 mean train/dev loss: 107.7424 / 116.1128
[2017-12-15 16:24:55] Epoch 0084 mean train/dev loss: 107.7249 / 114.1338
[2017-12-15 16:25:03] Epoch 0085 mean train/dev loss: 107.8630 / 116.2407
[2017-12-15 16:25:11] Epoch 0086 mean train/dev loss: 107.7358 / 114.8448
[2017-12-15 16:25:19] Epoch 0087 mean train/dev loss: 107.7525 / 115.4790
[2017-12-15 16:25:27] Epoch 0088 mean train/dev loss: 107.7934 / 114.8562
[2017-12-15 16:25:35] Epoch 0089 mean train/dev loss: 107.8393 / 115.1467
[2017-12-15 16:25:43] Epoch 0090 mean train/dev loss: 107.7365 / 114.8904
[2017-12-15 16:25:43] Learning rate decayed by 0.5000
[2017-12-15 16:25:43] Checkpointing model at epoch 90 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:25:44] Model Checkpointing finished.
[2017-12-15 16:25:52] Epoch 0091 mean train/dev loss: 107.4462 / 114.4403
[2017-12-15 16:26:00] Epoch 0092 mean train/dev loss: 107.3880 / 115.9239
[2017-12-15 16:26:08] Epoch 0093 mean train/dev loss: 107.3529 / 115.0909
[2017-12-15 16:26:16] Epoch 0094 mean train/dev loss: 107.3053 / 115.7151
[2017-12-15 16:26:24] Epoch 0095 mean train/dev loss: 107.2832 / 114.5969
[2017-12-15 16:26:32] Epoch 0096 mean train/dev loss: 107.2802 / 115.1933
[2017-12-15 16:26:39] Epoch 0097 mean train/dev loss: 107.2360 / 116.4324
[2017-12-15 16:26:46] Epoch 0098 mean train/dev loss: 107.2038 / 116.3620
[2017-12-15 16:26:53] Epoch 0099 mean train/dev loss: 107.2149 / 115.8042
[2017-12-15 16:26:59] Epoch 0100 mean train/dev loss: 107.1739 / 113.8755
[2017-12-15 16:26:59] Checkpointing model at epoch 100 for ffn.hl_100_100.lr_0.1.wd_1.0
[2017-12-15 16:27:00] Model Checkpointing finished.
[2017-12-15 16:27:06] Epoch 0101 mean train/dev loss: 107.0623 / 116.2232
[2017-12-15 16:27:14] Epoch 0102 mean train/dev loss: 106.9862 / 115.4220
[2017-12-15 16:27:20] Epoch 0103 mean train/dev loss: 106.9121 / 116.8292
[2017-12-15 16:27:26] Epoch 0104 mean train/dev loss: 106.8882 / 116.9059
[2017-12-15 16:27:31] Epoch 0105 mean train/dev loss: 106.9620 / 115.6399
[2017-12-15 16:27:31] Learning rate decayed by 0.5000
[2017-12-15 16:27:37] Epoch 0106 mean train/dev loss: 106.6866 / 114.1981
[2017-12-15 16:27:37] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 16:27:37] 
                       *** Training finished *** 
[2017-12-15 16:27:38] Dev MSE: 114.1981
[2017-12-15 16:27:43] Training MSE: 106.7471
[2017-12-15 16:27:44] Experiment ffn.hl_100_100.lr_0.1.wd_1.0 logging ended.
