[2017-12-15 14:20:16] Experiment ffn.hl_linear.lr_0.1.wd_0.01 logging started.
[2017-12-15 14:20:16] 
                       *** Starting Experiment ffn.hl_linear.lr_0.1.wd_0.01 ***
                      
[2017-12-15 14:20:16] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] []  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 14:20:19] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 1)
                      )
[2017-12-15 14:20:19]  *** Training on GPU ***
[2017-12-15 14:20:26] Epoch 0001 mean train/dev loss: 286492.3064 / 245082.2344
[2017-12-15 14:20:31] Epoch 0002 mean train/dev loss: 214288.0720 / 184860.7500
[2017-12-15 14:20:36] Epoch 0003 mean train/dev loss: 162380.8110 / 138636.0469
[2017-12-15 14:20:41] Epoch 0004 mean train/dev loss: 120584.8216 / 101281.5781
[2017-12-15 14:20:47] Epoch 0005 mean train/dev loss: 86744.7713 / 71331.1328
[2017-12-15 14:20:51] Epoch 0006 mean train/dev loss: 59915.5290 / 47924.0781
[2017-12-15 14:20:57] Epoch 0007 mean train/dev loss: 39288.0887 / 30349.9492
[2017-12-15 14:21:03] Epoch 0008 mean train/dev loss: 24103.0908 / 17779.7891
[2017-12-15 14:21:10] Epoch 0009 mean train/dev loss: 13572.2919 / 9419.4541
[2017-12-15 14:21:17] Epoch 0010 mean train/dev loss: 6847.4428 / 4388.9766
[2017-12-15 14:21:17] Checkpointing model at epoch 10 for ffn.hl_linear.lr_0.1.wd_0.01
[2017-12-15 14:21:17] Model Checkpointing finished.
[2017-12-15 14:21:24] Epoch 0011 mean train/dev loss: 3023.2781 / 1763.1829
[2017-12-15 14:21:29] Epoch 0012 mean train/dev loss: 1171.1264 / 637.3574
[2017-12-15 14:21:36] Epoch 0013 mean train/dev loss: 449.4206 / 272.9896
[2017-12-15 14:21:43] Epoch 0014 mean train/dev loss: 236.6251 / 187.3579
[2017-12-15 14:21:47] Epoch 0015 mean train/dev loss: 190.2035 / 174.4653
[2017-12-15 14:21:47] Learning rate decayed by 0.5000
[2017-12-15 14:21:52] Epoch 0016 mean train/dev loss: 182.5152 / 172.4545
[2017-12-15 14:21:57] Epoch 0017 mean train/dev loss: 181.3455 / 171.7015
[2017-12-15 14:22:03] Epoch 0018 mean train/dev loss: 180.9660 / 172.5569
[2017-12-15 14:22:07] Epoch 0019 mean train/dev loss: 180.6575 / 171.7206
[2017-12-15 14:22:12] Epoch 0020 mean train/dev loss: 180.8538 / 171.6478
[2017-12-15 14:22:12] Checkpointing model at epoch 20 for ffn.hl_linear.lr_0.1.wd_0.01
[2017-12-15 14:22:12] Model Checkpointing finished.
[2017-12-15 14:22:17] Epoch 0021 mean train/dev loss: 180.7316 / 172.5721
[2017-12-15 14:22:21] Epoch 0022 mean train/dev loss: 180.9667 / 170.6928
[2017-12-15 14:22:27] Epoch 0023 mean train/dev loss: 180.8171 / 171.4321
[2017-12-15 14:22:33] Epoch 0024 mean train/dev loss: 180.7759 / 171.9472
[2017-12-15 14:22:40] Epoch 0025 mean train/dev loss: 180.9738 / 171.7983
[2017-12-15 14:22:45] Epoch 0026 mean train/dev loss: 180.7231 / 171.4074
[2017-12-15 14:22:50] Epoch 0027 mean train/dev loss: 180.9139 / 170.7538
[2017-12-15 14:22:54] Epoch 0028 mean train/dev loss: 180.7428 / 173.6601
[2017-12-15 14:22:59] Epoch 0029 mean train/dev loss: 180.8942 / 171.4346
[2017-12-15 14:23:05] Epoch 0030 mean train/dev loss: 180.7867 / 171.0783
[2017-12-15 14:23:05] Learning rate decayed by 0.5000
[2017-12-15 14:23:05] Checkpointing model at epoch 30 for ffn.hl_linear.lr_0.1.wd_0.01
[2017-12-15 14:23:05] Model Checkpointing finished.
[2017-12-15 14:23:11] Epoch 0031 mean train/dev loss: 180.8099 / 171.7316
[2017-12-15 14:23:17] Epoch 0032 mean train/dev loss: 180.9082 / 171.5382
[2017-12-15 14:23:21] Epoch 0033 mean train/dev loss: 180.7159 / 171.0495
[2017-12-15 14:23:26] Epoch 0034 mean train/dev loss: 180.8140 / 171.4400
[2017-12-15 14:23:31] Epoch 0035 mean train/dev loss: 180.6758 / 170.8610
[2017-12-15 14:23:36] Epoch 0036 mean train/dev loss: 180.8413 / 171.7080
[2017-12-15 14:23:42] Epoch 0037 mean train/dev loss: 180.8533 / 171.5652
[2017-12-15 14:23:47] Epoch 0038 mean train/dev loss: 180.6795 / 171.4306
[2017-12-15 14:23:53] Epoch 0039 mean train/dev loss: 180.7674 / 171.8647
[2017-12-15 14:23:58] Epoch 0040 mean train/dev loss: 180.8463 / 172.0780
[2017-12-15 14:23:58] Checkpointing model at epoch 40 for ffn.hl_linear.lr_0.1.wd_0.01
[2017-12-15 14:23:58] Model Checkpointing finished.
[2017-12-15 14:24:03] Epoch 0041 mean train/dev loss: 180.6864 / 171.2533
[2017-12-15 14:24:07] Epoch 0042 mean train/dev loss: 181.0097 / 171.6096
[2017-12-15 14:24:12] Epoch 0043 mean train/dev loss: 180.6959 / 172.1596
[2017-12-15 14:24:18] Epoch 0044 mean train/dev loss: 180.7865 / 171.7648
[2017-12-15 14:24:23] Epoch 0045 mean train/dev loss: 180.9138 / 171.0845
[2017-12-15 14:24:23] Learning rate decayed by 0.5000
[2017-12-15 14:24:28] Epoch 0046 mean train/dev loss: 180.5815 / 171.3420
[2017-12-15 14:24:35] Epoch 0047 mean train/dev loss: 180.8284 / 171.9277
[2017-12-15 14:24:40] Epoch 0048 mean train/dev loss: 180.7336 / 171.6280
[2017-12-15 14:24:46] Epoch 0049 mean train/dev loss: 180.7278 / 171.4776
[2017-12-15 14:24:52] Epoch 0050 mean train/dev loss: 180.8104 / 171.1374
[2017-12-15 14:24:52] Checkpointing model at epoch 50 for ffn.hl_linear.lr_0.1.wd_0.01
[2017-12-15 14:24:52] Model Checkpointing finished.
[2017-12-15 14:24:57] Epoch 0051 mean train/dev loss: 180.6736 / 171.2422
[2017-12-15 14:25:04] Epoch 0052 mean train/dev loss: 180.7477 / 171.5496
[2017-12-15 14:25:10] Epoch 0053 mean train/dev loss: 180.6959 / 171.7803
[2017-12-15 14:25:14] Epoch 0054 mean train/dev loss: 180.7316 / 172.1350
[2017-12-15 14:25:19] Epoch 0055 mean train/dev loss: 180.9124 / 172.0201
[2017-12-15 14:25:23] Epoch 0056 mean train/dev loss: 180.7143 / 172.1967
[2017-12-15 14:25:28] Epoch 0057 mean train/dev loss: 180.8498 / 171.4376
[2017-12-15 14:25:35] Epoch 0058 mean train/dev loss: 180.7535 / 171.1819
[2017-12-15 14:25:40] Epoch 0059 mean train/dev loss: 180.7035 / 171.8070
[2017-12-15 14:25:44] Epoch 0060 mean train/dev loss: 180.7219 / 171.6582
[2017-12-15 14:25:44] Learning rate decayed by 0.5000
[2017-12-15 14:25:44] Checkpointing model at epoch 60 for ffn.hl_linear.lr_0.1.wd_0.01
[2017-12-15 14:25:44] Model Checkpointing finished.
[2017-12-15 14:25:49] Epoch 0061 mean train/dev loss: 180.8010 / 171.9324
[2017-12-15 14:25:55] Epoch 0062 mean train/dev loss: 180.7888 / 171.4112
[2017-12-15 14:25:59] Epoch 0063 mean train/dev loss: 180.6491 / 171.8083
[2017-12-15 14:25:59] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:25:59] 
                       *** Training finished *** 
[2017-12-15 14:26:00] Dev MSE: 171.8083
[2017-12-15 14:26:04] Training MSE: 181.1000
[2017-12-15 14:26:05] Experiment ffn.hl_linear.lr_0.1.wd_0.01 logging ended.
