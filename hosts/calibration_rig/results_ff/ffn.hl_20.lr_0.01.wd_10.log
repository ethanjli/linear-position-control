[2017-12-15 16:51:29] Experiment ffn.hl_20.lr_0.01.wd_10 logging started.
[2017-12-15 16:51:29] 
                       *** Starting Experiment ffn.hl_20.lr_0.01.wd_10 ***
                      
[2017-12-15 16:51:29] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 16:51:29] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 1)
                      )
[2017-12-15 16:51:29]  *** Training on GPU ***
[2017-12-15 16:51:36] Epoch 0001 mean train/dev loss: 103567.6738 / 10021.1533
[2017-12-15 16:51:43] Epoch 0002 mean train/dev loss: 3221.1095 / 1559.2632
[2017-12-15 16:51:50] Epoch 0003 mean train/dev loss: 1016.3246 / 1128.7771
[2017-12-15 16:51:57] Epoch 0004 mean train/dev loss: 708.2324 / 827.2561
[2017-12-15 16:52:05] Epoch 0005 mean train/dev loss: 546.1830 / 624.2918
[2017-12-15 16:52:12] Epoch 0006 mean train/dev loss: 454.9746 / 520.1562
[2017-12-15 16:52:20] Epoch 0007 mean train/dev loss: 392.8115 / 441.9393
[2017-12-15 16:52:27] Epoch 0008 mean train/dev loss: 328.1705 / 396.6767
[2017-12-15 16:52:34] Epoch 0009 mean train/dev loss: 286.5766 / 283.4234
[2017-12-15 16:52:42] Epoch 0010 mean train/dev loss: 261.2348 / 295.6555
[2017-12-15 16:52:42] Checkpointing model at epoch 10 for ffn.hl_20.lr_0.01.wd_10
[2017-12-15 16:52:42] Model Checkpointing finished.
[2017-12-15 16:52:49] Epoch 0011 mean train/dev loss: 246.6405 / 238.7222
[2017-12-15 16:52:56] Epoch 0012 mean train/dev loss: 238.9501 / 245.1672
[2017-12-15 16:53:04] Epoch 0013 mean train/dev loss: 232.5811 / 220.4070
[2017-12-15 16:53:11] Epoch 0014 mean train/dev loss: 228.2393 / 226.2761
[2017-12-15 16:53:18] Epoch 0015 mean train/dev loss: 226.3312 / 209.1845
[2017-12-15 16:53:18] Learning rate decayed by 0.5000
[2017-12-15 16:53:25] Epoch 0016 mean train/dev loss: 224.1655 / 213.9095
[2017-12-15 16:53:33] Epoch 0017 mean train/dev loss: 223.7666 / 216.9424
[2017-12-15 16:53:40] Epoch 0018 mean train/dev loss: 223.5325 / 212.9104
[2017-12-15 16:53:48] Epoch 0019 mean train/dev loss: 223.6361 / 209.4774
[2017-12-15 16:53:55] Epoch 0020 mean train/dev loss: 223.2815 / 211.7735
[2017-12-15 16:53:55] Checkpointing model at epoch 20 for ffn.hl_20.lr_0.01.wd_10
[2017-12-15 16:53:56] Model Checkpointing finished.
[2017-12-15 16:54:03] Epoch 0021 mean train/dev loss: 223.2483 / 216.5352
[2017-12-15 16:54:11] Epoch 0022 mean train/dev loss: 223.1421 / 218.5116
[2017-12-15 16:54:18] Epoch 0023 mean train/dev loss: 223.2049 / 213.5174
[2017-12-15 16:54:25] Epoch 0024 mean train/dev loss: 223.2955 / 210.5693
[2017-12-15 16:54:33] Epoch 0025 mean train/dev loss: 223.3564 / 213.7162
[2017-12-15 16:54:40] Epoch 0026 mean train/dev loss: 223.2451 / 205.3977
[2017-12-15 16:54:48] Epoch 0027 mean train/dev loss: 223.2788 / 209.9374
[2017-12-15 16:54:55] Epoch 0028 mean train/dev loss: 223.0274 / 211.6282
[2017-12-15 16:55:03] Epoch 0029 mean train/dev loss: 223.0403 / 212.6179
[2017-12-15 16:55:10] Epoch 0030 mean train/dev loss: 223.2286 / 215.5214
[2017-12-15 16:55:10] Learning rate decayed by 0.5000
[2017-12-15 16:55:10] Checkpointing model at epoch 30 for ffn.hl_20.lr_0.01.wd_10
[2017-12-15 16:55:10] Model Checkpointing finished.
[2017-12-15 16:55:18] Epoch 0031 mean train/dev loss: 222.8918 / 203.4025
[2017-12-15 16:55:25] Epoch 0032 mean train/dev loss: 222.5417 / 210.4457
[2017-12-15 16:55:33] Epoch 0033 mean train/dev loss: 222.5732 / 215.0791
[2017-12-15 16:55:41] Epoch 0034 mean train/dev loss: 222.7406 / 211.3400
[2017-12-15 16:55:48] Epoch 0035 mean train/dev loss: 222.6357 / 213.9186
[2017-12-15 16:55:56] Epoch 0036 mean train/dev loss: 222.8632 / 206.5077
[2017-12-15 16:56:03] Epoch 0037 mean train/dev loss: 222.5681 / 211.9656
[2017-12-15 16:56:11] Epoch 0038 mean train/dev loss: 222.7143 / 210.4632
[2017-12-15 16:56:18] Epoch 0039 mean train/dev loss: 222.6752 / 210.0077
[2017-12-15 16:56:25] Epoch 0040 mean train/dev loss: 222.6783 / 207.0718
[2017-12-15 16:56:25] Checkpointing model at epoch 40 for ffn.hl_20.lr_0.01.wd_10
[2017-12-15 16:56:25] Model Checkpointing finished.
[2017-12-15 16:56:32] Epoch 0041 mean train/dev loss: 222.6010 / 212.9258
[2017-12-15 16:56:40] Epoch 0042 mean train/dev loss: 222.8852 / 210.4956
[2017-12-15 16:56:47] Epoch 0043 mean train/dev loss: 222.5308 / 207.6634
[2017-12-15 16:56:55] Epoch 0044 mean train/dev loss: 222.6410 / 211.1749
[2017-12-15 16:57:02] Epoch 0045 mean train/dev loss: 222.6095 / 216.3032
[2017-12-15 16:57:02] Learning rate decayed by 0.5000
[2017-12-15 16:57:10] Epoch 0046 mean train/dev loss: 222.5499 / 208.7918
[2017-12-15 16:57:17] Epoch 0047 mean train/dev loss: 222.3845 / 207.5632
[2017-12-15 16:57:24] Epoch 0048 mean train/dev loss: 222.3642 / 211.8348
[2017-12-15 16:57:32] Epoch 0049 mean train/dev loss: 222.4983 / 207.3231
[2017-12-15 16:57:39] Epoch 0050 mean train/dev loss: 222.2857 / 209.5239
[2017-12-15 16:57:39] Checkpointing model at epoch 50 for ffn.hl_20.lr_0.01.wd_10
[2017-12-15 16:57:40] Model Checkpointing finished.
[2017-12-15 16:57:47] Epoch 0051 mean train/dev loss: 222.5067 / 209.3375
[2017-12-15 16:57:54] Epoch 0052 mean train/dev loss: 222.2623 / 210.3226
[2017-12-15 16:58:02] Epoch 0053 mean train/dev loss: 222.5357 / 211.2350
[2017-12-15 16:58:09] Epoch 0054 mean train/dev loss: 222.4421 / 210.0632
[2017-12-15 16:58:17] Epoch 0055 mean train/dev loss: 222.4005 / 212.9593
[2017-12-15 16:58:24] Epoch 0056 mean train/dev loss: 222.3867 / 207.7076
[2017-12-15 16:58:32] Epoch 0057 mean train/dev loss: 222.3956 / 210.6621
[2017-12-15 16:58:39] Epoch 0058 mean train/dev loss: 222.3735 / 209.5791
[2017-12-15 16:58:47] Epoch 0059 mean train/dev loss: 222.4061 / 210.8268
[2017-12-15 16:58:54] Epoch 0060 mean train/dev loss: 222.4202 / 212.2545
[2017-12-15 16:58:54] Learning rate decayed by 0.5000
[2017-12-15 16:58:54] Checkpointing model at epoch 60 for ffn.hl_20.lr_0.01.wd_10
[2017-12-15 16:58:54] Model Checkpointing finished.
[2017-12-15 16:59:02] Epoch 0061 mean train/dev loss: 222.2735 / 208.6537
[2017-12-15 16:59:10] Epoch 0062 mean train/dev loss: 222.2713 / 210.8403
[2017-12-15 16:59:17] Epoch 0063 mean train/dev loss: 222.1758 / 209.9397
[2017-12-15 16:59:24] Epoch 0064 mean train/dev loss: 222.1283 / 212.6830
[2017-12-15 16:59:31] Epoch 0065 mean train/dev loss: 222.4966 / 208.9848
[2017-12-15 16:59:38] Epoch 0066 mean train/dev loss: 222.1746 / 209.4291
[2017-12-15 16:59:45] Epoch 0067 mean train/dev loss: 222.1425 / 210.7854
[2017-12-15 16:59:52] Epoch 0068 mean train/dev loss: 222.3916 / 211.1377
[2017-12-15 17:00:00] Epoch 0069 mean train/dev loss: 222.2268 / 211.2387
[2017-12-15 17:00:08] Epoch 0070 mean train/dev loss: 222.3768 / 208.8992
[2017-12-15 17:00:08] Checkpointing model at epoch 70 for ffn.hl_20.lr_0.01.wd_10
[2017-12-15 17:00:08] Model Checkpointing finished.
[2017-12-15 17:00:16] Epoch 0071 mean train/dev loss: 222.1406 / 209.7263
[2017-12-15 17:00:23] Epoch 0072 mean train/dev loss: 222.2748 / 209.5477
[2017-12-15 17:00:23] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:00:23] 
                       *** Training finished *** 
[2017-12-15 17:00:24] Dev MSE: 209.5477
[2017-12-15 17:00:31] Training MSE: 221.0340
[2017-12-15 17:00:32] Experiment ffn.hl_20.lr_0.01.wd_10 logging ended.
