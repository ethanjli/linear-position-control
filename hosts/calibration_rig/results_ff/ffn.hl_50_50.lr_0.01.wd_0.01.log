[2017-12-15 17:41:06] Experiment ffn.hl_50_50.lr_0.01.wd_0.01 logging started.
[2017-12-15 17:41:06] 
                       *** Starting Experiment ffn.hl_50_50.lr_0.01.wd_0.01 ***
                      
[2017-12-15 17:41:06] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 17:41:06] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 1)
                      )
[2017-12-15 17:41:06]  *** Training on GPU ***
[2017-12-15 17:41:14] Epoch 0001 mean train/dev loss: 20938.0463 / 600.7512
[2017-12-15 17:41:22] Epoch 0002 mean train/dev loss: 162.8951 / 269.2509
[2017-12-15 17:41:30] Epoch 0003 mean train/dev loss: 115.9833 / 168.6193
[2017-12-15 17:41:38] Epoch 0004 mean train/dev loss: 109.0832 / 168.8555
[2017-12-15 17:41:46] Epoch 0005 mean train/dev loss: 106.0997 / 163.1000
[2017-12-15 17:41:54] Epoch 0006 mean train/dev loss: 104.2469 / 139.1181
[2017-12-15 17:42:02] Epoch 0007 mean train/dev loss: 103.0737 / 125.9803
[2017-12-15 17:42:10] Epoch 0008 mean train/dev loss: 101.4446 / 148.8530
[2017-12-15 17:42:18] Epoch 0009 mean train/dev loss: 100.5858 / 107.0441
[2017-12-15 17:42:26] Epoch 0010 mean train/dev loss: 98.9422 / 143.5370
[2017-12-15 17:42:26] Checkpointing model at epoch 10 for ffn.hl_50_50.lr_0.01.wd_0.01
[2017-12-15 17:42:27] Model Checkpointing finished.
[2017-12-15 17:42:35] Epoch 0011 mean train/dev loss: 97.5939 / 181.0201
[2017-12-15 17:42:42] Epoch 0012 mean train/dev loss: 95.9997 / 121.5746
[2017-12-15 17:42:50] Epoch 0013 mean train/dev loss: 93.4627 / 113.5935
[2017-12-15 17:42:58] Epoch 0014 mean train/dev loss: 91.4699 / 169.7598
[2017-12-15 17:43:06] Epoch 0015 mean train/dev loss: 89.3149 / 138.4707
[2017-12-15 17:43:06] Learning rate decayed by 0.5000
[2017-12-15 17:43:15] Epoch 0016 mean train/dev loss: 84.2795 / 129.4005
[2017-12-15 17:43:22] Epoch 0017 mean train/dev loss: 83.7557 / 114.2099
[2017-12-15 17:43:30] Epoch 0018 mean train/dev loss: 82.6946 / 114.7605
[2017-12-15 17:43:38] Epoch 0019 mean train/dev loss: 81.9656 / 109.7910
[2017-12-15 17:43:46] Epoch 0020 mean train/dev loss: 80.4338 / 126.0171
[2017-12-15 17:43:46] Checkpointing model at epoch 20 for ffn.hl_50_50.lr_0.01.wd_0.01
[2017-12-15 17:43:46] Model Checkpointing finished.
[2017-12-15 17:43:54] Epoch 0021 mean train/dev loss: 79.4641 / 110.7465
[2017-12-15 17:44:02] Epoch 0022 mean train/dev loss: 77.9887 / 117.0293
[2017-12-15 17:44:10] Epoch 0023 mean train/dev loss: 77.3452 / 109.4610
[2017-12-15 17:44:18] Epoch 0024 mean train/dev loss: 76.7753 / 129.7208
[2017-12-15 17:44:26] Epoch 0025 mean train/dev loss: 76.1922 / 119.5162
[2017-12-15 17:44:34] Epoch 0026 mean train/dev loss: 75.2072 / 126.3126
[2017-12-15 17:44:42] Epoch 0027 mean train/dev loss: 75.2868 / 109.7369
[2017-12-15 17:44:51] Epoch 0028 mean train/dev loss: 74.4359 / 114.0835
[2017-12-15 17:44:59] Epoch 0029 mean train/dev loss: 74.3482 / 141.5968
[2017-12-15 17:45:07] Epoch 0030 mean train/dev loss: 73.7520 / 117.1437
[2017-12-15 17:45:07] Learning rate decayed by 0.5000
[2017-12-15 17:45:07] Checkpointing model at epoch 30 for ffn.hl_50_50.lr_0.01.wd_0.01
[2017-12-15 17:45:07] Model Checkpointing finished.
[2017-12-15 17:45:15] Epoch 0031 mean train/dev loss: 71.9395 / 111.2845
[2017-12-15 17:45:23] Epoch 0032 mean train/dev loss: 71.8696 / 128.3908
[2017-12-15 17:45:31] Epoch 0033 mean train/dev loss: 71.6168 / 124.4601
[2017-12-15 17:45:39] Epoch 0034 mean train/dev loss: 71.4406 / 130.0725
[2017-12-15 17:45:47] Epoch 0035 mean train/dev loss: 71.4915 / 134.1912
[2017-12-15 17:45:55] Epoch 0036 mean train/dev loss: 71.1796 / 113.3590
[2017-12-15 17:46:03] Epoch 0037 mean train/dev loss: 71.0348 / 104.7600
[2017-12-15 17:46:11] Epoch 0038 mean train/dev loss: 70.8965 / 112.2067
[2017-12-15 17:46:19] Epoch 0039 mean train/dev loss: 70.6155 / 122.3790
[2017-12-15 17:46:27] Epoch 0040 mean train/dev loss: 70.4988 / 103.8479
[2017-12-15 17:46:27] Checkpointing model at epoch 40 for ffn.hl_50_50.lr_0.01.wd_0.01
[2017-12-15 17:46:27] Model Checkpointing finished.
[2017-12-15 17:46:35] Epoch 0041 mean train/dev loss: 70.3145 / 116.2780
[2017-12-15 17:46:44] Epoch 0042 mean train/dev loss: 70.0680 / 125.8143
[2017-12-15 17:46:51] Epoch 0043 mean train/dev loss: 70.1114 / 126.5331
[2017-12-15 17:46:59] Epoch 0044 mean train/dev loss: 69.7728 / 105.8193
[2017-12-15 17:47:07] Epoch 0045 mean train/dev loss: 69.5873 / 120.7805
[2017-12-15 17:47:07] Learning rate decayed by 0.5000
[2017-12-15 17:47:15] Epoch 0046 mean train/dev loss: 68.7075 / 119.9815
[2017-12-15 17:47:23] Epoch 0047 mean train/dev loss: 68.5784 / 118.9240
[2017-12-15 17:47:31] Epoch 0048 mean train/dev loss: 68.5718 / 111.2464
[2017-12-15 17:47:39] Epoch 0049 mean train/dev loss: 68.4649 / 118.3159
[2017-12-15 17:47:47] Epoch 0050 mean train/dev loss: 68.3962 / 114.6010
[2017-12-15 17:47:47] Checkpointing model at epoch 50 for ffn.hl_50_50.lr_0.01.wd_0.01
[2017-12-15 17:47:47] Model Checkpointing finished.
[2017-12-15 17:47:55] Epoch 0051 mean train/dev loss: 68.2770 / 110.0550
[2017-12-15 17:48:03] Epoch 0052 mean train/dev loss: 68.1673 / 124.4375
[2017-12-15 17:48:12] Epoch 0053 mean train/dev loss: 68.0559 / 112.1888
[2017-12-15 17:48:20] Epoch 0054 mean train/dev loss: 67.9133 / 123.9216
[2017-12-15 17:48:28] Epoch 0055 mean train/dev loss: 67.8065 / 109.0858
[2017-12-15 17:48:36] Epoch 0056 mean train/dev loss: 67.8031 / 109.8131
[2017-12-15 17:48:44] Epoch 0057 mean train/dev loss: 67.6474 / 111.9763
[2017-12-15 17:48:52] Epoch 0058 mean train/dev loss: 67.6899 / 125.1509
[2017-12-15 17:49:00] Epoch 0059 mean train/dev loss: 67.5823 / 113.7305
[2017-12-15 17:49:08] Epoch 0060 mean train/dev loss: 67.4741 / 118.9074
[2017-12-15 17:49:08] Learning rate decayed by 0.5000
[2017-12-15 17:49:08] Checkpointing model at epoch 60 for ffn.hl_50_50.lr_0.01.wd_0.01
[2017-12-15 17:49:08] Model Checkpointing finished.
[2017-12-15 17:49:16] Epoch 0061 mean train/dev loss: 66.9179 / 120.1721
[2017-12-15 17:49:24] Epoch 0062 mean train/dev loss: 66.8580 / 119.0511
[2017-12-15 17:49:32] Epoch 0063 mean train/dev loss: 66.7343 / 109.2055
[2017-12-15 17:49:40] Epoch 0064 mean train/dev loss: 66.8237 / 116.7383
[2017-12-15 17:49:47] Epoch 0065 mean train/dev loss: 66.7243 / 113.5842
[2017-12-15 17:49:54] Epoch 0066 mean train/dev loss: 66.6036 / 113.3452
[2017-12-15 17:50:00] Epoch 0067 mean train/dev loss: 66.6053 / 111.8131
[2017-12-15 17:50:08] Epoch 0068 mean train/dev loss: 66.6193 / 116.1623
[2017-12-15 17:50:14] Epoch 0069 mean train/dev loss: 66.4871 / 117.0467
[2017-12-15 17:50:22] Epoch 0070 mean train/dev loss: 66.4400 / 111.8353
[2017-12-15 17:50:22] Checkpointing model at epoch 70 for ffn.hl_50_50.lr_0.01.wd_0.01
[2017-12-15 17:50:22] Model Checkpointing finished.
[2017-12-15 17:50:30] Epoch 0071 mean train/dev loss: 66.4003 / 117.4935
[2017-12-15 17:50:35] Epoch 0072 mean train/dev loss: 66.3452 / 115.3422
[2017-12-15 17:50:41] Epoch 0073 mean train/dev loss: 66.2777 / 114.4855
[2017-12-15 17:50:46] Epoch 0074 mean train/dev loss: 66.2356 / 112.6333
[2017-12-15 17:50:51] Epoch 0075 mean train/dev loss: 66.2193 / 123.9989
[2017-12-15 17:50:51] Learning rate decayed by 0.5000
[2017-12-15 17:50:56] Epoch 0076 mean train/dev loss: 65.8822 / 118.4812
[2017-12-15 17:51:02] Epoch 0077 mean train/dev loss: 65.8343 / 115.3842
[2017-12-15 17:51:07] Epoch 0078 mean train/dev loss: 65.7812 / 119.4401
[2017-12-15 17:51:12] Epoch 0079 mean train/dev loss: 65.7827 / 114.9875
[2017-12-15 17:51:17] Epoch 0080 mean train/dev loss: 65.7704 / 112.3702
[2017-12-15 17:51:17] Checkpointing model at epoch 80 for ffn.hl_50_50.lr_0.01.wd_0.01
[2017-12-15 17:51:18] Model Checkpointing finished.
[2017-12-15 17:51:23] Epoch 0081 mean train/dev loss: 65.7176 / 120.5465
[2017-12-15 17:51:23] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:51:23] 
                       *** Training finished *** 
[2017-12-15 17:51:24] Dev MSE: 120.5465
[2017-12-15 17:51:29] Training MSE: 65.6834
[2017-12-15 17:51:30] Experiment ffn.hl_50_50.lr_0.01.wd_0.01 logging ended.
