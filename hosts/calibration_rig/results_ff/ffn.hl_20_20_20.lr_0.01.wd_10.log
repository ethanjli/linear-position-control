[2017-12-15 17:14:02] Experiment ffn.hl_20_20_20.lr_0.01.wd_10 logging started.
[2017-12-15 17:14:02] 
                       *** Starting Experiment ffn.hl_20_20_20.lr_0.01.wd_10 ***
                      
[2017-12-15 17:14:02] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 17:14:02] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 20)
                        (relu3): ReLU ()
                        (linear4): Linear (20 -> 1)
                      )
[2017-12-15 17:14:02]  *** Training on GPU ***
[2017-12-15 17:14:11] Epoch 0001 mean train/dev loss: 20587.2927 / 659.1418
[2017-12-15 17:14:19] Epoch 0002 mean train/dev loss: 281.2459 / 361.9574
[2017-12-15 17:14:28] Epoch 0003 mean train/dev loss: 194.0194 / 256.0229
[2017-12-15 17:14:37] Epoch 0004 mean train/dev loss: 153.2089 / 194.9322
[2017-12-15 17:14:45] Epoch 0005 mean train/dev loss: 127.8792 / 156.5671
[2017-12-15 17:14:54] Epoch 0006 mean train/dev loss: 120.8121 / 133.4765
[2017-12-15 17:15:02] Epoch 0007 mean train/dev loss: 117.3214 / 129.4701
[2017-12-15 17:15:10] Epoch 0008 mean train/dev loss: 116.8839 / 119.5032
[2017-12-15 17:15:19] Epoch 0009 mean train/dev loss: 116.2307 / 114.6487
[2017-12-15 17:15:28] Epoch 0010 mean train/dev loss: 115.7667 / 114.0182
[2017-12-15 17:15:28] Checkpointing model at epoch 10 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:15:28] Model Checkpointing finished.
[2017-12-15 17:15:37] Epoch 0011 mean train/dev loss: 116.1973 / 112.6412
[2017-12-15 17:15:45] Epoch 0012 mean train/dev loss: 116.9598 / 123.6311
[2017-12-15 17:15:54] Epoch 0013 mean train/dev loss: 116.5918 / 117.3745
[2017-12-15 17:16:02] Epoch 0014 mean train/dev loss: 115.8684 / 115.0620
[2017-12-15 17:16:11] Epoch 0015 mean train/dev loss: 116.6372 / 125.7923
[2017-12-15 17:16:11] Learning rate decayed by 0.5000
[2017-12-15 17:16:20] Epoch 0016 mean train/dev loss: 113.5799 / 117.9531
[2017-12-15 17:16:29] Epoch 0017 mean train/dev loss: 113.6700 / 117.2729
[2017-12-15 17:16:38] Epoch 0018 mean train/dev loss: 113.7682 / 118.2348
[2017-12-15 17:16:47] Epoch 0019 mean train/dev loss: 114.1932 / 120.9933
[2017-12-15 17:16:56] Epoch 0020 mean train/dev loss: 113.8827 / 118.6854
[2017-12-15 17:16:56] Checkpointing model at epoch 20 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:16:56] Model Checkpointing finished.
[2017-12-15 17:17:05] Epoch 0021 mean train/dev loss: 113.5301 / 116.5700
[2017-12-15 17:17:15] Epoch 0022 mean train/dev loss: 113.8292 / 127.8244
[2017-12-15 17:17:24] Epoch 0023 mean train/dev loss: 114.0098 / 114.9224
[2017-12-15 17:17:32] Epoch 0024 mean train/dev loss: 114.1323 / 129.1600
[2017-12-15 17:17:41] Epoch 0025 mean train/dev loss: 113.9724 / 116.3718
[2017-12-15 17:17:50] Epoch 0026 mean train/dev loss: 113.8251 / 115.4334
[2017-12-15 17:17:59] Epoch 0027 mean train/dev loss: 113.4976 / 112.6975
[2017-12-15 17:18:08] Epoch 0028 mean train/dev loss: 113.9777 / 115.0847
[2017-12-15 17:18:17] Epoch 0029 mean train/dev loss: 113.8502 / 112.4842
[2017-12-15 17:18:26] Epoch 0030 mean train/dev loss: 113.6910 / 114.4745
[2017-12-15 17:18:26] Learning rate decayed by 0.5000
[2017-12-15 17:18:26] Checkpointing model at epoch 30 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:18:26] Model Checkpointing finished.
[2017-12-15 17:18:35] Epoch 0031 mean train/dev loss: 112.4707 / 112.8444
[2017-12-15 17:18:44] Epoch 0032 mean train/dev loss: 112.3839 / 113.4275
[2017-12-15 17:18:53] Epoch 0033 mean train/dev loss: 112.6890 / 114.7966
[2017-12-15 17:19:02] Epoch 0034 mean train/dev loss: 112.7525 / 115.9401
[2017-12-15 17:19:11] Epoch 0035 mean train/dev loss: 112.7475 / 114.6063
[2017-12-15 17:19:20] Epoch 0036 mean train/dev loss: 112.7300 / 114.9328
[2017-12-15 17:19:29] Epoch 0037 mean train/dev loss: 112.7576 / 115.3671
[2017-12-15 17:19:38] Epoch 0038 mean train/dev loss: 112.5220 / 113.8946
[2017-12-15 17:19:47] Epoch 0039 mean train/dev loss: 112.7763 / 119.5885
[2017-12-15 17:19:56] Epoch 0040 mean train/dev loss: 112.7780 / 113.8760
[2017-12-15 17:19:56] Checkpointing model at epoch 40 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:19:56] Model Checkpointing finished.
[2017-12-15 17:20:06] Epoch 0041 mean train/dev loss: 112.7267 / 120.1043
[2017-12-15 17:20:14] Epoch 0042 mean train/dev loss: 112.5919 / 114.0333
[2017-12-15 17:20:23] Epoch 0043 mean train/dev loss: 112.6592 / 112.8388
[2017-12-15 17:20:32] Epoch 0044 mean train/dev loss: 112.6332 / 120.6496
[2017-12-15 17:20:41] Epoch 0045 mean train/dev loss: 112.7695 / 112.5017
[2017-12-15 17:20:41] Learning rate decayed by 0.5000
[2017-12-15 17:20:50] Epoch 0046 mean train/dev loss: 111.7164 / 115.2089
[2017-12-15 17:20:59] Epoch 0047 mean train/dev loss: 111.9578 / 111.9482
[2017-12-15 17:21:07] Epoch 0048 mean train/dev loss: 111.9096 / 112.8437
[2017-12-15 17:21:16] Epoch 0049 mean train/dev loss: 111.8968 / 112.4542
[2017-12-15 17:21:25] Epoch 0050 mean train/dev loss: 111.9084 / 113.0319
[2017-12-15 17:21:25] Checkpointing model at epoch 50 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:21:26] Model Checkpointing finished.
[2017-12-15 17:21:35] Epoch 0051 mean train/dev loss: 112.0194 / 111.2347
[2017-12-15 17:21:43] Epoch 0052 mean train/dev loss: 111.9442 / 112.2605
[2017-12-15 17:21:52] Epoch 0053 mean train/dev loss: 111.9512 / 112.0098
[2017-12-15 17:22:01] Epoch 0054 mean train/dev loss: 111.9297 / 114.3902
[2017-12-15 17:22:10] Epoch 0055 mean train/dev loss: 111.9959 / 113.0794
[2017-12-15 17:22:18] Epoch 0056 mean train/dev loss: 112.0905 / 114.4425
[2017-12-15 17:22:25] Epoch 0057 mean train/dev loss: 112.0139 / 112.7799
[2017-12-15 17:22:33] Epoch 0058 mean train/dev loss: 111.9668 / 113.9408
[2017-12-15 17:22:40] Epoch 0059 mean train/dev loss: 112.1281 / 111.2733
[2017-12-15 17:22:48] Epoch 0060 mean train/dev loss: 112.0688 / 112.7847
[2017-12-15 17:22:48] Learning rate decayed by 0.5000
[2017-12-15 17:22:48] Checkpointing model at epoch 60 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:22:48] Model Checkpointing finished.
[2017-12-15 17:22:56] Epoch 0061 mean train/dev loss: 111.5717 / 111.9975
[2017-12-15 17:23:04] Epoch 0062 mean train/dev loss: 111.6151 / 113.3318
[2017-12-15 17:23:11] Epoch 0063 mean train/dev loss: 111.6373 / 113.7313
[2017-12-15 17:23:19] Epoch 0064 mean train/dev loss: 111.6731 / 110.7318
[2017-12-15 17:23:27] Epoch 0065 mean train/dev loss: 111.6140 / 111.5118
[2017-12-15 17:23:34] Epoch 0066 mean train/dev loss: 111.6800 / 111.6212
[2017-12-15 17:23:40] Epoch 0067 mean train/dev loss: 111.6182 / 111.6527
[2017-12-15 17:23:46] Epoch 0068 mean train/dev loss: 111.7129 / 110.9065
[2017-12-15 17:23:52] Epoch 0069 mean train/dev loss: 111.6196 / 112.7314
[2017-12-15 17:23:58] Epoch 0070 mean train/dev loss: 111.5837 / 113.4945
[2017-12-15 17:23:58] Checkpointing model at epoch 70 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:23:58] Model Checkpointing finished.
[2017-12-15 17:24:04] Epoch 0071 mean train/dev loss: 111.6086 / 111.1142
[2017-12-15 17:24:10] Epoch 0072 mean train/dev loss: 111.6114 / 113.4858
[2017-12-15 17:24:16] Epoch 0073 mean train/dev loss: 111.6533 / 110.4176
[2017-12-15 17:24:23] Epoch 0074 mean train/dev loss: 111.6448 / 113.2353
[2017-12-15 17:24:29] Epoch 0075 mean train/dev loss: 111.6423 / 110.7095
[2017-12-15 17:24:29] Learning rate decayed by 0.5000
[2017-12-15 17:24:35] Epoch 0076 mean train/dev loss: 111.3832 / 111.5881
[2017-12-15 17:24:41] Epoch 0077 mean train/dev loss: 111.4585 / 111.6633
[2017-12-15 17:24:47] Epoch 0078 mean train/dev loss: 111.4179 / 111.4071
[2017-12-15 17:24:53] Epoch 0079 mean train/dev loss: 111.4306 / 111.2707
[2017-12-15 17:24:59] Epoch 0080 mean train/dev loss: 111.3607 / 111.0089
[2017-12-15 17:24:59] Checkpointing model at epoch 80 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:24:59] Model Checkpointing finished.
[2017-12-15 17:25:05] Epoch 0081 mean train/dev loss: 111.4221 / 112.0055
[2017-12-15 17:25:11] Epoch 0082 mean train/dev loss: 111.4588 / 112.2151
[2017-12-15 17:25:17] Epoch 0083 mean train/dev loss: 111.4256 / 112.8469
[2017-12-15 17:25:23] Epoch 0084 mean train/dev loss: 111.4246 / 111.3603
[2017-12-15 17:25:29] Epoch 0085 mean train/dev loss: 111.4785 / 112.0883
[2017-12-15 17:25:35] Epoch 0086 mean train/dev loss: 111.4640 / 111.3327
[2017-12-15 17:25:40] Epoch 0087 mean train/dev loss: 111.4122 / 111.7033
[2017-12-15 17:25:46] Epoch 0088 mean train/dev loss: 111.4338 / 111.4537
[2017-12-15 17:25:52] Epoch 0089 mean train/dev loss: 111.4636 / 112.0588
[2017-12-15 17:25:58] Epoch 0090 mean train/dev loss: 111.4160 / 111.8285
[2017-12-15 17:25:58] Learning rate decayed by 0.5000
[2017-12-15 17:25:58] Checkpointing model at epoch 90 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:25:58] Model Checkpointing finished.
[2017-12-15 17:26:04] Epoch 0091 mean train/dev loss: 111.3486 / 111.9632
[2017-12-15 17:26:10] Epoch 0092 mean train/dev loss: 111.3207 / 111.4631
[2017-12-15 17:26:16] Epoch 0093 mean train/dev loss: 111.3335 / 112.3343
[2017-12-15 17:26:22] Epoch 0094 mean train/dev loss: 111.3116 / 111.7580
[2017-12-15 17:26:28] Epoch 0095 mean train/dev loss: 111.3302 / 112.1828
[2017-12-15 17:26:34] Epoch 0096 mean train/dev loss: 111.3243 / 112.4332
[2017-12-15 17:26:40] Epoch 0097 mean train/dev loss: 111.2926 / 111.7801
[2017-12-15 17:26:46] Epoch 0098 mean train/dev loss: 111.3279 / 111.5810
[2017-12-15 17:26:52] Epoch 0099 mean train/dev loss: 111.3434 / 111.8673
[2017-12-15 17:26:58] Epoch 0100 mean train/dev loss: 111.3167 / 111.6158
[2017-12-15 17:26:58] Checkpointing model at epoch 100 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:26:58] Model Checkpointing finished.
[2017-12-15 17:27:04] Epoch 0101 mean train/dev loss: 111.3512 / 112.4010
[2017-12-15 17:27:10] Epoch 0102 mean train/dev loss: 111.3072 / 111.0692
[2017-12-15 17:27:16] Epoch 0103 mean train/dev loss: 111.3395 / 112.0093
[2017-12-15 17:27:22] Epoch 0104 mean train/dev loss: 111.3519 / 110.8546
[2017-12-15 17:27:28] Epoch 0105 mean train/dev loss: 111.3415 / 112.0683
[2017-12-15 17:27:28] Learning rate decayed by 0.5000
[2017-12-15 17:27:34] Epoch 0106 mean train/dev loss: 111.2537 / 111.5131
[2017-12-15 17:27:40] Epoch 0107 mean train/dev loss: 111.2421 / 111.6068
[2017-12-15 17:27:46] Epoch 0108 mean train/dev loss: 111.2610 / 111.6336
[2017-12-15 17:27:51] Epoch 0109 mean train/dev loss: 111.2603 / 111.4619
[2017-12-15 17:27:57] Epoch 0110 mean train/dev loss: 111.2407 / 111.8275
[2017-12-15 17:27:57] Checkpointing model at epoch 110 for ffn.hl_20_20_20.lr_0.01.wd_10
[2017-12-15 17:27:57] Model Checkpointing finished.
[2017-12-15 17:28:03] Epoch 0111 mean train/dev loss: 111.2698 / 111.5047
[2017-12-15 17:28:09] Epoch 0112 mean train/dev loss: 111.2588 / 111.6929
[2017-12-15 17:28:15] Epoch 0113 mean train/dev loss: 111.2553 / 111.2871
[2017-12-15 17:28:20] Epoch 0114 mean train/dev loss: 111.2797 / 111.4504
[2017-12-15 17:28:20] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:28:20] 
                       *** Training finished *** 
[2017-12-15 17:28:21] Dev MSE: 111.4504
[2017-12-15 17:28:26] Training MSE: 111.4335
[2017-12-15 17:28:27] Experiment ffn.hl_20_20_20.lr_0.01.wd_10 logging ended.
