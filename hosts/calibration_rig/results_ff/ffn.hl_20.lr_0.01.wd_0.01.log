[2017-12-15 16:51:28] Experiment ffn.hl_20.lr_0.01.wd_0.01 logging started.
[2017-12-15 16:51:28] 
                       *** Starting Experiment ffn.hl_20.lr_0.01.wd_0.01 ***
                      
[2017-12-15 16:51:28] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 16:51:28] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 1)
                      )
[2017-12-15 16:51:28]  *** Training on GPU ***
[2017-12-15 16:51:36] Epoch 0001 mean train/dev loss: 106824.8088 / 11115.1445
[2017-12-15 16:51:44] Epoch 0002 mean train/dev loss: 2551.1649 / 1429.8575
[2017-12-15 16:51:51] Epoch 0003 mean train/dev loss: 746.3970 / 866.6697
[2017-12-15 16:51:58] Epoch 0004 mean train/dev loss: 462.7927 / 602.7271
[2017-12-15 16:52:06] Epoch 0005 mean train/dev loss: 326.1690 / 481.3322
[2017-12-15 16:52:14] Epoch 0006 mean train/dev loss: 259.0353 / 437.8640
[2017-12-15 16:52:21] Epoch 0007 mean train/dev loss: 223.7052 / 425.0920
[2017-12-15 16:52:29] Epoch 0008 mean train/dev loss: 201.7760 / 357.8202
[2017-12-15 16:52:36] Epoch 0009 mean train/dev loss: 184.5304 / 319.9309
[2017-12-15 16:52:44] Epoch 0010 mean train/dev loss: 169.5399 / 272.2639
[2017-12-15 16:52:44] Checkpointing model at epoch 10 for ffn.hl_20.lr_0.01.wd_0.01
[2017-12-15 16:52:44] Model Checkpointing finished.
[2017-12-15 16:52:51] Epoch 0011 mean train/dev loss: 155.1879 / 242.5116
[2017-12-15 16:52:59] Epoch 0012 mean train/dev loss: 134.5634 / 208.5231
[2017-12-15 16:53:06] Epoch 0013 mean train/dev loss: 127.1838 / 218.6000
[2017-12-15 16:53:14] Epoch 0014 mean train/dev loss: 122.4477 / 192.0330
[2017-12-15 16:53:21] Epoch 0015 mean train/dev loss: 119.0947 / 247.3987
[2017-12-15 16:53:21] Learning rate decayed by 0.5000
[2017-12-15 16:53:29] Epoch 0016 mean train/dev loss: 116.7529 / 198.1819
[2017-12-15 16:53:37] Epoch 0017 mean train/dev loss: 115.6351 / 211.5279
[2017-12-15 16:53:44] Epoch 0018 mean train/dev loss: 114.6756 / 201.6471
[2017-12-15 16:53:51] Epoch 0019 mean train/dev loss: 113.8215 / 194.3600
[2017-12-15 16:53:59] Epoch 0020 mean train/dev loss: 113.0505 / 178.8405
[2017-12-15 16:53:59] Checkpointing model at epoch 20 for ffn.hl_20.lr_0.01.wd_0.01
[2017-12-15 16:54:00] Model Checkpointing finished.
[2017-12-15 16:54:07] Epoch 0021 mean train/dev loss: 112.3527 / 180.0047
[2017-12-15 16:54:14] Epoch 0022 mean train/dev loss: 111.9066 / 180.5633
[2017-12-15 16:54:22] Epoch 0023 mean train/dev loss: 111.4862 / 168.5416
[2017-12-15 16:54:29] Epoch 0024 mean train/dev loss: 111.1273 / 171.1327
[2017-12-15 16:54:36] Epoch 0025 mean train/dev loss: 110.8145 / 166.3520
[2017-12-15 16:54:43] Epoch 0026 mean train/dev loss: 110.5556 / 190.0768
[2017-12-15 16:54:50] Epoch 0027 mean train/dev loss: 110.0489 / 187.0413
[2017-12-15 16:54:58] Epoch 0028 mean train/dev loss: 109.8620 / 185.6974
[2017-12-15 16:55:05] Epoch 0029 mean train/dev loss: 109.6026 / 189.3879
[2017-12-15 16:55:13] Epoch 0030 mean train/dev loss: 109.3575 / 170.2730
[2017-12-15 16:55:13] Learning rate decayed by 0.5000
[2017-12-15 16:55:13] Checkpointing model at epoch 30 for ffn.hl_20.lr_0.01.wd_0.01
[2017-12-15 16:55:13] Model Checkpointing finished.
[2017-12-15 16:55:21] Epoch 0031 mean train/dev loss: 108.8044 / 167.2019
[2017-12-15 16:55:28] Epoch 0032 mean train/dev loss: 108.6922 / 172.3842
[2017-12-15 16:55:36] Epoch 0033 mean train/dev loss: 108.5972 / 164.0138
[2017-12-15 16:55:43] Epoch 0034 mean train/dev loss: 108.5327 / 165.2458
[2017-12-15 16:55:50] Epoch 0035 mean train/dev loss: 108.4414 / 163.5548
[2017-12-15 16:55:58] Epoch 0036 mean train/dev loss: 108.3400 / 164.6530
[2017-12-15 16:56:05] Epoch 0037 mean train/dev loss: 108.2049 / 169.7678
[2017-12-15 16:56:13] Epoch 0038 mean train/dev loss: 108.1817 / 162.7884
[2017-12-15 16:56:20] Epoch 0039 mean train/dev loss: 108.1583 / 165.5587
[2017-12-15 16:56:27] Epoch 0040 mean train/dev loss: 108.0733 / 161.8284
[2017-12-15 16:56:27] Checkpointing model at epoch 40 for ffn.hl_20.lr_0.01.wd_0.01
[2017-12-15 16:56:28] Model Checkpointing finished.
[2017-12-15 16:56:35] Epoch 0041 mean train/dev loss: 107.9765 / 161.9992
[2017-12-15 16:56:42] Epoch 0042 mean train/dev loss: 107.9107 / 165.7731
[2017-12-15 16:56:49] Epoch 0043 mean train/dev loss: 107.8449 / 153.3631
[2017-12-15 16:56:57] Epoch 0044 mean train/dev loss: 107.7729 / 164.7321
[2017-12-15 16:57:04] Epoch 0045 mean train/dev loss: 107.6904 / 156.2428
[2017-12-15 16:57:04] Learning rate decayed by 0.5000
[2017-12-15 16:57:11] Epoch 0046 mean train/dev loss: 107.5081 / 163.8675
[2017-12-15 16:57:19] Epoch 0047 mean train/dev loss: 107.4540 / 161.3939
[2017-12-15 16:57:26] Epoch 0048 mean train/dev loss: 107.4357 / 165.4067
[2017-12-15 16:57:33] Epoch 0049 mean train/dev loss: 107.3992 / 164.1846
[2017-12-15 16:57:41] Epoch 0050 mean train/dev loss: 107.3716 / 158.5583
[2017-12-15 16:57:41] Checkpointing model at epoch 50 for ffn.hl_20.lr_0.01.wd_0.01
[2017-12-15 16:57:42] Model Checkpointing finished.
[2017-12-15 16:57:49] Epoch 0051 mean train/dev loss: 107.3531 / 163.7885
[2017-12-15 16:57:57] Epoch 0052 mean train/dev loss: 107.3733 / 158.6132
[2017-12-15 16:58:04] Epoch 0053 mean train/dev loss: 107.3135 / 163.2499
[2017-12-15 16:58:12] Epoch 0054 mean train/dev loss: 107.3141 / 165.3263
[2017-12-15 16:58:19] Epoch 0055 mean train/dev loss: 107.2656 / 159.6977
[2017-12-15 16:58:27] Epoch 0056 mean train/dev loss: 107.1951 / 161.6059
[2017-12-15 16:58:34] Epoch 0057 mean train/dev loss: 107.2173 / 163.1628
[2017-12-15 16:58:41] Epoch 0058 mean train/dev loss: 107.1988 / 162.6036
[2017-12-15 16:58:49] Epoch 0059 mean train/dev loss: 107.1615 / 165.8057
[2017-12-15 16:58:56] Epoch 0060 mean train/dev loss: 107.1640 / 162.2513
[2017-12-15 16:58:56] Learning rate decayed by 0.5000
[2017-12-15 16:58:56] Checkpointing model at epoch 60 for ffn.hl_20.lr_0.01.wd_0.01
[2017-12-15 16:58:56] Model Checkpointing finished.
[2017-12-15 16:59:03] Epoch 0061 mean train/dev loss: 107.0033 / 165.4844
[2017-12-15 16:59:10] Epoch 0062 mean train/dev loss: 106.9959 / 162.5250
[2017-12-15 16:59:17] Epoch 0063 mean train/dev loss: 106.9944 / 165.9809
[2017-12-15 16:59:24] Epoch 0064 mean train/dev loss: 106.9784 / 160.9591
[2017-12-15 16:59:32] Epoch 0065 mean train/dev loss: 106.9746 / 162.1760
[2017-12-15 16:59:40] Epoch 0066 mean train/dev loss: 106.9771 / 159.5051
[2017-12-15 16:59:47] Epoch 0067 mean train/dev loss: 106.9334 / 162.5001
[2017-12-15 16:59:55] Epoch 0068 mean train/dev loss: 106.9471 / 166.0328
[2017-12-15 17:00:02] Epoch 0069 mean train/dev loss: 106.9543 / 165.1625
[2017-12-15 17:00:09] Epoch 0070 mean train/dev loss: 106.9267 / 164.6812
[2017-12-15 17:00:09] Checkpointing model at epoch 70 for ffn.hl_20.lr_0.01.wd_0.01
[2017-12-15 17:00:09] Model Checkpointing finished.
[2017-12-15 17:00:16] Epoch 0071 mean train/dev loss: 106.8942 / 163.5464
[2017-12-15 17:00:23] Epoch 0072 mean train/dev loss: 106.8847 / 164.9668
[2017-12-15 17:00:31] Epoch 0073 mean train/dev loss: 106.8820 / 165.1490
[2017-12-15 17:00:37] Epoch 0074 mean train/dev loss: 106.9118 / 160.3387
[2017-12-15 17:00:43] Epoch 0075 mean train/dev loss: 106.8499 / 164.8289
[2017-12-15 17:00:43] Learning rate decayed by 0.5000
[2017-12-15 17:00:48] Epoch 0076 mean train/dev loss: 106.8003 / 163.3703
[2017-12-15 17:00:53] Epoch 0077 mean train/dev loss: 106.8070 / 164.3309
[2017-12-15 17:00:58] Epoch 0078 mean train/dev loss: 106.7748 / 162.5203
[2017-12-15 17:01:03] Epoch 0079 mean train/dev loss: 106.7845 / 164.5378
[2017-12-15 17:01:08] Epoch 0080 mean train/dev loss: 106.7539 / 164.4438
[2017-12-15 17:01:08] Checkpointing model at epoch 80 for ffn.hl_20.lr_0.01.wd_0.01
[2017-12-15 17:01:09] Model Checkpointing finished.
[2017-12-15 17:01:14] Epoch 0081 mean train/dev loss: 106.7914 / 162.4565
[2017-12-15 17:01:19] Epoch 0082 mean train/dev loss: 106.7599 / 165.2559
[2017-12-15 17:01:24] Epoch 0083 mean train/dev loss: 106.7602 / 163.8187
[2017-12-15 17:01:29] Epoch 0084 mean train/dev loss: 106.7654 / 161.9810
[2017-12-15 17:01:29] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:01:29] 
                       *** Training finished *** 
[2017-12-15 17:01:30] Dev MSE: 161.9810
[2017-12-15 17:01:35] Training MSE: 106.7226
[2017-12-15 17:01:36] Experiment ffn.hl_20.lr_0.01.wd_0.01 logging ended.
