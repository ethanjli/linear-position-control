[2017-12-15 16:01:47] Experiment ffn.hl_100.lr_0.1.wd_0.001 logging started.
[2017-12-15 16:01:47] 
                       *** Starting Experiment ffn.hl_100.lr_0.1.wd_0.001 ***
                      
[2017-12-15 16:01:47] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 16:01:47] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 1)
                      )
[2017-12-15 16:01:47]  *** Training on GPU ***
[2017-12-15 16:01:54] Epoch 0001 mean train/dev loss: 7848.0801 / 299.8036
[2017-12-15 16:02:02] Epoch 0002 mean train/dev loss: 153.7335 / 188.9974
[2017-12-15 16:02:09] Epoch 0003 mean train/dev loss: 132.5935 / 227.7060
[2017-12-15 16:02:16] Epoch 0004 mean train/dev loss: 125.4632 / 176.8079
[2017-12-15 16:02:24] Epoch 0005 mean train/dev loss: 121.5417 / 166.8479
[2017-12-15 16:02:31] Epoch 0006 mean train/dev loss: 116.4158 / 188.8953
[2017-12-15 16:02:39] Epoch 0007 mean train/dev loss: 116.5415 / 119.1058
[2017-12-15 16:02:46] Epoch 0008 mean train/dev loss: 114.6315 / 127.9129
[2017-12-15 16:02:53] Epoch 0009 mean train/dev loss: 110.3818 / 135.8705
[2017-12-15 16:03:00] Epoch 0010 mean train/dev loss: 112.1659 / 137.0859
[2017-12-15 16:03:00] Checkpointing model at epoch 10 for ffn.hl_100.lr_0.1.wd_0.001
[2017-12-15 16:03:00] Model Checkpointing finished.
[2017-12-15 16:03:08] Epoch 0011 mean train/dev loss: 105.1860 / 153.5706
[2017-12-15 16:03:15] Epoch 0012 mean train/dev loss: 98.9621 / 156.1038
[2017-12-15 16:03:22] Epoch 0013 mean train/dev loss: 96.8810 / 198.6935
[2017-12-15 16:03:29] Epoch 0014 mean train/dev loss: 89.4281 / 202.0964
[2017-12-15 16:03:36] Epoch 0015 mean train/dev loss: 85.7985 / 155.1992
[2017-12-15 16:03:36] Learning rate decayed by 0.5000
[2017-12-15 16:03:44] Epoch 0016 mean train/dev loss: 76.6483 / 173.4467
[2017-12-15 16:03:51] Epoch 0017 mean train/dev loss: 75.8095 / 132.9755
[2017-12-15 16:03:58] Epoch 0018 mean train/dev loss: 75.5192 / 163.3052
[2017-12-15 16:04:06] Epoch 0019 mean train/dev loss: 74.4452 / 122.7535
[2017-12-15 16:04:13] Epoch 0020 mean train/dev loss: 73.8998 / 126.2689
[2017-12-15 16:04:13] Checkpointing model at epoch 20 for ffn.hl_100.lr_0.1.wd_0.001
[2017-12-15 16:04:13] Model Checkpointing finished.
[2017-12-15 16:04:21] Epoch 0021 mean train/dev loss: 72.6480 / 110.8335
[2017-12-15 16:04:28] Epoch 0022 mean train/dev loss: 72.2081 / 102.7800
[2017-12-15 16:04:35] Epoch 0023 mean train/dev loss: 72.2354 / 105.6881
[2017-12-15 16:04:42] Epoch 0024 mean train/dev loss: 71.4792 / 103.4164
[2017-12-15 16:04:49] Epoch 0025 mean train/dev loss: 71.0509 / 121.9872
[2017-12-15 16:04:56] Epoch 0026 mean train/dev loss: 71.0911 / 110.6817
[2017-12-15 16:05:04] Epoch 0027 mean train/dev loss: 70.2441 / 96.1360
[2017-12-15 16:05:11] Epoch 0028 mean train/dev loss: 68.3798 / 111.7354
[2017-12-15 16:05:18] Epoch 0029 mean train/dev loss: 66.7104 / 106.0720
[2017-12-15 16:05:25] Epoch 0030 mean train/dev loss: 64.5879 / 77.2660
[2017-12-15 16:05:25] Learning rate decayed by 0.5000
[2017-12-15 16:05:25] Checkpointing model at epoch 30 for ffn.hl_100.lr_0.1.wd_0.001
[2017-12-15 16:05:26] Model Checkpointing finished.
[2017-12-15 16:05:33] Epoch 0031 mean train/dev loss: 59.9853 / 84.4580
[2017-12-15 16:05:40] Epoch 0032 mean train/dev loss: 59.5971 / 81.6894
[2017-12-15 16:05:47] Epoch 0033 mean train/dev loss: 58.8358 / 87.8271
[2017-12-15 16:05:55] Epoch 0034 mean train/dev loss: 58.4670 / 98.5758
[2017-12-15 16:06:02] Epoch 0035 mean train/dev loss: 58.4410 / 85.6902
[2017-12-15 16:06:09] Epoch 0036 mean train/dev loss: 57.5902 / 86.8232
[2017-12-15 16:06:17] Epoch 0037 mean train/dev loss: 57.5964 / 82.5979
[2017-12-15 16:06:24] Epoch 0038 mean train/dev loss: 57.5333 / 92.9179
[2017-12-15 16:06:32] Epoch 0039 mean train/dev loss: 57.4586 / 88.9894
[2017-12-15 16:06:39] Epoch 0040 mean train/dev loss: 57.9551 / 85.3690
[2017-12-15 16:06:39] Checkpointing model at epoch 40 for ffn.hl_100.lr_0.1.wd_0.001
[2017-12-15 16:06:39] Model Checkpointing finished.
[2017-12-15 16:06:47] Epoch 0041 mean train/dev loss: 57.0631 / 90.9660
[2017-12-15 16:06:54] Epoch 0042 mean train/dev loss: 56.7560 / 85.5011
[2017-12-15 16:07:01] Epoch 0043 mean train/dev loss: 57.6079 / 90.0756
[2017-12-15 16:07:08] Epoch 0044 mean train/dev loss: 56.7961 / 110.2381
[2017-12-15 16:07:16] Epoch 0045 mean train/dev loss: 56.8828 / 98.3291
[2017-12-15 16:07:16] Learning rate decayed by 0.5000
[2017-12-15 16:07:23] Epoch 0046 mean train/dev loss: 55.2485 / 86.4381
[2017-12-15 16:07:30] Epoch 0047 mean train/dev loss: 55.3168 / 74.7183
[2017-12-15 16:07:37] Epoch 0048 mean train/dev loss: 55.2484 / 91.2018
[2017-12-15 16:07:44] Epoch 0049 mean train/dev loss: 55.3568 / 87.3467
[2017-12-15 16:07:51] Epoch 0050 mean train/dev loss: 55.2184 / 89.5013
[2017-12-15 16:07:51] Checkpointing model at epoch 50 for ffn.hl_100.lr_0.1.wd_0.001
[2017-12-15 16:07:52] Model Checkpointing finished.
[2017-12-15 16:07:59] Epoch 0051 mean train/dev loss: 55.4547 / 81.2997
[2017-12-15 16:08:06] Epoch 0052 mean train/dev loss: 55.2573 / 87.8130
[2017-12-15 16:08:13] Epoch 0053 mean train/dev loss: 55.3098 / 91.6259
[2017-12-15 16:08:20] Epoch 0054 mean train/dev loss: 55.1728 / 87.0199
[2017-12-15 16:08:27] Epoch 0055 mean train/dev loss: 55.1292 / 85.8149
[2017-12-15 16:08:35] Epoch 0056 mean train/dev loss: 55.2357 / 88.3095
[2017-12-15 16:08:43] Epoch 0057 mean train/dev loss: 55.0317 / 92.5533
[2017-12-15 16:08:50] Epoch 0058 mean train/dev loss: 55.0857 / 92.2930
[2017-12-15 16:08:56] Epoch 0059 mean train/dev loss: 55.0445 / 81.8463
[2017-12-15 16:09:02] Epoch 0060 mean train/dev loss: 54.9586 / 91.0752
[2017-12-15 16:09:02] Learning rate decayed by 0.5000
[2017-12-15 16:09:02] Checkpointing model at epoch 60 for ffn.hl_100.lr_0.1.wd_0.001
[2017-12-15 16:09:03] Model Checkpointing finished.
[2017-12-15 16:09:09] Epoch 0061 mean train/dev loss: 54.0718 / 88.0582
[2017-12-15 16:09:15] Epoch 0062 mean train/dev loss: 54.2031 / 82.6745
[2017-12-15 16:09:20] Epoch 0063 mean train/dev loss: 54.1058 / 90.1769
[2017-12-15 16:09:25] Epoch 0064 mean train/dev loss: 54.1511 / 87.8216
[2017-12-15 16:09:30] Epoch 0065 mean train/dev loss: 54.0911 / 83.9615
[2017-12-15 16:09:35] Epoch 0066 mean train/dev loss: 54.0542 / 85.2866
[2017-12-15 16:09:40] Epoch 0067 mean train/dev loss: 54.2327 / 97.3108
[2017-12-15 16:09:45] Epoch 0068 mean train/dev loss: 54.1282 / 88.4002
[2017-12-15 16:09:50] Epoch 0069 mean train/dev loss: 54.1614 / 87.2747
[2017-12-15 16:09:55] Epoch 0070 mean train/dev loss: 54.0247 / 94.9229
[2017-12-15 16:09:55] Checkpointing model at epoch 70 for ffn.hl_100.lr_0.1.wd_0.001
[2017-12-15 16:09:55] Model Checkpointing finished.
[2017-12-15 16:10:00] Epoch 0071 mean train/dev loss: 54.0077 / 84.0502
[2017-12-15 16:10:05] Epoch 0072 mean train/dev loss: 53.9879 / 89.0824
[2017-12-15 16:10:09] Epoch 0073 mean train/dev loss: 53.9920 / 84.3167
[2017-12-15 16:10:14] Epoch 0074 mean train/dev loss: 53.9906 / 87.7598
[2017-12-15 16:10:19] Epoch 0075 mean train/dev loss: 53.9654 / 85.1614
[2017-12-15 16:10:19] Learning rate decayed by 0.5000
[2017-12-15 16:10:24] Epoch 0076 mean train/dev loss: 53.5201 / 88.2127
[2017-12-15 16:10:29] Epoch 0077 mean train/dev loss: 53.5228 / 86.0469
[2017-12-15 16:10:34] Epoch 0078 mean train/dev loss: 53.5263 / 88.8067
[2017-12-15 16:10:39] Epoch 0079 mean train/dev loss: 53.4894 / 88.0470
[2017-12-15 16:10:44] Epoch 0080 mean train/dev loss: 53.5246 / 89.0931
[2017-12-15 16:10:44] Checkpointing model at epoch 80 for ffn.hl_100.lr_0.1.wd_0.001
[2017-12-15 16:10:44] Model Checkpointing finished.
[2017-12-15 16:10:49] Epoch 0081 mean train/dev loss: 53.4658 / 89.3705
[2017-12-15 16:10:54] Epoch 0082 mean train/dev loss: 53.4849 / 85.6358
[2017-12-15 16:10:58] Epoch 0083 mean train/dev loss: 53.4951 / 85.4441
[2017-12-15 16:11:03] Epoch 0084 mean train/dev loss: 53.4344 / 84.2317
[2017-12-15 16:11:08] Epoch 0085 mean train/dev loss: 53.4573 / 84.4993
[2017-12-15 16:11:13] Epoch 0086 mean train/dev loss: 53.4167 / 81.6512
[2017-12-15 16:11:18] Epoch 0087 mean train/dev loss: 53.4581 / 88.9741
[2017-12-15 16:11:23] Epoch 0088 mean train/dev loss: 53.4064 / 87.0028
[2017-12-15 16:11:23] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 16:11:23] 
                       *** Training finished *** 
[2017-12-15 16:11:24] Dev MSE: 87.0028
[2017-12-15 16:11:28] Training MSE: 53.1039
[2017-12-15 16:11:30] Experiment ffn.hl_100.lr_0.1.wd_0.001 logging ended.
