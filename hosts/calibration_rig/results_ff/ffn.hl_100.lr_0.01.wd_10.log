[2017-12-15 18:07:54] Experiment ffn.hl_100.lr_0.01.wd_10 logging started.
[2017-12-15 18:07:54] 
                       *** Starting Experiment ffn.hl_100.lr_0.01.wd_10 ***
                      
[2017-12-15 18:07:54] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 18:07:54] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 1)
                      )
[2017-12-15 18:07:54]  *** Training on GPU ***
[2017-12-15 18:08:02] Epoch 0001 mean train/dev loss: 51263.3993 / 1183.5461
[2017-12-15 18:08:10] Epoch 0002 mean train/dev loss: 692.6460 / 680.8682
[2017-12-15 18:08:17] Epoch 0003 mean train/dev loss: 454.7927 / 454.3252
[2017-12-15 18:08:24] Epoch 0004 mean train/dev loss: 350.2127 / 347.6347
[2017-12-15 18:08:32] Epoch 0005 mean train/dev loss: 294.3208 / 283.4336
[2017-12-15 18:08:40] Epoch 0006 mean train/dev loss: 263.0667 / 262.2268
[2017-12-15 18:08:47] Epoch 0007 mean train/dev loss: 246.6442 / 233.1449
[2017-12-15 18:08:55] Epoch 0008 mean train/dev loss: 238.3598 / 224.2360
[2017-12-15 18:09:03] Epoch 0009 mean train/dev loss: 233.8436 / 224.6373
[2017-12-15 18:09:10] Epoch 0010 mean train/dev loss: 230.0095 / 209.4199
[2017-12-15 18:09:10] Checkpointing model at epoch 10 for ffn.hl_100.lr_0.01.wd_10
[2017-12-15 18:09:11] Model Checkpointing finished.
[2017-12-15 18:09:18] Epoch 0011 mean train/dev loss: 227.7424 / 218.9515
[2017-12-15 18:09:26] Epoch 0012 mean train/dev loss: 227.1856 / 217.6490
[2017-12-15 18:09:33] Epoch 0013 mean train/dev loss: 227.0590 / 220.9281
[2017-12-15 18:09:40] Epoch 0014 mean train/dev loss: 226.7247 / 219.6848
[2017-12-15 18:09:48] Epoch 0015 mean train/dev loss: 226.4172 / 227.7124
[2017-12-15 18:09:48] Learning rate decayed by 0.5000
[2017-12-15 18:09:55] Epoch 0016 mean train/dev loss: 224.8291 / 218.2327
[2017-12-15 18:10:03] Epoch 0017 mean train/dev loss: 224.7831 / 209.7541
[2017-12-15 18:10:10] Epoch 0018 mean train/dev loss: 224.6529 / 213.0720
[2017-12-15 18:10:18] Epoch 0019 mean train/dev loss: 224.8807 / 206.1160
[2017-12-15 18:10:25] Epoch 0020 mean train/dev loss: 224.7476 / 211.7497
[2017-12-15 18:10:25] Checkpointing model at epoch 20 for ffn.hl_100.lr_0.01.wd_10
[2017-12-15 18:10:26] Model Checkpointing finished.
[2017-12-15 18:10:33] Epoch 0021 mean train/dev loss: 224.8489 / 207.0543
[2017-12-15 18:10:40] Epoch 0022 mean train/dev loss: 224.5762 / 222.2767
[2017-12-15 18:10:47] Epoch 0023 mean train/dev loss: 224.4524 / 210.2688
[2017-12-15 18:10:54] Epoch 0024 mean train/dev loss: 224.3997 / 215.8022
[2017-12-15 18:11:02] Epoch 0025 mean train/dev loss: 224.5074 / 206.3240
[2017-12-15 18:11:09] Epoch 0026 mean train/dev loss: 224.6319 / 207.3952
[2017-12-15 18:11:16] Epoch 0027 mean train/dev loss: 224.6116 / 220.2039
[2017-12-15 18:11:24] Epoch 0028 mean train/dev loss: 224.0973 / 214.3203
[2017-12-15 18:11:31] Epoch 0029 mean train/dev loss: 224.3761 / 223.4752
[2017-12-15 18:11:38] Epoch 0030 mean train/dev loss: 224.5627 / 210.5679
[2017-12-15 18:11:38] Learning rate decayed by 0.5000
[2017-12-15 18:11:38] Checkpointing model at epoch 30 for ffn.hl_100.lr_0.01.wd_10
[2017-12-15 18:11:39] Model Checkpointing finished.
[2017-12-15 18:11:47] Epoch 0031 mean train/dev loss: 223.1177 / 203.5788
[2017-12-15 18:11:54] Epoch 0032 mean train/dev loss: 223.2765 / 209.3728
[2017-12-15 18:12:01] Epoch 0033 mean train/dev loss: 223.4223 / 212.6541
[2017-12-15 18:12:09] Epoch 0034 mean train/dev loss: 223.4744 / 209.2456
[2017-12-15 18:12:17] Epoch 0035 mean train/dev loss: 223.4637 / 211.8871
[2017-12-15 18:12:24] Epoch 0036 mean train/dev loss: 223.4159 / 206.9893
[2017-12-15 18:12:32] Epoch 0037 mean train/dev loss: 223.2567 / 211.6662
[2017-12-15 18:12:39] Epoch 0038 mean train/dev loss: 223.3646 / 205.1316
[2017-12-15 18:12:46] Epoch 0039 mean train/dev loss: 223.2114 / 215.4519
[2017-12-15 18:12:54] Epoch 0040 mean train/dev loss: 223.2545 / 213.8633
[2017-12-15 18:12:54] Checkpointing model at epoch 40 for ffn.hl_100.lr_0.01.wd_10
[2017-12-15 18:12:54] Model Checkpointing finished.
[2017-12-15 18:13:01] Epoch 0041 mean train/dev loss: 223.4666 / 215.7300
[2017-12-15 18:13:09] Epoch 0042 mean train/dev loss: 223.3511 / 210.2040
[2017-12-15 18:13:16] Epoch 0043 mean train/dev loss: 223.2717 / 214.7016
[2017-12-15 18:13:23] Epoch 0044 mean train/dev loss: 223.1860 / 217.9219
[2017-12-15 18:13:30] Epoch 0045 mean train/dev loss: 223.3029 / 207.4582
[2017-12-15 18:13:30] Learning rate decayed by 0.5000
[2017-12-15 18:13:38] Epoch 0046 mean train/dev loss: 222.8301 / 206.0459
[2017-12-15 18:13:45] Epoch 0047 mean train/dev loss: 222.5528 / 214.9810
[2017-12-15 18:13:52] Epoch 0048 mean train/dev loss: 222.8816 / 211.0188
[2017-12-15 18:14:00] Epoch 0049 mean train/dev loss: 222.7913 / 216.2790
[2017-12-15 18:14:07] Epoch 0050 mean train/dev loss: 222.7496 / 211.7800
[2017-12-15 18:14:07] Checkpointing model at epoch 50 for ffn.hl_100.lr_0.01.wd_10
[2017-12-15 18:14:08] Model Checkpointing finished.
[2017-12-15 18:14:15] Epoch 0051 mean train/dev loss: 222.8127 / 211.7879
[2017-12-15 18:14:21] Epoch 0052 mean train/dev loss: 222.8239 / 207.6230
[2017-12-15 18:14:28] Epoch 0053 mean train/dev loss: 222.6126 / 211.0331
[2017-12-15 18:14:34] Epoch 0054 mean train/dev loss: 222.8541 / 212.1100
[2017-12-15 18:14:40] Epoch 0055 mean train/dev loss: 222.8169 / 205.9934
[2017-12-15 18:14:46] Epoch 0056 mean train/dev loss: 222.8129 / 213.6919
[2017-12-15 18:14:52] Epoch 0057 mean train/dev loss: 222.7818 / 206.3544
[2017-12-15 18:14:58] Epoch 0058 mean train/dev loss: 222.6927 / 212.7746
[2017-12-15 18:15:04] Epoch 0059 mean train/dev loss: 222.6871 / 212.6393
[2017-12-15 18:15:10] Epoch 0060 mean train/dev loss: 222.7870 / 210.9964
[2017-12-15 18:15:10] Learning rate decayed by 0.5000
[2017-12-15 18:15:10] Checkpointing model at epoch 60 for ffn.hl_100.lr_0.01.wd_10
[2017-12-15 18:15:10] Model Checkpointing finished.
[2017-12-15 18:15:17] Epoch 0061 mean train/dev loss: 222.5450 / 212.4756
[2017-12-15 18:15:23] Epoch 0062 mean train/dev loss: 222.5290 / 209.0768
[2017-12-15 18:15:30] Epoch 0063 mean train/dev loss: 222.3747 / 213.9798
[2017-12-15 18:15:36] Epoch 0064 mean train/dev loss: 222.5422 / 216.1216
[2017-12-15 18:15:42] Epoch 0065 mean train/dev loss: 222.4667 / 207.4588
[2017-12-15 18:15:49] Epoch 0066 mean train/dev loss: 222.5711 / 212.9712
[2017-12-15 18:15:55] Epoch 0067 mean train/dev loss: 222.4408 / 209.5765
[2017-12-15 18:16:02] Epoch 0068 mean train/dev loss: 222.4702 / 209.0207
[2017-12-15 18:16:08] Epoch 0069 mean train/dev loss: 222.4878 / 206.8506
[2017-12-15 18:16:15] Epoch 0070 mean train/dev loss: 222.3374 / 209.7176
[2017-12-15 18:16:15] Checkpointing model at epoch 70 for ffn.hl_100.lr_0.01.wd_10
[2017-12-15 18:16:15] Model Checkpointing finished.
[2017-12-15 18:16:21] Epoch 0071 mean train/dev loss: 222.3874 / 212.4895
[2017-12-15 18:16:27] Epoch 0072 mean train/dev loss: 222.6995 / 207.6597
[2017-12-15 18:16:27] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:16:27] 
                       *** Training finished *** 
[2017-12-15 18:16:28] Dev MSE: 207.6597
[2017-12-15 18:16:32] Training MSE: 219.5289
[2017-12-15 18:16:33] Experiment ffn.hl_100.lr_0.01.wd_10 logging ended.
