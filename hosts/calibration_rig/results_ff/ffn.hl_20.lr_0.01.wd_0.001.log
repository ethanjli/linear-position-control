[2017-12-15 16:51:27] Experiment ffn.hl_20.lr_0.01.wd_0.001 logging started.
[2017-12-15 16:51:27] 
                       *** Starting Experiment ffn.hl_20.lr_0.01.wd_0.001 ***
                      
[2017-12-15 16:51:27] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 16:51:27] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 1)
                      )
[2017-12-15 16:51:27]  *** Training on GPU ***
[2017-12-15 16:51:34] Epoch 0001 mean train/dev loss: 108488.1190 / 4854.6470
[2017-12-15 16:51:41] Epoch 0002 mean train/dev loss: 1298.8692 / 1442.8488
[2017-12-15 16:51:49] Epoch 0003 mean train/dev loss: 453.4810 / 828.4538
[2017-12-15 16:51:56] Epoch 0004 mean train/dev loss: 306.4741 / 562.7610
[2017-12-15 16:52:04] Epoch 0005 mean train/dev loss: 233.2445 / 398.9868
[2017-12-15 16:52:11] Epoch 0006 mean train/dev loss: 189.7326 / 360.5856
[2017-12-15 16:52:18] Epoch 0007 mean train/dev loss: 162.7519 / 272.8109
[2017-12-15 16:52:25] Epoch 0008 mean train/dev loss: 142.1964 / 250.4368
[2017-12-15 16:52:32] Epoch 0009 mean train/dev loss: 129.5635 / 202.3873
[2017-12-15 16:52:40] Epoch 0010 mean train/dev loss: 122.7653 / 274.5665
[2017-12-15 16:52:40] Checkpointing model at epoch 10 for ffn.hl_20.lr_0.01.wd_0.001
[2017-12-15 16:52:40] Model Checkpointing finished.
[2017-12-15 16:52:48] Epoch 0011 mean train/dev loss: 118.2144 / 164.5681
[2017-12-15 16:52:55] Epoch 0012 mean train/dev loss: 114.4020 / 151.1136
[2017-12-15 16:53:02] Epoch 0013 mean train/dev loss: 111.8978 / 125.8458
[2017-12-15 16:53:09] Epoch 0014 mean train/dev loss: 110.3575 / 132.2167
[2017-12-15 16:53:17] Epoch 0015 mean train/dev loss: 109.2690 / 125.1990
[2017-12-15 16:53:17] Learning rate decayed by 0.5000
[2017-12-15 16:53:24] Epoch 0016 mean train/dev loss: 107.9030 / 119.3204
[2017-12-15 16:53:31] Epoch 0017 mean train/dev loss: 107.5242 / 121.2565
[2017-12-15 16:53:39] Epoch 0018 mean train/dev loss: 107.0671 / 120.7181
[2017-12-15 16:53:46] Epoch 0019 mean train/dev loss: 106.6758 / 117.6047
[2017-12-15 16:53:54] Epoch 0020 mean train/dev loss: 106.2018 / 119.4713
[2017-12-15 16:53:54] Checkpointing model at epoch 20 for ffn.hl_20.lr_0.01.wd_0.001
[2017-12-15 16:53:54] Model Checkpointing finished.
[2017-12-15 16:54:01] Epoch 0021 mean train/dev loss: 105.5935 / 120.7197
[2017-12-15 16:54:09] Epoch 0022 mean train/dev loss: 105.0235 / 118.8395
[2017-12-15 16:54:16] Epoch 0023 mean train/dev loss: 104.5364 / 118.5784
[2017-12-15 16:54:23] Epoch 0024 mean train/dev loss: 104.0224 / 116.9963
[2017-12-15 16:54:31] Epoch 0025 mean train/dev loss: 103.4732 / 119.5413
[2017-12-15 16:54:38] Epoch 0026 mean train/dev loss: 103.0304 / 118.5402
[2017-12-15 16:54:45] Epoch 0027 mean train/dev loss: 102.6008 / 118.3931
[2017-12-15 16:54:53] Epoch 0028 mean train/dev loss: 101.6275 / 120.5226
[2017-12-15 16:55:00] Epoch 0029 mean train/dev loss: 101.0343 / 121.1212
[2017-12-15 16:55:07] Epoch 0030 mean train/dev loss: 100.6590 / 120.9832
[2017-12-15 16:55:07] Learning rate decayed by 0.5000
[2017-12-15 16:55:07] Checkpointing model at epoch 30 for ffn.hl_20.lr_0.01.wd_0.001
[2017-12-15 16:55:08] Model Checkpointing finished.
[2017-12-15 16:55:15] Epoch 0031 mean train/dev loss: 99.9174 / 121.1150
[2017-12-15 16:55:22] Epoch 0032 mean train/dev loss: 99.7268 / 117.9612
[2017-12-15 16:55:29] Epoch 0033 mean train/dev loss: 99.5532 / 114.8299
[2017-12-15 16:55:36] Epoch 0034 mean train/dev loss: 99.2841 / 120.2545
[2017-12-15 16:55:44] Epoch 0035 mean train/dev loss: 99.1291 / 116.8630
[2017-12-15 16:55:51] Epoch 0036 mean train/dev loss: 98.9427 / 123.6100
[2017-12-15 16:55:58] Epoch 0037 mean train/dev loss: 98.7225 / 118.2580
[2017-12-15 16:56:05] Epoch 0038 mean train/dev loss: 98.5247 / 121.6567
[2017-12-15 16:56:12] Epoch 0039 mean train/dev loss: 98.2877 / 120.6872
[2017-12-15 16:56:19] Epoch 0040 mean train/dev loss: 98.1566 / 121.3279
[2017-12-15 16:56:19] Checkpointing model at epoch 40 for ffn.hl_20.lr_0.01.wd_0.001
[2017-12-15 16:56:19] Model Checkpointing finished.
[2017-12-15 16:56:27] Epoch 0041 mean train/dev loss: 97.9216 / 123.5453
[2017-12-15 16:56:35] Epoch 0042 mean train/dev loss: 97.7514 / 125.9944
[2017-12-15 16:56:42] Epoch 0043 mean train/dev loss: 97.5330 / 121.1072
[2017-12-15 16:56:50] Epoch 0044 mean train/dev loss: 97.3897 / 122.4772
[2017-12-15 16:56:57] Epoch 0045 mean train/dev loss: 97.2059 / 126.8050
[2017-12-15 16:56:57] Learning rate decayed by 0.5000
[2017-12-15 16:57:05] Epoch 0046 mean train/dev loss: 96.8568 / 122.1309
[2017-12-15 16:57:12] Epoch 0047 mean train/dev loss: 96.7709 / 125.1673
[2017-12-15 16:57:20] Epoch 0048 mean train/dev loss: 96.6366 / 126.7004
[2017-12-15 16:57:28] Epoch 0049 mean train/dev loss: 96.5665 / 125.5810
[2017-12-15 16:57:35] Epoch 0050 mean train/dev loss: 96.5062 / 128.4732
[2017-12-15 16:57:35] Checkpointing model at epoch 50 for ffn.hl_20.lr_0.01.wd_0.001
[2017-12-15 16:57:35] Model Checkpointing finished.
[2017-12-15 16:57:43] Epoch 0051 mean train/dev loss: 96.4216 / 124.9742
[2017-12-15 16:57:50] Epoch 0052 mean train/dev loss: 96.3089 / 128.0772
[2017-12-15 16:57:57] Epoch 0053 mean train/dev loss: 96.2091 / 131.7173
[2017-12-15 16:58:04] Epoch 0054 mean train/dev loss: 96.1257 / 129.3604
[2017-12-15 16:58:11] Epoch 0055 mean train/dev loss: 96.0784 / 127.7327
[2017-12-15 16:58:19] Epoch 0056 mean train/dev loss: 95.9684 / 130.1273
[2017-12-15 16:58:26] Epoch 0057 mean train/dev loss: 95.9114 / 129.8782
[2017-12-15 16:58:33] Epoch 0058 mean train/dev loss: 95.8328 / 128.2591
[2017-12-15 16:58:40] Epoch 0059 mean train/dev loss: 95.7444 / 132.5379
[2017-12-15 16:58:47] Epoch 0060 mean train/dev loss: 95.6773 / 132.8016
[2017-12-15 16:58:47] Learning rate decayed by 0.5000
[2017-12-15 16:58:47] Checkpointing model at epoch 60 for ffn.hl_20.lr_0.01.wd_0.001
[2017-12-15 16:58:48] Model Checkpointing finished.
[2017-12-15 16:58:55] Epoch 0061 mean train/dev loss: 95.5379 / 133.8482
[2017-12-15 16:59:02] Epoch 0062 mean train/dev loss: 95.5187 / 130.9022
[2017-12-15 16:59:10] Epoch 0063 mean train/dev loss: 95.4373 / 132.5119
[2017-12-15 16:59:17] Epoch 0064 mean train/dev loss: 95.3974 / 134.4498
[2017-12-15 16:59:25] Epoch 0065 mean train/dev loss: 95.3407 / 136.1109
[2017-12-15 16:59:32] Epoch 0066 mean train/dev loss: 95.3172 / 135.5383
[2017-12-15 16:59:39] Epoch 0067 mean train/dev loss: 95.2839 / 136.9729
[2017-12-15 16:59:47] Epoch 0068 mean train/dev loss: 95.2486 / 134.2384
[2017-12-15 16:59:54] Epoch 0069 mean train/dev loss: 95.2011 / 135.1266
[2017-12-15 17:00:02] Epoch 0070 mean train/dev loss: 95.1673 / 135.8831
[2017-12-15 17:00:02] Checkpointing model at epoch 70 for ffn.hl_20.lr_0.01.wd_0.001
[2017-12-15 17:00:02] Model Checkpointing finished.
[2017-12-15 17:00:10] Epoch 0071 mean train/dev loss: 95.1207 / 135.0998
[2017-12-15 17:00:18] Epoch 0072 mean train/dev loss: 95.0946 / 137.3247
[2017-12-15 17:00:25] Epoch 0073 mean train/dev loss: 95.0606 / 135.3498
[2017-12-15 17:00:33] Epoch 0074 mean train/dev loss: 95.0049 / 134.0462
[2017-12-15 17:00:33] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:00:33] 
                       *** Training finished *** 
[2017-12-15 17:00:34] Dev MSE: 134.0462
[2017-12-15 17:00:39] Training MSE: 95.0437
[2017-12-15 17:00:40] Experiment ffn.hl_20.lr_0.01.wd_0.001 logging ended.
