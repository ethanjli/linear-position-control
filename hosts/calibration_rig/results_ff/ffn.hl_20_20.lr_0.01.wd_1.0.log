[2017-12-15 17:03:44] Experiment ffn.hl_20_20.lr_0.01.wd_1.0 logging started.
[2017-12-15 17:03:44] 
                       *** Starting Experiment ffn.hl_20_20.lr_0.01.wd_1.0 ***
                      
[2017-12-15 17:03:44] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 17:03:44] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 1)
                      )
[2017-12-15 17:03:44]  *** Training on GPU ***
[2017-12-15 17:03:52] Epoch 0001 mean train/dev loss: 42621.5347 / 552.3612
[2017-12-15 17:04:00] Epoch 0002 mean train/dev loss: 221.2601 / 183.4563
[2017-12-15 17:04:08] Epoch 0003 mean train/dev loss: 140.1776 / 147.7489
[2017-12-15 17:04:16] Epoch 0004 mean train/dev loss: 124.5944 / 134.5235
[2017-12-15 17:04:24] Epoch 0005 mean train/dev loss: 119.0744 / 123.2289
[2017-12-15 17:04:31] Epoch 0006 mean train/dev loss: 116.7163 / 128.7660
[2017-12-15 17:04:39] Epoch 0007 mean train/dev loss: 115.3562 / 121.9641
[2017-12-15 17:04:47] Epoch 0008 mean train/dev loss: 113.8738 / 116.6268
[2017-12-15 17:04:55] Epoch 0009 mean train/dev loss: 114.2950 / 123.6309
[2017-12-15 17:05:03] Epoch 0010 mean train/dev loss: 113.2428 / 118.0721
[2017-12-15 17:05:03] Checkpointing model at epoch 10 for ffn.hl_20_20.lr_0.01.wd_1.0
[2017-12-15 17:05:03] Model Checkpointing finished.
[2017-12-15 17:05:11] Epoch 0011 mean train/dev loss: 112.8826 / 111.5941
[2017-12-15 17:05:18] Epoch 0012 mean train/dev loss: 113.1955 / 119.6424
[2017-12-15 17:05:26] Epoch 0013 mean train/dev loss: 112.8984 / 118.3993
[2017-12-15 17:05:35] Epoch 0014 mean train/dev loss: 112.5106 / 122.5781
[2017-12-15 17:05:43] Epoch 0015 mean train/dev loss: 113.0731 / 117.0199
[2017-12-15 17:05:43] Learning rate decayed by 0.5000
[2017-12-15 17:05:51] Epoch 0016 mean train/dev loss: 110.9910 / 112.2178
[2017-12-15 17:05:59] Epoch 0017 mean train/dev loss: 111.3440 / 115.7801
[2017-12-15 17:06:07] Epoch 0018 mean train/dev loss: 111.1436 / 113.3135
[2017-12-15 17:06:15] Epoch 0019 mean train/dev loss: 111.5385 / 110.4374
[2017-12-15 17:06:24] Epoch 0020 mean train/dev loss: 111.0918 / 111.3655
[2017-12-15 17:06:24] Checkpointing model at epoch 20 for ffn.hl_20_20.lr_0.01.wd_1.0
[2017-12-15 17:06:24] Model Checkpointing finished.
[2017-12-15 17:06:32] Epoch 0021 mean train/dev loss: 111.2049 / 114.5757
[2017-12-15 17:06:40] Epoch 0022 mean train/dev loss: 111.2065 / 113.7681
[2017-12-15 17:06:49] Epoch 0023 mean train/dev loss: 111.1276 / 108.9462
[2017-12-15 17:06:57] Epoch 0024 mean train/dev loss: 111.1431 / 116.0573
[2017-12-15 17:07:05] Epoch 0025 mean train/dev loss: 111.3162 / 115.3652
[2017-12-15 17:07:12] Epoch 0026 mean train/dev loss: 111.2506 / 116.1893
[2017-12-15 17:07:20] Epoch 0027 mean train/dev loss: 111.1003 / 114.0457
[2017-12-15 17:07:29] Epoch 0028 mean train/dev loss: 111.2947 / 111.2876
[2017-12-15 17:07:37] Epoch 0029 mean train/dev loss: 111.2249 / 113.2071
[2017-12-15 17:07:45] Epoch 0030 mean train/dev loss: 111.3301 / 110.1629
[2017-12-15 17:07:45] Learning rate decayed by 0.5000
[2017-12-15 17:07:45] Checkpointing model at epoch 30 for ffn.hl_20_20.lr_0.01.wd_1.0
[2017-12-15 17:07:45] Model Checkpointing finished.
[2017-12-15 17:07:53] Epoch 0031 mean train/dev loss: 110.2548 / 108.6526
[2017-12-15 17:08:01] Epoch 0032 mean train/dev loss: 109.0312 / 106.8151
[2017-12-15 17:08:09] Epoch 0033 mean train/dev loss: 108.8025 / 109.0170
[2017-12-15 17:08:17] Epoch 0034 mean train/dev loss: 108.6945 / 105.4754
[2017-12-15 17:08:25] Epoch 0035 mean train/dev loss: 108.5981 / 106.3799
[2017-12-15 17:08:33] Epoch 0036 mean train/dev loss: 108.4981 / 106.2859
[2017-12-15 17:08:42] Epoch 0037 mean train/dev loss: 108.4359 / 107.1251
[2017-12-15 17:08:50] Epoch 0038 mean train/dev loss: 108.3308 / 107.6508
[2017-12-15 17:08:58] Epoch 0039 mean train/dev loss: 108.2578 / 105.5520
[2017-12-15 17:09:07] Epoch 0040 mean train/dev loss: 108.1945 / 105.0394
[2017-12-15 17:09:07] Checkpointing model at epoch 40 for ffn.hl_20_20.lr_0.01.wd_1.0
[2017-12-15 17:09:07] Model Checkpointing finished.
[2017-12-15 17:09:15] Epoch 0041 mean train/dev loss: 108.2241 / 108.1761
[2017-12-15 17:09:23] Epoch 0042 mean train/dev loss: 108.2915 / 107.8674
[2017-12-15 17:09:31] Epoch 0043 mean train/dev loss: 108.0922 / 104.6903
[2017-12-15 17:09:39] Epoch 0044 mean train/dev loss: 108.2117 / 105.7266
[2017-12-15 17:09:46] Epoch 0045 mean train/dev loss: 108.0670 / 105.6842
[2017-12-15 17:09:46] Learning rate decayed by 0.5000
[2017-12-15 17:09:55] Epoch 0046 mean train/dev loss: 107.5534 / 105.3974
[2017-12-15 17:10:03] Epoch 0047 mean train/dev loss: 107.6320 / 105.6538
[2017-12-15 17:10:11] Epoch 0048 mean train/dev loss: 107.6523 / 105.9349
[2017-12-15 17:10:19] Epoch 0049 mean train/dev loss: 107.5213 / 106.4211
[2017-12-15 17:10:27] Epoch 0050 mean train/dev loss: 107.6192 / 105.1415
[2017-12-15 17:10:27] Checkpointing model at epoch 50 for ffn.hl_20_20.lr_0.01.wd_1.0
[2017-12-15 17:10:27] Model Checkpointing finished.
[2017-12-15 17:10:36] Epoch 0051 mean train/dev loss: 107.5935 / 106.0959
[2017-12-15 17:10:43] Epoch 0052 mean train/dev loss: 107.5567 / 105.5803
[2017-12-15 17:10:52] Epoch 0053 mean train/dev loss: 107.5643 / 107.7699
[2017-12-15 17:11:00] Epoch 0054 mean train/dev loss: 107.6078 / 106.5996
[2017-12-15 17:11:08] Epoch 0055 mean train/dev loss: 107.4751 / 107.8600
[2017-12-15 17:11:17] Epoch 0056 mean train/dev loss: 107.5139 / 106.9232
[2017-12-15 17:11:23] Epoch 0057 mean train/dev loss: 107.4680 / 105.8441
[2017-12-15 17:11:28] Epoch 0058 mean train/dev loss: 107.4879 / 105.3742
[2017-12-15 17:11:33] Epoch 0059 mean train/dev loss: 107.4809 / 104.8725
[2017-12-15 17:11:38] Epoch 0060 mean train/dev loss: 107.4962 / 106.0546
[2017-12-15 17:11:38] Learning rate decayed by 0.5000
[2017-12-15 17:11:38] Checkpointing model at epoch 60 for ffn.hl_20_20.lr_0.01.wd_1.0
[2017-12-15 17:11:39] Model Checkpointing finished.
[2017-12-15 17:11:44] Epoch 0061 mean train/dev loss: 107.2598 / 106.0031
[2017-12-15 17:11:50] Epoch 0062 mean train/dev loss: 107.2520 / 105.1780
[2017-12-15 17:11:55] Epoch 0063 mean train/dev loss: 107.2046 / 105.7315
[2017-12-15 17:12:01] Epoch 0064 mean train/dev loss: 107.3047 / 107.2225
[2017-12-15 17:12:06] Epoch 0065 mean train/dev loss: 107.2299 / 106.8831
[2017-12-15 17:12:11] Epoch 0066 mean train/dev loss: 107.2426 / 106.8347
[2017-12-15 17:12:17] Epoch 0067 mean train/dev loss: 107.1971 / 105.9123
[2017-12-15 17:12:22] Epoch 0068 mean train/dev loss: 107.2511 / 106.0144
[2017-12-15 17:12:28] Epoch 0069 mean train/dev loss: 107.1914 / 106.8503
[2017-12-15 17:12:33] Epoch 0070 mean train/dev loss: 107.2194 / 106.4537
[2017-12-15 17:12:33] Checkpointing model at epoch 70 for ffn.hl_20_20.lr_0.01.wd_1.0
[2017-12-15 17:12:34] Model Checkpointing finished.
[2017-12-15 17:12:39] Epoch 0071 mean train/dev loss: 107.1848 / 106.2525
[2017-12-15 17:12:45] Epoch 0072 mean train/dev loss: 107.2139 / 106.1994
[2017-12-15 17:12:50] Epoch 0073 mean train/dev loss: 107.1996 / 106.7022
[2017-12-15 17:12:55] Epoch 0074 mean train/dev loss: 107.2010 / 106.0849
[2017-12-15 17:13:01] Epoch 0075 mean train/dev loss: 107.1955 / 105.9828
[2017-12-15 17:13:01] Learning rate decayed by 0.5000
[2017-12-15 17:13:07] Epoch 0076 mean train/dev loss: 107.0732 / 106.8259
[2017-12-15 17:13:12] Epoch 0077 mean train/dev loss: 107.0567 / 105.9598
[2017-12-15 17:13:17] Epoch 0078 mean train/dev loss: 107.0490 / 106.1781
[2017-12-15 17:13:23] Epoch 0079 mean train/dev loss: 107.0318 / 105.6675
[2017-12-15 17:13:28] Epoch 0080 mean train/dev loss: 107.0611 / 107.8293
[2017-12-15 17:13:28] Checkpointing model at epoch 80 for ffn.hl_20_20.lr_0.01.wd_1.0
[2017-12-15 17:13:29] Model Checkpointing finished.
[2017-12-15 17:13:34] Epoch 0081 mean train/dev loss: 107.0321 / 106.2028
[2017-12-15 17:13:40] Epoch 0082 mean train/dev loss: 107.0393 / 106.0157
[2017-12-15 17:13:45] Epoch 0083 mean train/dev loss: 107.0452 / 105.9887
[2017-12-15 17:13:51] Epoch 0084 mean train/dev loss: 107.0452 / 105.8103
[2017-12-15 17:13:51] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:13:51] 
                       *** Training finished *** 
[2017-12-15 17:13:52] Dev MSE: 105.8103
[2017-12-15 17:13:57] Training MSE: 106.9473
[2017-12-15 17:13:58] Experiment ffn.hl_20_20.lr_0.01.wd_1.0 logging ended.
