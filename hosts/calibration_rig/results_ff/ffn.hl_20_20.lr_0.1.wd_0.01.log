[2017-12-15 14:42:15] Experiment ffn.hl_20_20.lr_0.1.wd_0.01 logging started.
[2017-12-15 14:42:15] 
                       *** Starting Experiment ffn.hl_20_20.lr_0.1.wd_0.01 ***
                      
[2017-12-15 14:42:15] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 14:42:15] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 1)
                      )
[2017-12-15 14:42:15]  *** Training on GPU ***
[2017-12-15 14:42:22] Epoch 0001 mean train/dev loss: 5869.3503 / 280.2493
[2017-12-15 14:42:29] Epoch 0002 mean train/dev loss: 148.6179 / 240.2438
[2017-12-15 14:42:36] Epoch 0003 mean train/dev loss: 129.2337 / 197.4753
[2017-12-15 14:42:43] Epoch 0004 mean train/dev loss: 123.4607 / 163.7404
[2017-12-15 14:42:50] Epoch 0005 mean train/dev loss: 126.7625 / 157.4180
[2017-12-15 14:42:58] Epoch 0006 mean train/dev loss: 130.2370 / 116.7935
[2017-12-15 14:43:04] Epoch 0007 mean train/dev loss: 133.4116 / 141.5701
[2017-12-15 14:43:11] Epoch 0008 mean train/dev loss: 135.1083 / 197.9126
[2017-12-15 14:43:18] Epoch 0009 mean train/dev loss: 131.0740 / 147.0960
[2017-12-15 14:43:25] Epoch 0010 mean train/dev loss: 135.8723 / 153.2785
[2017-12-15 14:43:25] Checkpointing model at epoch 10 for ffn.hl_20_20.lr_0.1.wd_0.01
[2017-12-15 14:43:26] Model Checkpointing finished.
[2017-12-15 14:43:32] Epoch 0011 mean train/dev loss: 125.6999 / 143.4217
[2017-12-15 14:43:39] Epoch 0012 mean train/dev loss: 130.3253 / 159.7770
[2017-12-15 14:43:45] Epoch 0013 mean train/dev loss: 122.6731 / 137.6618
[2017-12-15 14:43:52] Epoch 0014 mean train/dev loss: 123.4500 / 165.4208
[2017-12-15 14:43:59] Epoch 0015 mean train/dev loss: 121.0242 / 138.4908
[2017-12-15 14:43:59] Learning rate decayed by 0.5000
[2017-12-15 14:44:06] Epoch 0016 mean train/dev loss: 97.8098 / 205.6214
[2017-12-15 14:44:13] Epoch 0017 mean train/dev loss: 91.9881 / 158.8840
[2017-12-15 14:44:20] Epoch 0018 mean train/dev loss: 88.9495 / 179.3252
[2017-12-15 14:44:26] Epoch 0019 mean train/dev loss: 84.9822 / 147.0529
[2017-12-15 14:44:33] Epoch 0020 mean train/dev loss: 84.6752 / 146.5616
[2017-12-15 14:44:33] Checkpointing model at epoch 20 for ffn.hl_20_20.lr_0.1.wd_0.01
[2017-12-15 14:44:33] Model Checkpointing finished.
[2017-12-15 14:44:40] Epoch 0021 mean train/dev loss: 83.2881 / 138.1174
[2017-12-15 14:44:46] Epoch 0022 mean train/dev loss: 82.2570 / 144.6461
[2017-12-15 14:44:53] Epoch 0023 mean train/dev loss: 80.8149 / 163.6332
[2017-12-15 14:45:00] Epoch 0024 mean train/dev loss: 82.3057 / 152.4684
[2017-12-15 14:45:07] Epoch 0025 mean train/dev loss: 81.1498 / 132.7448
[2017-12-15 14:45:14] Epoch 0026 mean train/dev loss: 79.0501 / 134.0806
[2017-12-15 14:45:21] Epoch 0027 mean train/dev loss: 75.0361 / 123.3602
[2017-12-15 14:45:28] Epoch 0028 mean train/dev loss: 71.4642 / 136.2712
[2017-12-15 14:45:35] Epoch 0029 mean train/dev loss: 69.9819 / 94.5394
[2017-12-15 14:45:42] Epoch 0030 mean train/dev loss: 68.8651 / 108.0209
[2017-12-15 14:45:42] Learning rate decayed by 0.5000
[2017-12-15 14:45:42] Checkpointing model at epoch 30 for ffn.hl_20_20.lr_0.1.wd_0.01
[2017-12-15 14:45:42] Model Checkpointing finished.
[2017-12-15 14:45:49] Epoch 0031 mean train/dev loss: 61.0838 / 93.8921
[2017-12-15 14:45:56] Epoch 0032 mean train/dev loss: 61.0464 / 95.1571
[2017-12-15 14:46:03] Epoch 0033 mean train/dev loss: 60.5364 / 114.7444
[2017-12-15 14:46:09] Epoch 0034 mean train/dev loss: 61.0117 / 91.0808
[2017-12-15 14:46:15] Epoch 0035 mean train/dev loss: 61.3533 / 114.8568
[2017-12-15 14:46:22] Epoch 0036 mean train/dev loss: 60.2124 / 95.7248
[2017-12-15 14:46:29] Epoch 0037 mean train/dev loss: 60.8641 / 91.6974
[2017-12-15 14:46:35] Epoch 0038 mean train/dev loss: 59.8793 / 107.3496
[2017-12-15 14:46:42] Epoch 0039 mean train/dev loss: 59.0398 / 121.2090
[2017-12-15 14:46:49] Epoch 0040 mean train/dev loss: 59.6193 / 96.7742
[2017-12-15 14:46:49] Checkpointing model at epoch 40 for ffn.hl_20_20.lr_0.1.wd_0.01
[2017-12-15 14:46:49] Model Checkpointing finished.
[2017-12-15 14:46:55] Epoch 0041 mean train/dev loss: 59.5298 / 109.9340
[2017-12-15 14:47:02] Epoch 0042 mean train/dev loss: 58.9203 / 79.0948
[2017-12-15 14:47:09] Epoch 0043 mean train/dev loss: 59.1697 / 91.1569
[2017-12-15 14:47:15] Epoch 0044 mean train/dev loss: 59.1705 / 104.2512
[2017-12-15 14:47:22] Epoch 0045 mean train/dev loss: 59.2141 / 95.2679
[2017-12-15 14:47:22] Learning rate decayed by 0.5000
[2017-12-15 14:47:29] Epoch 0046 mean train/dev loss: 56.0806 / 90.6297
[2017-12-15 14:47:35] Epoch 0047 mean train/dev loss: 56.1190 / 96.7458
[2017-12-15 14:47:42] Epoch 0048 mean train/dev loss: 56.4043 / 100.5803
[2017-12-15 14:47:48] Epoch 0049 mean train/dev loss: 56.0935 / 87.8544
[2017-12-15 14:47:55] Epoch 0050 mean train/dev loss: 56.1858 / 109.3623
[2017-12-15 14:47:55] Checkpointing model at epoch 50 for ffn.hl_20_20.lr_0.1.wd_0.01
[2017-12-15 14:47:55] Model Checkpointing finished.
[2017-12-15 14:48:02] Epoch 0051 mean train/dev loss: 56.3149 / 88.4682
[2017-12-15 14:48:09] Epoch 0052 mean train/dev loss: 56.2119 / 97.2812
[2017-12-15 14:48:16] Epoch 0053 mean train/dev loss: 56.2031 / 100.0828
[2017-12-15 14:48:23] Epoch 0054 mean train/dev loss: 56.0480 / 93.4968
[2017-12-15 14:48:30] Epoch 0055 mean train/dev loss: 56.3498 / 106.8518
[2017-12-15 14:48:36] Epoch 0056 mean train/dev loss: 56.0998 / 106.6875
[2017-12-15 14:48:43] Epoch 0057 mean train/dev loss: 56.2851 / 91.1476
[2017-12-15 14:48:49] Epoch 0058 mean train/dev loss: 55.8689 / 89.3967
[2017-12-15 14:48:56] Epoch 0059 mean train/dev loss: 55.9855 / 80.5201
[2017-12-15 14:49:03] Epoch 0060 mean train/dev loss: 55.9077 / 90.2164
[2017-12-15 14:49:03] Learning rate decayed by 0.5000
[2017-12-15 14:49:03] Checkpointing model at epoch 60 for ffn.hl_20_20.lr_0.1.wd_0.01
[2017-12-15 14:49:03] Model Checkpointing finished.
[2017-12-15 14:49:10] Epoch 0061 mean train/dev loss: 54.5327 / 80.7027
[2017-12-15 14:49:16] Epoch 0062 mean train/dev loss: 54.4442 / 82.3198
[2017-12-15 14:49:23] Epoch 0063 mean train/dev loss: 54.4487 / 84.7839
[2017-12-15 14:49:30] Epoch 0064 mean train/dev loss: 54.6076 / 88.4415
[2017-12-15 14:49:36] Epoch 0065 mean train/dev loss: 54.4638 / 88.8235
[2017-12-15 14:49:43] Epoch 0066 mean train/dev loss: 54.5570 / 85.1157
[2017-12-15 14:49:50] Epoch 0067 mean train/dev loss: 54.2977 / 84.0324
[2017-12-15 14:49:57] Epoch 0068 mean train/dev loss: 54.4328 / 82.6272
[2017-12-15 14:50:04] Epoch 0069 mean train/dev loss: 54.4857 / 88.9511
[2017-12-15 14:50:11] Epoch 0070 mean train/dev loss: 54.3948 / 89.5433
[2017-12-15 14:50:11] Checkpointing model at epoch 70 for ffn.hl_20_20.lr_0.1.wd_0.01
[2017-12-15 14:50:11] Model Checkpointing finished.
[2017-12-15 14:50:18] Epoch 0071 mean train/dev loss: 54.3717 / 89.0400
[2017-12-15 14:50:25] Epoch 0072 mean train/dev loss: 54.1710 / 81.8089
[2017-12-15 14:50:32] Epoch 0073 mean train/dev loss: 54.2894 / 93.3376
[2017-12-15 14:50:38] Epoch 0074 mean train/dev loss: 54.1104 / 89.0297
[2017-12-15 14:50:45] Epoch 0075 mean train/dev loss: 54.2670 / 88.5679
[2017-12-15 14:50:45] Learning rate decayed by 0.5000
[2017-12-15 14:50:51] Epoch 0076 mean train/dev loss: 53.3674 / 85.5463
[2017-12-15 14:50:58] Epoch 0077 mean train/dev loss: 53.3581 / 89.3172
[2017-12-15 14:51:05] Epoch 0078 mean train/dev loss: 53.4189 / 84.3385
[2017-12-15 14:51:11] Epoch 0079 mean train/dev loss: 53.3963 / 93.1338
[2017-12-15 14:51:18] Epoch 0080 mean train/dev loss: 53.4268 / 91.4133
[2017-12-15 14:51:18] Checkpointing model at epoch 80 for ffn.hl_20_20.lr_0.1.wd_0.01
[2017-12-15 14:51:18] Model Checkpointing finished.
[2017-12-15 14:51:25] Epoch 0081 mean train/dev loss: 53.3511 / 87.5050
[2017-12-15 14:51:31] Epoch 0082 mean train/dev loss: 53.4093 / 87.9557
[2017-12-15 14:51:37] Epoch 0083 mean train/dev loss: 53.4302 / 93.0741
[2017-12-15 14:51:37] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:51:37] 
                       *** Training finished *** 
[2017-12-15 14:51:38] Dev MSE: 93.0741
[2017-12-15 14:51:43] Training MSE: 53.3551
[2017-12-15 14:51:45] Experiment ffn.hl_20_20.lr_0.1.wd_0.01 logging ended.
