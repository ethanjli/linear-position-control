[2017-12-15 14:20:16] Experiment ffn.hl_linear.lr_0.1.wd_0.1 logging started.
[2017-12-15 14:20:16] 
                       *** Starting Experiment ffn.hl_linear.lr_0.1.wd_0.1 ***
                      
[2017-12-15 14:20:16] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] []  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 14:20:18] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 1)
                      )
[2017-12-15 14:20:18]  *** Training on GPU ***
[2017-12-15 14:20:22] Epoch 0001 mean train/dev loss: 286610.8817 / 245431.7031
[2017-12-15 14:20:26] Epoch 0002 mean train/dev loss: 214821.1879 / 185724.6250
[2017-12-15 14:20:32] Epoch 0003 mean train/dev loss: 163394.9041 / 140074.6094
[2017-12-15 14:20:39] Epoch 0004 mean train/dev loss: 122219.6256 / 103389.8906
[2017-12-15 14:20:45] Epoch 0005 mean train/dev loss: 89068.9819 / 74103.8828
[2017-12-15 14:20:50] Epoch 0006 mean train/dev loss: 62893.4022 / 51304.1094
[2017-12-15 14:20:55] Epoch 0007 mean train/dev loss: 42838.8877 / 34217.7695
[2017-12-15 14:20:59] Epoch 0008 mean train/dev loss: 28087.5460 / 21997.7852
[2017-12-15 14:21:04] Epoch 0009 mean train/dev loss: 17837.2932 / 13821.5605
[2017-12-15 14:21:08] Epoch 0010 mean train/dev loss: 11217.6287 / 8810.6504
[2017-12-15 14:21:08] Checkpointing model at epoch 10 for ffn.hl_linear.lr_0.1.wd_0.1
[2017-12-15 14:21:09] Model Checkpointing finished.
[2017-12-15 14:21:14] Epoch 0011 mean train/dev loss: 7352.8912 / 6096.7026
[2017-12-15 14:21:21] Epoch 0012 mean train/dev loss: 5360.1746 / 4790.6509
[2017-12-15 14:21:27] Epoch 0013 mean train/dev loss: 4452.5680 / 4268.4453
[2017-12-15 14:21:33] Epoch 0014 mean train/dev loss: 4098.9789 / 4073.4092
[2017-12-15 14:21:37] Epoch 0015 mean train/dev loss: 3988.1424 / 4037.5198
[2017-12-15 14:21:37] Learning rate decayed by 0.5000
[2017-12-15 14:21:42] Epoch 0016 mean train/dev loss: 3962.5113 / 4023.6379
[2017-12-15 14:21:47] Epoch 0017 mean train/dev loss: 3956.3631 / 4014.3096
[2017-12-15 14:21:53] Epoch 0018 mean train/dev loss: 3955.8879 / 4024.1377
[2017-12-15 14:22:00] Epoch 0019 mean train/dev loss: 3954.5735 / 4007.9004
[2017-12-15 14:22:06] Epoch 0020 mean train/dev loss: 3955.3258 / 4016.7993
[2017-12-15 14:22:06] Checkpointing model at epoch 20 for ffn.hl_linear.lr_0.1.wd_0.1
[2017-12-15 14:22:07] Model Checkpointing finished.
[2017-12-15 14:22:12] Epoch 0021 mean train/dev loss: 3954.9806 / 4012.1040
[2017-12-15 14:22:17] Epoch 0022 mean train/dev loss: 3953.5015 / 4013.9885
[2017-12-15 14:22:22] Epoch 0023 mean train/dev loss: 3955.9896 / 4021.7571
[2017-12-15 14:22:27] Epoch 0024 mean train/dev loss: 3956.3357 / 4016.5959
[2017-12-15 14:22:33] Epoch 0025 mean train/dev loss: 3956.8222 / 4012.4607
[2017-12-15 14:22:39] Epoch 0026 mean train/dev loss: 3953.7835 / 4016.1021
[2017-12-15 14:22:44] Epoch 0027 mean train/dev loss: 3954.1189 / 4021.1460
[2017-12-15 14:22:49] Epoch 0028 mean train/dev loss: 3958.6235 / 4015.2444
[2017-12-15 14:22:54] Epoch 0029 mean train/dev loss: 3955.8801 / 4011.1921
[2017-12-15 14:23:00] Epoch 0030 mean train/dev loss: 3953.0161 / 4012.6392
[2017-12-15 14:23:00] Learning rate decayed by 0.5000
[2017-12-15 14:23:00] Checkpointing model at epoch 30 for ffn.hl_linear.lr_0.1.wd_0.1
[2017-12-15 14:23:00] Model Checkpointing finished.
[2017-12-15 14:23:04] Epoch 0031 mean train/dev loss: 3954.3976 / 4018.4253
[2017-12-15 14:23:10] Epoch 0032 mean train/dev loss: 3957.8915 / 4020.8977
[2017-12-15 14:23:14] Epoch 0033 mean train/dev loss: 3952.9727 / 4011.8594
[2017-12-15 14:23:19] Epoch 0034 mean train/dev loss: 3953.9681 / 4024.3171
[2017-12-15 14:23:26] Epoch 0035 mean train/dev loss: 3957.9657 / 3999.2898
[2017-12-15 14:23:32] Epoch 0036 mean train/dev loss: 3954.5841 / 4006.3335
[2017-12-15 14:23:38] Epoch 0037 mean train/dev loss: 3956.3489 / 4012.3928
[2017-12-15 14:23:44] Epoch 0038 mean train/dev loss: 3953.7990 / 4007.1184
[2017-12-15 14:23:49] Epoch 0039 mean train/dev loss: 3953.2017 / 4013.5024
[2017-12-15 14:23:54] Epoch 0040 mean train/dev loss: 3955.3289 / 4019.0774
[2017-12-15 14:23:54] Checkpointing model at epoch 40 for ffn.hl_linear.lr_0.1.wd_0.1
[2017-12-15 14:23:55] Model Checkpointing finished.
[2017-12-15 14:24:02] Epoch 0041 mean train/dev loss: 3957.2574 / 3992.3923
[2017-12-15 14:24:09] Epoch 0042 mean train/dev loss: 3954.4007 / 4025.0730
[2017-12-15 14:24:14] Epoch 0043 mean train/dev loss: 3953.4661 / 4016.2854
[2017-12-15 14:24:18] Epoch 0044 mean train/dev loss: 3956.0761 / 4008.0942
[2017-12-15 14:24:24] Epoch 0045 mean train/dev loss: 3955.1661 / 4009.6973
[2017-12-15 14:24:24] Learning rate decayed by 0.5000
[2017-12-15 14:24:30] Epoch 0046 mean train/dev loss: 3952.4021 / 4020.3428
[2017-12-15 14:24:35] Epoch 0047 mean train/dev loss: 3953.9501 / 4020.3723
[2017-12-15 14:24:40] Epoch 0048 mean train/dev loss: 3958.8150 / 4007.7449
[2017-12-15 14:24:46] Epoch 0049 mean train/dev loss: 3953.3098 / 4015.9956
[2017-12-15 14:24:53] Epoch 0050 mean train/dev loss: 3956.3649 / 4013.0388
[2017-12-15 14:24:53] Checkpointing model at epoch 50 for ffn.hl_linear.lr_0.1.wd_0.1
[2017-12-15 14:24:53] Model Checkpointing finished.
[2017-12-15 14:24:59] Epoch 0051 mean train/dev loss: 3952.0398 / 4017.1279
[2017-12-15 14:25:04] Epoch 0052 mean train/dev loss: 3959.6467 / 4003.3328
[2017-12-15 14:25:08] Epoch 0053 mean train/dev loss: 3950.0287 / 4017.9771
[2017-12-15 14:25:14] Epoch 0054 mean train/dev loss: 3954.7008 / 4019.0044
[2017-12-15 14:25:19] Epoch 0055 mean train/dev loss: 3956.1956 / 4015.2083
[2017-12-15 14:25:24] Epoch 0056 mean train/dev loss: 3955.2548 / 4016.5317
[2017-12-15 14:25:28] Epoch 0057 mean train/dev loss: 3958.1367 / 4004.1592
[2017-12-15 14:25:33] Epoch 0058 mean train/dev loss: 3952.2498 / 4014.1536
[2017-12-15 14:25:37] Epoch 0059 mean train/dev loss: 3953.8338 / 4016.3318
[2017-12-15 14:25:42] Epoch 0060 mean train/dev loss: 3954.5452 / 4018.5583
[2017-12-15 14:25:42] Learning rate decayed by 0.5000
[2017-12-15 14:25:42] Checkpointing model at epoch 60 for ffn.hl_linear.lr_0.1.wd_0.1
[2017-12-15 14:25:42] Model Checkpointing finished.
[2017-12-15 14:25:47] Epoch 0061 mean train/dev loss: 3953.8945 / 4021.0967
[2017-12-15 14:25:51] Epoch 0062 mean train/dev loss: 3959.0019 / 4013.2927
[2017-12-15 14:25:56] Epoch 0063 mean train/dev loss: 3954.9597 / 4014.7974
[2017-12-15 14:26:01] Epoch 0064 mean train/dev loss: 3953.6922 / 4016.3113
[2017-12-15 14:26:07] Epoch 0065 mean train/dev loss: 3955.6999 / 4016.0254
[2017-12-15 14:26:12] Epoch 0066 mean train/dev loss: 3956.3908 / 4014.3982
[2017-12-15 14:26:16] Epoch 0067 mean train/dev loss: 3953.9837 / 4015.2644
[2017-12-15 14:26:20] Epoch 0068 mean train/dev loss: 3955.5709 / 4012.2202
[2017-12-15 14:26:25] Epoch 0069 mean train/dev loss: 3956.3447 / 4014.6956
[2017-12-15 14:26:29] Epoch 0070 mean train/dev loss: 3954.7073 / 4016.7244
[2017-12-15 14:26:29] Checkpointing model at epoch 70 for ffn.hl_linear.lr_0.1.wd_0.1
[2017-12-15 14:26:30] Model Checkpointing finished.
[2017-12-15 14:26:34] Epoch 0071 mean train/dev loss: 3953.4144 / 4014.2097
[2017-12-15 14:26:38] Epoch 0072 mean train/dev loss: 3956.3189 / 4013.5767
[2017-12-15 14:26:43] Epoch 0073 mean train/dev loss: 3957.5954 / 4012.1721
[2017-12-15 14:26:47] Epoch 0074 mean train/dev loss: 3955.2949 / 4011.7627
[2017-12-15 14:26:51] Epoch 0075 mean train/dev loss: 3955.1719 / 4010.6853
[2017-12-15 14:26:51] Learning rate decayed by 0.5000
[2017-12-15 14:26:56] Epoch 0076 mean train/dev loss: 3951.9083 / 4013.2480
[2017-12-15 14:27:00] Epoch 0077 mean train/dev loss: 3953.8382 / 4014.3120
[2017-12-15 14:27:05] Epoch 0078 mean train/dev loss: 3954.1643 / 4013.7209
[2017-12-15 14:27:09] Epoch 0079 mean train/dev loss: 3955.3294 / 4013.3584
[2017-12-15 14:27:14] Epoch 0080 mean train/dev loss: 3954.2491 / 4013.9082
[2017-12-15 14:27:14] Checkpointing model at epoch 80 for ffn.hl_linear.lr_0.1.wd_0.1
[2017-12-15 14:27:14] Model Checkpointing finished.
[2017-12-15 14:27:18] Epoch 0081 mean train/dev loss: 3954.5534 / 4013.3091
[2017-12-15 14:27:22] Epoch 0082 mean train/dev loss: 3954.1087 / 4014.8687
[2017-12-15 14:27:22] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:27:22] 
                       *** Training finished *** 
[2017-12-15 14:27:23] Dev MSE: 4014.8687
[2017-12-15 14:27:27] Training MSE: 3955.4375
[2017-12-15 14:27:27] Experiment ffn.hl_linear.lr_0.1.wd_0.1 logging ended.
