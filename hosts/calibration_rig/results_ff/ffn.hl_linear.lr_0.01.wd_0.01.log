[2017-12-15 16:30:20] Experiment ffn.hl_linear.lr_0.01.wd_0.01 logging started.
[2017-12-15 16:30:20] 
                       *** Starting Experiment ffn.hl_linear.lr_0.01.wd_0.01 ***
                      
[2017-12-15 16:30:20] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] []  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 16:30:20] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 1)
                      )
[2017-12-15 16:30:20]  *** Training on GPU ***
[2017-12-15 16:30:27] Epoch 0001 mean train/dev loss: 329168.8082 / 326148.9062
[2017-12-15 16:30:35] Epoch 0002 mean train/dev loss: 318335.0180 / 315181.6875
[2017-12-15 16:30:42] Epoch 0003 mean train/dev loss: 308021.1789 / 304744.6562
[2017-12-15 16:30:49] Epoch 0004 mean train/dev loss: 298175.7573 / 294751.2500
[2017-12-15 16:30:56] Epoch 0005 mean train/dev loss: 288762.6115 / 285201.8125
[2017-12-15 16:31:04] Epoch 0006 mean train/dev loss: 279740.8593 / 276047.1250
[2017-12-15 16:31:10] Epoch 0007 mean train/dev loss: 271148.2849 / 267276.8125
[2017-12-15 16:31:17] Epoch 0008 mean train/dev loss: 262890.5153 / 258873.6094
[2017-12-15 16:31:24] Epoch 0009 mean train/dev loss: 254959.6215 / 250810.6094
[2017-12-15 16:31:31] Epoch 0010 mean train/dev loss: 247323.9661 / 243079.8906
[2017-12-15 16:31:31] Checkpointing model at epoch 10 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:31:32] Model Checkpointing finished.
[2017-12-15 16:31:39] Epoch 0011 mean train/dev loss: 240067.0032 / 235677.8125
[2017-12-15 16:31:45] Epoch 0012 mean train/dev loss: 233038.6034 / 228568.9688
[2017-12-15 16:31:51] Epoch 0013 mean train/dev loss: 226309.1180 / 221743.5312
[2017-12-15 16:31:58] Epoch 0014 mean train/dev loss: 219827.7772 / 215173.5469
[2017-12-15 16:32:04] Epoch 0015 mean train/dev loss: 213521.1099 / 208834.9062
[2017-12-15 16:32:04] Learning rate decayed by 0.5000
[2017-12-15 16:32:12] Epoch 0016 mean train/dev loss: 208929.2979 / 205731.3281
[2017-12-15 16:32:18] Epoch 0017 mean train/dev loss: 205897.4604 / 202655.1406
[2017-12-15 16:32:26] Epoch 0018 mean train/dev loss: 202895.3737 / 199616.4062
[2017-12-15 16:32:31] Epoch 0019 mean train/dev loss: 199907.7494 / 196615.3594
[2017-12-15 16:32:37] Epoch 0020 mean train/dev loss: 196970.4291 / 193652.7344
[2017-12-15 16:32:37] Checkpointing model at epoch 20 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:32:38] Model Checkpointing finished.
[2017-12-15 16:32:44] Epoch 0021 mean train/dev loss: 194059.2210 / 190724.5312
[2017-12-15 16:32:50] Epoch 0022 mean train/dev loss: 191154.0651 / 187831.4688
[2017-12-15 16:32:56] Epoch 0023 mean train/dev loss: 188322.2114 / 184970.7500
[2017-12-15 16:33:03] Epoch 0024 mean train/dev loss: 185494.9193 / 182147.6094
[2017-12-15 16:33:10] Epoch 0025 mean train/dev loss: 182666.9883 / 179350.7812
[2017-12-15 16:33:17] Epoch 0026 mean train/dev loss: 179880.3128 / 176581.9688
[2017-12-15 16:33:24] Epoch 0027 mean train/dev loss: 177120.8001 / 173847.8438
[2017-12-15 16:33:30] Epoch 0028 mean train/dev loss: 174414.8253 / 171141.0000
[2017-12-15 16:33:36] Epoch 0029 mean train/dev loss: 171714.2798 / 168462.5156
[2017-12-15 16:33:44] Epoch 0030 mean train/dev loss: 169032.9051 / 165810.8594
[2017-12-15 16:33:44] Learning rate decayed by 0.5000
[2017-12-15 16:33:44] Checkpointing model at epoch 30 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:33:44] Model Checkpointing finished.
[2017-12-15 16:33:52] Epoch 0031 mean train/dev loss: 167035.3506 / 164492.8906
[2017-12-15 16:33:59] Epoch 0032 mean train/dev loss: 165719.8474 / 163179.1406
[2017-12-15 16:34:04] Epoch 0033 mean train/dev loss: 164389.3504 / 161869.8438
[2017-12-15 16:34:11] Epoch 0034 mean train/dev loss: 163090.9346 / 160565.2188
[2017-12-15 16:34:18] Epoch 0035 mean train/dev loss: 161790.6564 / 159267.6875
[2017-12-15 16:34:25] Epoch 0036 mean train/dev loss: 160470.0072 / 157977.2344
[2017-12-15 16:34:32] Epoch 0037 mean train/dev loss: 159193.8266 / 156692.9844
[2017-12-15 16:34:39] Epoch 0038 mean train/dev loss: 157888.2971 / 155414.9531
[2017-12-15 16:34:46] Epoch 0039 mean train/dev loss: 156603.9032 / 154144.2500
[2017-12-15 16:34:54] Epoch 0040 mean train/dev loss: 155335.2411 / 152878.1250
[2017-12-15 16:34:54] Checkpointing model at epoch 40 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:34:54] Model Checkpointing finished.
[2017-12-15 16:35:01] Epoch 0041 mean train/dev loss: 154059.2272 / 151619.6094
[2017-12-15 16:35:07] Epoch 0042 mean train/dev loss: 152807.1903 / 150364.5625
[2017-12-15 16:35:14] Epoch 0043 mean train/dev loss: 151540.2461 / 149117.4844
[2017-12-15 16:35:21] Epoch 0044 mean train/dev loss: 150267.6898 / 147877.5781
[2017-12-15 16:35:28] Epoch 0045 mean train/dev loss: 149045.1469 / 146643.0469
[2017-12-15 16:35:28] Learning rate decayed by 0.5000
[2017-12-15 16:35:36] Epoch 0046 mean train/dev loss: 148092.6027 / 146028.1250
[2017-12-15 16:35:43] Epoch 0047 mean train/dev loss: 147472.7286 / 145413.4375
[2017-12-15 16:35:50] Epoch 0048 mean train/dev loss: 146868.8852 / 144799.8281
[2017-12-15 16:35:57] Epoch 0049 mean train/dev loss: 146243.2409 / 144186.4219
[2017-12-15 16:36:04] Epoch 0050 mean train/dev loss: 145636.3696 / 143573.6562
[2017-12-15 16:36:04] Checkpointing model at epoch 50 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:36:05] Model Checkpointing finished.
[2017-12-15 16:36:12] Epoch 0051 mean train/dev loss: 145009.5255 / 142963.5312
[2017-12-15 16:36:18] Epoch 0052 mean train/dev loss: 144403.2777 / 142354.1562
[2017-12-15 16:36:25] Epoch 0053 mean train/dev loss: 143771.8050 / 141746.3125
[2017-12-15 16:36:31] Epoch 0054 mean train/dev loss: 143163.4741 / 141140.8125
[2017-12-15 16:36:38] Epoch 0055 mean train/dev loss: 142540.4877 / 140536.5938
[2017-12-15 16:36:45] Epoch 0056 mean train/dev loss: 141944.9058 / 139933.2031
[2017-12-15 16:36:53] Epoch 0057 mean train/dev loss: 141321.4057 / 139331.3906
[2017-12-15 16:36:59] Epoch 0058 mean train/dev loss: 140727.6556 / 138731.2812
[2017-12-15 16:37:06] Epoch 0059 mean train/dev loss: 140130.6244 / 138132.6719
[2017-12-15 16:37:14] Epoch 0060 mean train/dev loss: 139513.9533 / 137535.3750
[2017-12-15 16:37:14] Learning rate decayed by 0.5000
[2017-12-15 16:37:14] Checkpointing model at epoch 60 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:37:14] Model Checkpointing finished.
[2017-12-15 16:37:20] Epoch 0061 mean train/dev loss: 139067.2038 / 137236.1406
[2017-12-15 16:37:27] Epoch 0062 mean train/dev loss: 138767.9931 / 136937.8594
[2017-12-15 16:37:33] Epoch 0063 mean train/dev loss: 138460.8956 / 136639.1875
[2017-12-15 16:37:38] Epoch 0064 mean train/dev loss: 138169.9123 / 136340.8906
[2017-12-15 16:37:46] Epoch 0065 mean train/dev loss: 137870.4255 / 136042.9688
[2017-12-15 16:37:52] Epoch 0066 mean train/dev loss: 137565.5320 / 135745.6250
[2017-12-15 16:37:59] Epoch 0067 mean train/dev loss: 137254.2188 / 135448.8438
[2017-12-15 16:38:07] Epoch 0068 mean train/dev loss: 136977.8828 / 135152.0781
[2017-12-15 16:38:12] Epoch 0069 mean train/dev loss: 136666.3361 / 134855.6094
[2017-12-15 16:38:17] Epoch 0070 mean train/dev loss: 136353.5825 / 134559.5312
[2017-12-15 16:38:17] Checkpointing model at epoch 70 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:38:17] Model Checkpointing finished.
[2017-12-15 16:38:23] Epoch 0071 mean train/dev loss: 136052.9349 / 134263.9531
[2017-12-15 16:38:28] Epoch 0072 mean train/dev loss: 135766.3227 / 133968.8594
[2017-12-15 16:38:34] Epoch 0073 mean train/dev loss: 135469.2171 / 133673.6562
[2017-12-15 16:38:41] Epoch 0074 mean train/dev loss: 135161.3317 / 133379.0625
[2017-12-15 16:38:46] Epoch 0075 mean train/dev loss: 134865.9506 / 133084.7188
[2017-12-15 16:38:46] Learning rate decayed by 0.5000
[2017-12-15 16:38:53] Epoch 0076 mean train/dev loss: 134631.7079 / 132939.5312
[2017-12-15 16:39:00] Epoch 0077 mean train/dev loss: 134518.2009 / 132793.8438
[2017-12-15 16:39:06] Epoch 0078 mean train/dev loss: 134360.7054 / 132648.5000
[2017-12-15 16:39:13] Epoch 0079 mean train/dev loss: 134194.5233 / 132503.2188
[2017-12-15 16:39:20] Epoch 0080 mean train/dev loss: 134067.7061 / 132358.0781
[2017-12-15 16:39:20] Checkpointing model at epoch 80 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:39:20] Model Checkpointing finished.
[2017-12-15 16:39:28] Epoch 0081 mean train/dev loss: 133906.6630 / 132213.0625
[2017-12-15 16:39:35] Epoch 0082 mean train/dev loss: 133761.6528 / 132068.0312
[2017-12-15 16:39:42] Epoch 0083 mean train/dev loss: 133612.8288 / 131923.0625
[2017-12-15 16:39:49] Epoch 0084 mean train/dev loss: 133478.6661 / 131778.8750
[2017-12-15 16:39:54] Epoch 0085 mean train/dev loss: 133316.4234 / 131634.1406
[2017-12-15 16:39:59] Epoch 0086 mean train/dev loss: 133178.5679 / 131489.2188
[2017-12-15 16:40:06] Epoch 0087 mean train/dev loss: 133016.8274 / 131344.5781
[2017-12-15 16:40:13] Epoch 0088 mean train/dev loss: 132893.9315 / 131200.0781
[2017-12-15 16:40:18] Epoch 0089 mean train/dev loss: 132755.4174 / 131055.5000
[2017-12-15 16:40:25] Epoch 0090 mean train/dev loss: 132601.4056 / 130911.5234
[2017-12-15 16:40:25] Learning rate decayed by 0.5000
[2017-12-15 16:40:25] Checkpointing model at epoch 90 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:40:26] Model Checkpointing finished.
[2017-12-15 16:40:33] Epoch 0091 mean train/dev loss: 132483.2968 / 130840.1641
[2017-12-15 16:40:40] Epoch 0092 mean train/dev loss: 132409.9789 / 130768.7422
[2017-12-15 16:40:46] Epoch 0093 mean train/dev loss: 132338.5730 / 130697.4062
[2017-12-15 16:40:52] Epoch 0094 mean train/dev loss: 132284.4787 / 130625.9766
[2017-12-15 16:40:59] Epoch 0095 mean train/dev loss: 132204.7534 / 130554.6641
[2017-12-15 16:41:06] Epoch 0096 mean train/dev loss: 132129.7546 / 130483.3359
[2017-12-15 16:41:13] Epoch 0097 mean train/dev loss: 132051.9740 / 130412.0234
[2017-12-15 16:41:18] Epoch 0098 mean train/dev loss: 131987.4085 / 130340.6797
[2017-12-15 16:41:24] Epoch 0099 mean train/dev loss: 131920.6403 / 130269.3828
[2017-12-15 16:41:32] Epoch 0100 mean train/dev loss: 131848.9027 / 130198.2734
[2017-12-15 16:41:32] Checkpointing model at epoch 100 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:41:32] Model Checkpointing finished.
[2017-12-15 16:41:40] Epoch 0101 mean train/dev loss: 131763.6320 / 130127.0391
[2017-12-15 16:41:45] Epoch 0102 mean train/dev loss: 131693.6374 / 130055.8750
[2017-12-15 16:41:50] Epoch 0103 mean train/dev loss: 131620.0788 / 129984.8047
[2017-12-15 16:41:55] Epoch 0104 mean train/dev loss: 131568.9071 / 129913.6016
[2017-12-15 16:41:59] Epoch 0105 mean train/dev loss: 131480.4671 / 129842.4531
[2017-12-15 16:41:59] Learning rate decayed by 0.5000
[2017-12-15 16:42:04] Epoch 0106 mean train/dev loss: 131428.3340 / 129806.9844
[2017-12-15 16:42:10] Epoch 0107 mean train/dev loss: 131381.0485 / 129771.4375
[2017-12-15 16:42:15] Epoch 0108 mean train/dev loss: 131345.6839 / 129735.8594
[2017-12-15 16:42:19] Epoch 0109 mean train/dev loss: 131318.8952 / 129700.3359
[2017-12-15 16:42:24] Epoch 0110 mean train/dev loss: 131289.4317 / 129664.8750
[2017-12-15 16:42:24] Checkpointing model at epoch 110 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:42:24] Model Checkpointing finished.
[2017-12-15 16:42:29] Epoch 0111 mean train/dev loss: 131250.5794 / 129629.3828
[2017-12-15 16:42:34] Epoch 0112 mean train/dev loss: 131211.9061 / 129593.8750
[2017-12-15 16:42:39] Epoch 0113 mean train/dev loss: 131170.4421 / 129558.3828
[2017-12-15 16:42:47] Epoch 0114 mean train/dev loss: 131144.8552 / 129522.9375
[2017-12-15 16:42:54] Epoch 0115 mean train/dev loss: 131116.9110 / 129487.4297
[2017-12-15 16:43:02] Epoch 0116 mean train/dev loss: 131062.7909 / 129451.9297
[2017-12-15 16:43:07] Epoch 0117 mean train/dev loss: 131039.2758 / 129416.3750
[2017-12-15 16:43:14] Epoch 0118 mean train/dev loss: 131006.6779 / 129380.8828
[2017-12-15 16:43:18] Epoch 0119 mean train/dev loss: 130953.6798 / 129345.4219
[2017-12-15 16:43:23] Epoch 0120 mean train/dev loss: 130924.5250 / 129309.9219
[2017-12-15 16:43:23] Learning rate decayed by 0.5000
[2017-12-15 16:43:23] Checkpointing model at epoch 120 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:43:23] Model Checkpointing finished.
[2017-12-15 16:43:29] Epoch 0121 mean train/dev loss: 130897.6307 / 129289.1719
[2017-12-15 16:43:35] Epoch 0122 mean train/dev loss: 130871.7193 / 129268.4062
[2017-12-15 16:43:39] Epoch 0123 mean train/dev loss: 130861.8241 / 129247.6641
[2017-12-15 16:43:44] Epoch 0124 mean train/dev loss: 130838.2207 / 129226.9531
[2017-12-15 16:43:49] Epoch 0125 mean train/dev loss: 130811.0251 / 129206.2656
[2017-12-15 16:43:53] Epoch 0126 mean train/dev loss: 130770.3248 / 129185.5391
[2017-12-15 16:43:58] Epoch 0127 mean train/dev loss: 130777.8838 / 129164.8438
[2017-12-15 16:44:05] Epoch 0128 mean train/dev loss: 130752.4617 / 129144.1172
[2017-12-15 16:44:11] Epoch 0129 mean train/dev loss: 130741.0654 / 129123.3906
[2017-12-15 16:44:16] Epoch 0130 mean train/dev loss: 130705.3644 / 129102.6797
[2017-12-15 16:44:16] Checkpointing model at epoch 130 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:44:16] Model Checkpointing finished.
[2017-12-15 16:44:22] Epoch 0131 mean train/dev loss: 130703.1814 / 129081.9375
[2017-12-15 16:44:29] Epoch 0132 mean train/dev loss: 130655.9117 / 129061.2109
[2017-12-15 16:44:35] Epoch 0133 mean train/dev loss: 130645.4388 / 129040.5000
[2017-12-15 16:44:40] Epoch 0134 mean train/dev loss: 130625.5930 / 129019.7969
[2017-12-15 16:44:46] Epoch 0135 mean train/dev loss: 130606.9574 / 128999.0859
[2017-12-15 16:44:46] Learning rate decayed by 0.5000
[2017-12-15 16:44:53] Epoch 0136 mean train/dev loss: 130585.7620 / 128991.9141
[2017-12-15 16:45:01] Epoch 0137 mean train/dev loss: 130591.4124 / 128984.6484
[2017-12-15 16:45:08] Epoch 0138 mean train/dev loss: 130576.9450 / 128977.4766
[2017-12-15 16:45:15] Epoch 0139 mean train/dev loss: 130571.2804 / 128970.2500
[2017-12-15 16:45:20] Epoch 0140 mean train/dev loss: 130570.9638 / 128963.0469
[2017-12-15 16:45:20] Checkpointing model at epoch 140 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:45:20] Model Checkpointing finished.
[2017-12-15 16:45:25] Epoch 0141 mean train/dev loss: 130552.1767 / 128955.9062
[2017-12-15 16:45:32] Epoch 0142 mean train/dev loss: 130534.9579 / 128948.7188
[2017-12-15 16:45:38] Epoch 0143 mean train/dev loss: 130535.1642 / 128941.5312
[2017-12-15 16:45:45] Epoch 0144 mean train/dev loss: 130531.6185 / 128934.3047
[2017-12-15 16:45:51] Epoch 0145 mean train/dev loss: 130511.5511 / 128927.1172
[2017-12-15 16:45:55] Epoch 0146 mean train/dev loss: 130526.6054 / 128919.9062
[2017-12-15 16:46:01] Epoch 0147 mean train/dev loss: 130507.8010 / 128912.6953
[2017-12-15 16:46:06] Epoch 0148 mean train/dev loss: 130512.2485 / 128905.5078
[2017-12-15 16:46:12] Epoch 0149 mean train/dev loss: 130491.8671 / 128898.2969
[2017-12-15 16:46:18] Epoch 0150 mean train/dev loss: 130471.7561 / 128891.1094
[2017-12-15 16:46:18] Learning rate decayed by 0.5000
[2017-12-15 16:46:18] Checkpointing model at epoch 150 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:46:18] Model Checkpointing finished.
[2017-12-15 16:46:23] Epoch 0151 mean train/dev loss: 130474.1928 / 128884.2344
[2017-12-15 16:46:27] Epoch 0152 mean train/dev loss: 130466.7913 / 128877.3750
[2017-12-15 16:46:32] Epoch 0153 mean train/dev loss: 130459.5942 / 128870.4766
[2017-12-15 16:46:40] Epoch 0154 mean train/dev loss: 130442.5782 / 128863.5938
[2017-12-15 16:46:45] Epoch 0155 mean train/dev loss: 130459.5959 / 128856.7188
[2017-12-15 16:46:50] Epoch 0156 mean train/dev loss: 130439.6505 / 128849.8438
[2017-12-15 16:46:54] Epoch 0157 mean train/dev loss: 130451.5631 / 128842.9453
[2017-12-15 16:47:00] Epoch 0158 mean train/dev loss: 130412.9613 / 128836.0703
[2017-12-15 16:47:04] Epoch 0159 mean train/dev loss: 130421.0724 / 128829.1875
[2017-12-15 16:47:09] Epoch 0160 mean train/dev loss: 130418.2665 / 128822.3281
[2017-12-15 16:47:09] Checkpointing model at epoch 160 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:47:09] Model Checkpointing finished.
[2017-12-15 16:47:14] Epoch 0161 mean train/dev loss: 130412.6770 / 128815.4531
[2017-12-15 16:47:19] Epoch 0162 mean train/dev loss: 130402.9784 / 128808.5703
[2017-12-15 16:47:25] Epoch 0163 mean train/dev loss: 130398.8149 / 128801.6719
[2017-12-15 16:47:31] Epoch 0164 mean train/dev loss: 130380.7730 / 128794.8281
[2017-12-15 16:47:36] Epoch 0165 mean train/dev loss: 130392.1674 / 128787.9297
[2017-12-15 16:47:36] Learning rate decayed by 0.5000
[2017-12-15 16:47:40] Epoch 0166 mean train/dev loss: 130379.1015 / 128787.9375
[2017-12-15 16:47:46] Epoch 0167 mean train/dev loss: 130389.4133 / 128787.9141
[2017-12-15 16:47:50] Epoch 0168 mean train/dev loss: 130371.2914 / 128787.9141
[2017-12-15 16:47:55] Epoch 0169 mean train/dev loss: 130392.1561 / 128787.9141
[2017-12-15 16:48:00] Epoch 0170 mean train/dev loss: 130385.5478 / 128787.9297
[2017-12-15 16:48:00] Checkpointing model at epoch 170 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:48:01] Model Checkpointing finished.
[2017-12-15 16:48:08] Epoch 0171 mean train/dev loss: 130375.4770 / 128787.8984
[2017-12-15 16:48:15] Epoch 0172 mean train/dev loss: 130369.4837 / 128787.9375
[2017-12-15 16:48:20] Epoch 0173 mean train/dev loss: 130376.1089 / 128787.9297
[2017-12-15 16:48:25] Epoch 0174 mean train/dev loss: 130392.4483 / 128787.9297
[2017-12-15 16:48:30] Epoch 0175 mean train/dev loss: 130385.4545 / 128787.8828
[2017-12-15 16:48:38] Epoch 0176 mean train/dev loss: 130379.3843 / 128787.8984
[2017-12-15 16:48:44] Epoch 0177 mean train/dev loss: 130373.2102 / 128787.9141
[2017-12-15 16:48:49] Epoch 0178 mean train/dev loss: 130393.7134 / 128787.8594
[2017-12-15 16:48:54] Epoch 0179 mean train/dev loss: 130377.5373 / 128787.8750
[2017-12-15 16:49:01] Epoch 0180 mean train/dev loss: 130375.3198 / 128787.8750
[2017-12-15 16:49:01] Learning rate decayed by 0.5000
[2017-12-15 16:49:01] Checkpointing model at epoch 180 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:49:01] Model Checkpointing finished.
[2017-12-15 16:49:06] Epoch 0181 mean train/dev loss: 130387.3247 / 128787.8594
[2017-12-15 16:49:12] Epoch 0182 mean train/dev loss: 130378.6943 / 128787.8594
[2017-12-15 16:49:16] Epoch 0183 mean train/dev loss: 130380.2194 / 128787.8594
[2017-12-15 16:49:21] Epoch 0184 mean train/dev loss: 130373.3289 / 128787.8750
[2017-12-15 16:49:26] Epoch 0185 mean train/dev loss: 130375.7775 / 128787.8750
[2017-12-15 16:49:33] Epoch 0186 mean train/dev loss: 130364.1640 / 128787.8594
[2017-12-15 16:49:39] Epoch 0187 mean train/dev loss: 130383.4948 / 128787.8750
[2017-12-15 16:49:44] Epoch 0188 mean train/dev loss: 130367.3478 / 128787.8750
[2017-12-15 16:49:49] Epoch 0189 mean train/dev loss: 130374.0605 / 128787.8750
[2017-12-15 16:49:54] Epoch 0190 mean train/dev loss: 130384.0408 / 128787.8750
[2017-12-15 16:49:54] Checkpointing model at epoch 190 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:49:54] Model Checkpointing finished.
[2017-12-15 16:49:59] Epoch 0191 mean train/dev loss: 130390.7814 / 128787.8750
[2017-12-15 16:50:03] Epoch 0192 mean train/dev loss: 130389.5797 / 128787.8594
[2017-12-15 16:50:09] Epoch 0193 mean train/dev loss: 130371.2381 / 128787.8594
[2017-12-15 16:50:16] Epoch 0194 mean train/dev loss: 130378.6940 / 128787.8594
[2017-12-15 16:50:22] Epoch 0195 mean train/dev loss: 130367.9267 / 128787.8750
[2017-12-15 16:50:22] Learning rate decayed by 0.5000
[2017-12-15 16:50:28] Epoch 0196 mean train/dev loss: 130387.7669 / 128787.8750
[2017-12-15 16:50:33] Epoch 0197 mean train/dev loss: 130375.9368 / 128787.8594
[2017-12-15 16:50:39] Epoch 0198 mean train/dev loss: 130379.6404 / 128787.8594
[2017-12-15 16:50:45] Epoch 0199 mean train/dev loss: 130380.3258 / 128787.8594
[2017-12-15 16:50:49] Epoch 0200 mean train/dev loss: 130372.2152 / 128787.8594
[2017-12-15 16:50:49] Checkpointing model at epoch 200 for ffn.hl_linear.lr_0.01.wd_0.01
[2017-12-15 16:50:49] Model Checkpointing finished.
[2017-12-15 16:50:49] 
                       *** Training finished *** 
[2017-12-15 16:50:50] Dev MSE: 128787.8594
[2017-12-15 16:50:54] Training MSE: 130377.2188
[2017-12-15 16:50:55] Experiment ffn.hl_linear.lr_0.01.wd_0.01 logging ended.
