[2017-12-15 15:38:11] Experiment ffn.hl_50_50_50.lr_0.1.wd_0.001 logging started.
[2017-12-15 15:38:11] 
                       *** Starting Experiment ffn.hl_50_50_50.lr_0.1.wd_0.001 ***
                      
[2017-12-15 15:38:11] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 15:38:11] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 50)
                        (relu3): ReLU ()
                        (linear4): Linear (50 -> 1)
                      )
[2017-12-15 15:38:11]  *** Training on GPU ***
[2017-12-15 15:38:20] Epoch 0001 mean train/dev loss: 2568.8210 / 658.5235
[2017-12-15 15:38:28] Epoch 0002 mean train/dev loss: 354.3223 / 224.5293
[2017-12-15 15:38:37] Epoch 0003 mean train/dev loss: 247.0971 / 633.3403
[2017-12-15 15:38:46] Epoch 0004 mean train/dev loss: 1024.2947 / 159.9670
[2017-12-15 15:38:54] Epoch 0005 mean train/dev loss: 106.9873 / 132.6776
[2017-12-15 15:39:03] Epoch 0006 mean train/dev loss: 113.8760 / 179.8199
[2017-12-15 15:39:11] Epoch 0007 mean train/dev loss: 141.4074 / 175.4567
[2017-12-15 15:39:20] Epoch 0008 mean train/dev loss: 157.2644 / 161.5759
[2017-12-15 15:39:28] Epoch 0009 mean train/dev loss: 2160.8404 / 133.5140
[2017-12-15 15:39:37] Epoch 0010 mean train/dev loss: 110.3761 / 116.1798
[2017-12-15 15:39:37] Checkpointing model at epoch 10 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:39:37] Model Checkpointing finished.
[2017-12-15 15:39:46] Epoch 0011 mean train/dev loss: 99.7809 / 105.7094
[2017-12-15 15:39:54] Epoch 0012 mean train/dev loss: 112.0588 / 240.3623
[2017-12-15 15:40:02] Epoch 0013 mean train/dev loss: 330.4484 / 159.9451
[2017-12-15 15:40:10] Epoch 0014 mean train/dev loss: 117.3938 / 148.3040
[2017-12-15 15:40:19] Epoch 0015 mean train/dev loss: 109.7956 / 136.9641
[2017-12-15 15:40:19] Learning rate decayed by 0.5000
[2017-12-15 15:40:27] Epoch 0016 mean train/dev loss: 93.3110 / 116.4834
[2017-12-15 15:40:36] Epoch 0017 mean train/dev loss: 89.7549 / 130.7852
[2017-12-15 15:40:45] Epoch 0018 mean train/dev loss: 270.7827 / 126.1963
[2017-12-15 15:40:53] Epoch 0019 mean train/dev loss: 94.7102 / 137.5280
[2017-12-15 15:41:02] Epoch 0020 mean train/dev loss: 90.7416 / 108.3434
[2017-12-15 15:41:02] Checkpointing model at epoch 20 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:41:02] Model Checkpointing finished.
[2017-12-15 15:41:11] Epoch 0021 mean train/dev loss: 86.9801 / 143.3582
[2017-12-15 15:41:19] Epoch 0022 mean train/dev loss: 88.7333 / 125.6254
[2017-12-15 15:41:27] Epoch 0023 mean train/dev loss: 97.2523 / 139.6539
[2017-12-15 15:41:36] Epoch 0024 mean train/dev loss: 83.4341 / 185.9449
[2017-12-15 15:41:44] Epoch 0025 mean train/dev loss: 86.7089 / 124.9465
[2017-12-15 15:41:52] Epoch 0026 mean train/dev loss: 87.8750 / 95.5910
[2017-12-15 15:42:01] Epoch 0027 mean train/dev loss: 80.0339 / 140.7717
[2017-12-15 15:42:10] Epoch 0028 mean train/dev loss: 82.9902 / 138.3941
[2017-12-15 15:42:18] Epoch 0029 mean train/dev loss: 83.2337 / 97.9509
[2017-12-15 15:42:26] Epoch 0030 mean train/dev loss: 80.4534 / 111.7247
[2017-12-15 15:42:26] Learning rate decayed by 0.5000
[2017-12-15 15:42:26] Checkpointing model at epoch 30 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:42:27] Model Checkpointing finished.
[2017-12-15 15:42:35] Epoch 0031 mean train/dev loss: 66.7107 / 96.3006
[2017-12-15 15:42:44] Epoch 0032 mean train/dev loss: 66.9614 / 99.3082
[2017-12-15 15:42:52] Epoch 0033 mean train/dev loss: 67.9235 / 110.5905
[2017-12-15 15:43:01] Epoch 0034 mean train/dev loss: 67.9029 / 100.9990
[2017-12-15 15:43:09] Epoch 0035 mean train/dev loss: 68.4311 / 103.5928
[2017-12-15 15:43:17] Epoch 0036 mean train/dev loss: 67.2766 / 93.5908
[2017-12-15 15:43:26] Epoch 0037 mean train/dev loss: 71.3918 / 107.4285
[2017-12-15 15:43:34] Epoch 0038 mean train/dev loss: 67.2013 / 132.9057
[2017-12-15 15:43:42] Epoch 0039 mean train/dev loss: 65.5823 / 111.3947
[2017-12-15 15:43:51] Epoch 0040 mean train/dev loss: 65.9284 / 102.5653
[2017-12-15 15:43:51] Checkpointing model at epoch 40 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:43:51] Model Checkpointing finished.
[2017-12-15 15:44:00] Epoch 0041 mean train/dev loss: 65.6922 / 92.9800
[2017-12-15 15:44:08] Epoch 0042 mean train/dev loss: 66.8711 / 110.5521
[2017-12-15 15:44:16] Epoch 0043 mean train/dev loss: 65.3809 / 110.2519
[2017-12-15 15:44:25] Epoch 0044 mean train/dev loss: 65.4895 / 120.9792
[2017-12-15 15:44:33] Epoch 0045 mean train/dev loss: 64.5646 / 127.0832
[2017-12-15 15:44:33] Learning rate decayed by 0.5000
[2017-12-15 15:44:42] Epoch 0046 mean train/dev loss: 60.0370 / 109.2390
[2017-12-15 15:44:50] Epoch 0047 mean train/dev loss: 60.2564 / 98.6479
[2017-12-15 15:44:59] Epoch 0048 mean train/dev loss: 60.4288 / 113.7291
[2017-12-15 15:45:07] Epoch 0049 mean train/dev loss: 59.4407 / 114.9765
[2017-12-15 15:45:15] Epoch 0050 mean train/dev loss: 58.7570 / 126.1697
[2017-12-15 15:45:15] Checkpointing model at epoch 50 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:45:16] Model Checkpointing finished.
[2017-12-15 15:45:24] Epoch 0051 mean train/dev loss: 58.5801 / 116.3315
[2017-12-15 15:45:32] Epoch 0052 mean train/dev loss: 58.9433 / 105.6819
[2017-12-15 15:45:41] Epoch 0053 mean train/dev loss: 58.3071 / 146.2316
[2017-12-15 15:45:49] Epoch 0054 mean train/dev loss: 58.3996 / 114.8425
[2017-12-15 15:45:57] Epoch 0055 mean train/dev loss: 58.2390 / 111.6386
[2017-12-15 15:46:06] Epoch 0056 mean train/dev loss: 57.6598 / 110.5683
[2017-12-15 15:46:14] Epoch 0057 mean train/dev loss: 57.0827 / 113.1477
[2017-12-15 15:46:23] Epoch 0058 mean train/dev loss: 54.7799 / 106.7419
[2017-12-15 15:46:31] Epoch 0059 mean train/dev loss: 53.7970 / 109.3161
[2017-12-15 15:46:40] Epoch 0060 mean train/dev loss: 52.9412 / 100.5462
[2017-12-15 15:46:40] Learning rate decayed by 0.5000
[2017-12-15 15:46:40] Checkpointing model at epoch 60 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:46:40] Model Checkpointing finished.
[2017-12-15 15:46:49] Epoch 0061 mean train/dev loss: 50.4274 / 105.6174
[2017-12-15 15:46:57] Epoch 0062 mean train/dev loss: 50.1153 / 99.9365
[2017-12-15 15:47:05] Epoch 0063 mean train/dev loss: 50.1080 / 104.2949
[2017-12-15 15:47:13] Epoch 0064 mean train/dev loss: 50.0393 / 103.2045
[2017-12-15 15:47:21] Epoch 0065 mean train/dev loss: 49.8170 / 95.7769
[2017-12-15 15:47:30] Epoch 0066 mean train/dev loss: 49.8853 / 94.2090
[2017-12-15 15:47:38] Epoch 0067 mean train/dev loss: 49.7554 / 99.0201
[2017-12-15 15:47:47] Epoch 0068 mean train/dev loss: 49.6235 / 93.8212
[2017-12-15 15:47:55] Epoch 0069 mean train/dev loss: 49.5009 / 91.3236
[2017-12-15 15:48:04] Epoch 0070 mean train/dev loss: 49.2514 / 96.6785
[2017-12-15 15:48:04] Checkpointing model at epoch 70 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:48:05] Model Checkpointing finished.
[2017-12-15 15:48:13] Epoch 0071 mean train/dev loss: 49.1396 / 85.1515
[2017-12-15 15:48:21] Epoch 0072 mean train/dev loss: 48.9901 / 95.4102
[2017-12-15 15:48:28] Epoch 0073 mean train/dev loss: 49.1132 / 94.6552
[2017-12-15 15:48:35] Epoch 0074 mean train/dev loss: 48.8599 / 97.6596
[2017-12-15 15:48:43] Epoch 0075 mean train/dev loss: 49.0169 / 87.1510
[2017-12-15 15:48:43] Learning rate decayed by 0.5000
[2017-12-15 15:48:51] Epoch 0076 mean train/dev loss: 47.8419 / 88.2638
[2017-12-15 15:48:58] Epoch 0077 mean train/dev loss: 47.7104 / 89.3181
[2017-12-15 15:49:05] Epoch 0078 mean train/dev loss: 47.7941 / 90.2533
[2017-12-15 15:49:12] Epoch 0079 mean train/dev loss: 47.6469 / 90.1102
[2017-12-15 15:49:20] Epoch 0080 mean train/dev loss: 47.5165 / 95.1141
[2017-12-15 15:49:20] Checkpointing model at epoch 80 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:49:20] Model Checkpointing finished.
[2017-12-15 15:49:28] Epoch 0081 mean train/dev loss: 47.4371 / 91.2934
[2017-12-15 15:49:35] Epoch 0082 mean train/dev loss: 47.4991 / 88.2890
[2017-12-15 15:49:42] Epoch 0083 mean train/dev loss: 47.2854 / 92.6122
[2017-12-15 15:49:49] Epoch 0084 mean train/dev loss: 47.4208 / 86.5493
[2017-12-15 15:49:57] Epoch 0085 mean train/dev loss: 47.1256 / 87.9727
[2017-12-15 15:50:04] Epoch 0086 mean train/dev loss: 47.0647 / 95.2306
[2017-12-15 15:50:11] Epoch 0087 mean train/dev loss: 47.1533 / 86.3499
[2017-12-15 15:50:18] Epoch 0088 mean train/dev loss: 47.1221 / 87.9712
[2017-12-15 15:50:26] Epoch 0089 mean train/dev loss: 47.0269 / 90.1141
[2017-12-15 15:50:34] Epoch 0090 mean train/dev loss: 46.9346 / 87.1036
[2017-12-15 15:50:34] Learning rate decayed by 0.5000
[2017-12-15 15:50:34] Checkpointing model at epoch 90 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:50:34] Model Checkpointing finished.
[2017-12-15 15:50:42] Epoch 0091 mean train/dev loss: 46.3038 / 88.2318
[2017-12-15 15:50:50] Epoch 0092 mean train/dev loss: 46.2992 / 88.0910
[2017-12-15 15:50:57] Epoch 0093 mean train/dev loss: 46.3276 / 83.5476
[2017-12-15 15:51:05] Epoch 0094 mean train/dev loss: 46.2419 / 87.6220
[2017-12-15 15:51:12] Epoch 0095 mean train/dev loss: 46.1980 / 88.9560
[2017-12-15 15:51:19] Epoch 0096 mean train/dev loss: 46.1435 / 88.8237
[2017-12-15 15:51:26] Epoch 0097 mean train/dev loss: 46.1833 / 90.7727
[2017-12-15 15:51:33] Epoch 0098 mean train/dev loss: 46.0890 / 87.8668
[2017-12-15 15:51:41] Epoch 0099 mean train/dev loss: 46.0900 / 92.8280
[2017-12-15 15:51:48] Epoch 0100 mean train/dev loss: 46.0485 / 88.4149
[2017-12-15 15:51:48] Checkpointing model at epoch 100 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:51:48] Model Checkpointing finished.
[2017-12-15 15:51:56] Epoch 0101 mean train/dev loss: 46.0431 / 89.4454
[2017-12-15 15:52:03] Epoch 0102 mean train/dev loss: 45.9290 / 91.0218
[2017-12-15 15:52:11] Epoch 0103 mean train/dev loss: 45.9091 / 88.9705
[2017-12-15 15:52:18] Epoch 0104 mean train/dev loss: 45.9111 / 84.2888
[2017-12-15 15:52:25] Epoch 0105 mean train/dev loss: 45.9353 / 88.0921
[2017-12-15 15:52:25] Learning rate decayed by 0.5000
[2017-12-15 15:52:33] Epoch 0106 mean train/dev loss: 45.5434 / 89.1867
[2017-12-15 15:52:40] Epoch 0107 mean train/dev loss: 45.5410 / 88.7226
[2017-12-15 15:52:47] Epoch 0108 mean train/dev loss: 45.4954 / 88.5169
[2017-12-15 15:52:54] Epoch 0109 mean train/dev loss: 45.5040 / 91.1962
[2017-12-15 15:53:01] Epoch 0110 mean train/dev loss: 45.5005 / 88.4364
[2017-12-15 15:53:01] Checkpointing model at epoch 110 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:53:01] Model Checkpointing finished.
[2017-12-15 15:53:08] Epoch 0111 mean train/dev loss: 45.4467 / 90.3654
[2017-12-15 15:53:16] Epoch 0112 mean train/dev loss: 45.4340 / 91.0838
[2017-12-15 15:53:23] Epoch 0113 mean train/dev loss: 45.3704 / 91.9878
[2017-12-15 15:53:30] Epoch 0114 mean train/dev loss: 45.3550 / 89.0598
[2017-12-15 15:53:37] Epoch 0115 mean train/dev loss: 45.3114 / 90.8436
[2017-12-15 15:53:44] Epoch 0116 mean train/dev loss: 45.2588 / 88.4047
[2017-12-15 15:53:50] Epoch 0117 mean train/dev loss: 45.1882 / 88.5971
[2017-12-15 15:53:56] Epoch 0118 mean train/dev loss: 45.1407 / 90.8286
[2017-12-15 15:54:02] Epoch 0119 mean train/dev loss: 45.1383 / 87.1697
[2017-12-15 15:54:07] Epoch 0120 mean train/dev loss: 45.0940 / 91.1087
[2017-12-15 15:54:07] Learning rate decayed by 0.5000
[2017-12-15 15:54:07] Checkpointing model at epoch 120 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:54:08] Model Checkpointing finished.
[2017-12-15 15:54:13] Epoch 0121 mean train/dev loss: 44.8891 / 89.7653
[2017-12-15 15:54:19] Epoch 0122 mean train/dev loss: 44.8784 / 91.0379
[2017-12-15 15:54:25] Epoch 0123 mean train/dev loss: 44.8655 / 89.0918
[2017-12-15 15:54:31] Epoch 0124 mean train/dev loss: 44.8446 / 89.2647
[2017-12-15 15:54:36] Epoch 0125 mean train/dev loss: 44.8338 / 90.5206
[2017-12-15 15:54:42] Epoch 0126 mean train/dev loss: 44.8210 / 88.3411
[2017-12-15 15:54:48] Epoch 0127 mean train/dev loss: 44.8049 / 88.6304
[2017-12-15 15:54:53] Epoch 0128 mean train/dev loss: 44.7611 / 89.3924
[2017-12-15 15:54:59] Epoch 0129 mean train/dev loss: 44.7796 / 89.1514
[2017-12-15 15:55:05] Epoch 0130 mean train/dev loss: 44.7311 / 91.7336
[2017-12-15 15:55:05] Checkpointing model at epoch 130 for ffn.hl_50_50_50.lr_0.1.wd_0.001
[2017-12-15 15:55:05] Model Checkpointing finished.
[2017-12-15 15:55:10] Epoch 0131 mean train/dev loss: 44.7313 / 88.5795
[2017-12-15 15:55:16] Epoch 0132 mean train/dev loss: 44.6868 / 90.9070
[2017-12-15 15:55:22] Epoch 0133 mean train/dev loss: 44.6863 / 89.6982
[2017-12-15 15:55:28] Epoch 0134 mean train/dev loss: 44.6574 / 89.7004
[2017-12-15 15:55:28] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:55:28] 
                       *** Training finished *** 
[2017-12-15 15:55:29] Dev MSE: 89.7004
[2017-12-15 15:55:34] Training MSE: 44.6166
[2017-12-15 15:55:36] Experiment ffn.hl_50_50_50.lr_0.1.wd_0.001 logging ended.
