[2017-12-15 18:20:00] Experiment ffn.hl_100_100.lr_0.01.wd_0.01 logging started.
[2017-12-15 18:20:00] 
                       *** Starting Experiment ffn.hl_100_100.lr_0.01.wd_0.01 ***
                      
[2017-12-15 18:20:00] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100, 100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 18:20:00] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 100)
                        (relu2): ReLU ()
                        (linear3): Linear (100 -> 1)
                      )
[2017-12-15 18:20:00]  *** Training on GPU ***
[2017-12-15 18:20:08] Epoch 0001 mean train/dev loss: 14415.5976 / 338.3936
[2017-12-15 18:20:17] Epoch 0002 mean train/dev loss: 129.8341 / 158.9848
[2017-12-15 18:20:25] Epoch 0003 mean train/dev loss: 114.6497 / 157.8243
[2017-12-15 18:20:33] Epoch 0004 mean train/dev loss: 112.8840 / 153.7085
[2017-12-15 18:20:41] Epoch 0005 mean train/dev loss: 112.7818 / 149.9001
[2017-12-15 18:20:49] Epoch 0006 mean train/dev loss: 110.4106 / 160.2469
[2017-12-15 18:20:57] Epoch 0007 mean train/dev loss: 108.4995 / 166.0479
[2017-12-15 18:21:05] Epoch 0008 mean train/dev loss: 103.0927 / 172.6159
[2017-12-15 18:21:13] Epoch 0009 mean train/dev loss: 97.1179 / 163.6877
[2017-12-15 18:21:21] Epoch 0010 mean train/dev loss: 95.4489 / 158.3867
[2017-12-15 18:21:21] Checkpointing model at epoch 10 for ffn.hl_100_100.lr_0.01.wd_0.01
[2017-12-15 18:21:22] Model Checkpointing finished.
[2017-12-15 18:21:30] Epoch 0011 mean train/dev loss: 90.4464 / 142.0432
[2017-12-15 18:21:37] Epoch 0012 mean train/dev loss: 89.3700 / 157.4133
[2017-12-15 18:21:45] Epoch 0013 mean train/dev loss: 88.1040 / 140.1777
[2017-12-15 18:21:53] Epoch 0014 mean train/dev loss: 85.2941 / 136.1026
[2017-12-15 18:22:01] Epoch 0015 mean train/dev loss: 86.6860 / 162.5023
[2017-12-15 18:22:01] Learning rate decayed by 0.5000
[2017-12-15 18:22:09] Epoch 0016 mean train/dev loss: 78.0149 / 119.3437
[2017-12-15 18:22:17] Epoch 0017 mean train/dev loss: 77.2565 / 128.6720
[2017-12-15 18:22:25] Epoch 0018 mean train/dev loss: 76.4139 / 141.2234
[2017-12-15 18:22:33] Epoch 0019 mean train/dev loss: 75.7942 / 136.5246
[2017-12-15 18:22:41] Epoch 0020 mean train/dev loss: 75.6770 / 157.0750
[2017-12-15 18:22:41] Checkpointing model at epoch 20 for ffn.hl_100_100.lr_0.01.wd_0.01
[2017-12-15 18:22:41] Model Checkpointing finished.
[2017-12-15 18:22:49] Epoch 0021 mean train/dev loss: 75.2518 / 151.2925
[2017-12-15 18:22:57] Epoch 0022 mean train/dev loss: 74.2739 / 128.3802
[2017-12-15 18:23:05] Epoch 0023 mean train/dev loss: 75.2091 / 132.9258
[2017-12-15 18:23:13] Epoch 0024 mean train/dev loss: 73.9563 / 146.3625
[2017-12-15 18:23:21] Epoch 0025 mean train/dev loss: 74.0929 / 122.2750
[2017-12-15 18:23:29] Epoch 0026 mean train/dev loss: 73.8479 / 125.4906
[2017-12-15 18:23:38] Epoch 0027 mean train/dev loss: 73.1337 / 126.4128
[2017-12-15 18:23:46] Epoch 0028 mean train/dev loss: 73.6016 / 123.9414
[2017-12-15 18:23:54] Epoch 0029 mean train/dev loss: 72.3487 / 128.3878
[2017-12-15 18:24:02] Epoch 0030 mean train/dev loss: 72.8415 / 135.5518
[2017-12-15 18:24:02] Learning rate decayed by 0.5000
[2017-12-15 18:24:02] Checkpointing model at epoch 30 for ffn.hl_100_100.lr_0.01.wd_0.01
[2017-12-15 18:24:02] Model Checkpointing finished.
[2017-12-15 18:24:10] Epoch 0031 mean train/dev loss: 69.6950 / 127.0443
[2017-12-15 18:24:18] Epoch 0032 mean train/dev loss: 69.5191 / 125.9059
[2017-12-15 18:24:26] Epoch 0033 mean train/dev loss: 69.5576 / 127.3413
[2017-12-15 18:24:34] Epoch 0034 mean train/dev loss: 69.2258 / 123.4119
[2017-12-15 18:24:42] Epoch 0035 mean train/dev loss: 69.3035 / 117.2554
[2017-12-15 18:24:50] Epoch 0036 mean train/dev loss: 69.3023 / 109.5325
[2017-12-15 18:24:58] Epoch 0037 mean train/dev loss: 68.6881 / 106.4186
[2017-12-15 18:25:06] Epoch 0038 mean train/dev loss: 68.6526 / 98.1464
[2017-12-15 18:25:14] Epoch 0039 mean train/dev loss: 68.2217 / 127.7707
[2017-12-15 18:25:22] Epoch 0040 mean train/dev loss: 68.4796 / 110.1290
[2017-12-15 18:25:22] Checkpointing model at epoch 40 for ffn.hl_100_100.lr_0.01.wd_0.01
[2017-12-15 18:25:22] Model Checkpointing finished.
[2017-12-15 18:25:30] Epoch 0041 mean train/dev loss: 67.8436 / 112.3967
[2017-12-15 18:25:38] Epoch 0042 mean train/dev loss: 67.6217 / 119.5018
[2017-12-15 18:25:46] Epoch 0043 mean train/dev loss: 67.4482 / 125.6711
[2017-12-15 18:25:54] Epoch 0044 mean train/dev loss: 67.3777 / 108.3243
[2017-12-15 18:26:03] Epoch 0045 mean train/dev loss: 67.3830 / 118.1072
[2017-12-15 18:26:03] Learning rate decayed by 0.5000
[2017-12-15 18:26:11] Epoch 0046 mean train/dev loss: 65.5838 / 111.0077
[2017-12-15 18:26:19] Epoch 0047 mean train/dev loss: 65.7119 / 117.7213
[2017-12-15 18:26:27] Epoch 0048 mean train/dev loss: 65.6314 / 111.4625
[2017-12-15 18:26:35] Epoch 0049 mean train/dev loss: 65.5664 / 113.6846
[2017-12-15 18:26:43] Epoch 0050 mean train/dev loss: 65.3639 / 103.4061
[2017-12-15 18:26:43] Checkpointing model at epoch 50 for ffn.hl_100_100.lr_0.01.wd_0.01
[2017-12-15 18:26:43] Model Checkpointing finished.
[2017-12-15 18:26:51] Epoch 0051 mean train/dev loss: 65.3353 / 120.3685
[2017-12-15 18:26:59] Epoch 0052 mean train/dev loss: 65.3284 / 114.7286
[2017-12-15 18:27:07] Epoch 0053 mean train/dev loss: 65.2858 / 127.0700
[2017-12-15 18:27:15] Epoch 0054 mean train/dev loss: 65.2134 / 111.7895
[2017-12-15 18:27:23] Epoch 0055 mean train/dev loss: 64.9821 / 111.7437
[2017-12-15 18:27:31] Epoch 0056 mean train/dev loss: 64.8433 / 119.6280
[2017-12-15 18:27:39] Epoch 0057 mean train/dev loss: 64.6736 / 113.9739
[2017-12-15 18:27:47] Epoch 0058 mean train/dev loss: 64.7558 / 102.9179
[2017-12-15 18:27:54] Epoch 0059 mean train/dev loss: 64.6128 / 105.4250
[2017-12-15 18:28:03] Epoch 0060 mean train/dev loss: 64.4510 / 108.7820
[2017-12-15 18:28:03] Learning rate decayed by 0.5000
[2017-12-15 18:28:03] Checkpointing model at epoch 60 for ffn.hl_100_100.lr_0.01.wd_0.01
[2017-12-15 18:28:03] Model Checkpointing finished.
[2017-12-15 18:28:11] Epoch 0061 mean train/dev loss: 63.6675 / 103.5696
[2017-12-15 18:28:19] Epoch 0062 mean train/dev loss: 63.6944 / 111.6302
[2017-12-15 18:28:27] Epoch 0063 mean train/dev loss: 63.7115 / 106.3386
[2017-12-15 18:28:35] Epoch 0064 mean train/dev loss: 63.6027 / 111.3972
[2017-12-15 18:28:43] Epoch 0065 mean train/dev loss: 63.5809 / 112.0141
[2017-12-15 18:28:51] Epoch 0066 mean train/dev loss: 63.4915 / 120.2318
[2017-12-15 18:28:59] Epoch 0067 mean train/dev loss: 63.4002 / 109.9074
[2017-12-15 18:29:07] Epoch 0068 mean train/dev loss: 63.4408 / 110.5725
[2017-12-15 18:29:15] Epoch 0069 mean train/dev loss: 63.3401 / 110.6614
[2017-12-15 18:29:23] Epoch 0070 mean train/dev loss: 63.3248 / 112.4089
[2017-12-15 18:29:23] Checkpointing model at epoch 70 for ffn.hl_100_100.lr_0.01.wd_0.01
[2017-12-15 18:29:23] Model Checkpointing finished.
[2017-12-15 18:29:30] Epoch 0071 mean train/dev loss: 63.2837 / 119.2724
[2017-12-15 18:29:35] Epoch 0072 mean train/dev loss: 63.1320 / 111.8709
[2017-12-15 18:29:40] Epoch 0073 mean train/dev loss: 63.1854 / 112.6209
[2017-12-15 18:29:45] Epoch 0074 mean train/dev loss: 63.1032 / 111.3082
[2017-12-15 18:29:51] Epoch 0075 mean train/dev loss: 63.0053 / 113.0655
[2017-12-15 18:29:51] Learning rate decayed by 0.5000
[2017-12-15 18:29:56] Epoch 0076 mean train/dev loss: 62.5914 / 112.0808
[2017-12-15 18:30:02] Epoch 0077 mean train/dev loss: 62.5876 / 111.4632
[2017-12-15 18:30:07] Epoch 0078 mean train/dev loss: 62.5189 / 109.9393
[2017-12-15 18:30:13] Epoch 0079 mean train/dev loss: 62.5762 / 112.3758
[2017-12-15 18:30:13] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:30:13] 
                       *** Training finished *** 
[2017-12-15 18:30:14] Dev MSE: 112.3758
[2017-12-15 18:30:19] Training MSE: 62.3647
[2017-12-15 18:30:20] Experiment ffn.hl_100_100.lr_0.01.wd_0.01 logging ended.
