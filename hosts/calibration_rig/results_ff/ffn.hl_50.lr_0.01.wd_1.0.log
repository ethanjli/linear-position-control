[2017-12-15 17:28:30] Experiment ffn.hl_50.lr_0.01.wd_1.0 logging started.
[2017-12-15 17:28:30] 
                       *** Starting Experiment ffn.hl_50.lr_0.01.wd_1.0 ***
                      
[2017-12-15 17:28:30] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 17:28:30] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 1)
                      )
[2017-12-15 17:28:30]  *** Training on GPU ***
[2017-12-15 17:28:38] Epoch 0001 mean train/dev loss: 72798.7400 / 2142.3552
[2017-12-15 17:28:46] Epoch 0002 mean train/dev loss: 752.9168 / 1188.5557
[2017-12-15 17:28:53] Epoch 0003 mean train/dev loss: 369.1880 / 489.0258
[2017-12-15 17:29:00] Epoch 0004 mean train/dev loss: 238.2434 / 319.5695
[2017-12-15 17:29:08] Epoch 0005 mean train/dev loss: 196.1043 / 255.1696
[2017-12-15 17:29:16] Epoch 0006 mean train/dev loss: 170.9910 / 210.0204
[2017-12-15 17:29:24] Epoch 0007 mean train/dev loss: 145.5030 / 167.4792
[2017-12-15 17:29:31] Epoch 0008 mean train/dev loss: 134.8445 / 143.2023
[2017-12-15 17:29:39] Epoch 0009 mean train/dev loss: 127.6961 / 130.8276
[2017-12-15 17:29:46] Epoch 0010 mean train/dev loss: 122.2106 / 123.6220
[2017-12-15 17:29:46] Checkpointing model at epoch 10 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:29:46] Model Checkpointing finished.
[2017-12-15 17:29:54] Epoch 0011 mean train/dev loss: 120.5543 / 116.8136
[2017-12-15 17:30:01] Epoch 0012 mean train/dev loss: 118.0216 / 114.7551
[2017-12-15 17:30:08] Epoch 0013 mean train/dev loss: 116.8274 / 112.8579
[2017-12-15 17:30:16] Epoch 0014 mean train/dev loss: 116.2486 / 112.0709
[2017-12-15 17:30:23] Epoch 0015 mean train/dev loss: 115.9948 / 110.5187
[2017-12-15 17:30:23] Learning rate decayed by 0.5000
[2017-12-15 17:30:31] Epoch 0016 mean train/dev loss: 114.6914 / 111.1281
[2017-12-15 17:30:38] Epoch 0017 mean train/dev loss: 114.6858 / 113.0889
[2017-12-15 17:30:45] Epoch 0018 mean train/dev loss: 114.6966 / 110.4821
[2017-12-15 17:30:52] Epoch 0019 mean train/dev loss: 114.6062 / 110.5128
[2017-12-15 17:30:59] Epoch 0020 mean train/dev loss: 114.6792 / 111.0014
[2017-12-15 17:30:59] Checkpointing model at epoch 20 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:30:59] Model Checkpointing finished.
[2017-12-15 17:31:07] Epoch 0021 mean train/dev loss: 114.5860 / 111.4607
[2017-12-15 17:31:14] Epoch 0022 mean train/dev loss: 114.6523 / 111.8153
[2017-12-15 17:31:22] Epoch 0023 mean train/dev loss: 114.6989 / 108.9079
[2017-12-15 17:31:29] Epoch 0024 mean train/dev loss: 114.5506 / 110.3649
[2017-12-15 17:31:36] Epoch 0025 mean train/dev loss: 114.7544 / 111.7830
[2017-12-15 17:31:43] Epoch 0026 mean train/dev loss: 114.6939 / 108.8400
[2017-12-15 17:31:50] Epoch 0027 mean train/dev loss: 114.6676 / 110.2566
[2017-12-15 17:31:58] Epoch 0028 mean train/dev loss: 114.6053 / 108.9798
[2017-12-15 17:32:05] Epoch 0029 mean train/dev loss: 114.6044 / 109.5902
[2017-12-15 17:32:13] Epoch 0030 mean train/dev loss: 114.7121 / 108.9842
[2017-12-15 17:32:13] Learning rate decayed by 0.5000
[2017-12-15 17:32:13] Checkpointing model at epoch 30 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:32:13] Model Checkpointing finished.
[2017-12-15 17:32:20] Epoch 0031 mean train/dev loss: 114.2319 / 109.7102
[2017-12-15 17:32:27] Epoch 0032 mean train/dev loss: 114.1552 / 110.2424
[2017-12-15 17:32:34] Epoch 0033 mean train/dev loss: 114.2134 / 109.9017
[2017-12-15 17:32:42] Epoch 0034 mean train/dev loss: 114.2777 / 109.2720
[2017-12-15 17:32:49] Epoch 0035 mean train/dev loss: 114.1917 / 111.2492
[2017-12-15 17:32:56] Epoch 0036 mean train/dev loss: 114.1879 / 109.8311
[2017-12-15 17:33:03] Epoch 0037 mean train/dev loss: 114.1716 / 111.4807
[2017-12-15 17:33:11] Epoch 0038 mean train/dev loss: 114.2030 / 110.0076
[2017-12-15 17:33:18] Epoch 0039 mean train/dev loss: 114.1515 / 110.9579
[2017-12-15 17:33:25] Epoch 0040 mean train/dev loss: 114.1175 / 109.4804
[2017-12-15 17:33:25] Checkpointing model at epoch 40 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:33:26] Model Checkpointing finished.
[2017-12-15 17:33:33] Epoch 0041 mean train/dev loss: 114.1464 / 111.4182
[2017-12-15 17:33:41] Epoch 0042 mean train/dev loss: 114.0218 / 108.3061
[2017-12-15 17:33:48] Epoch 0043 mean train/dev loss: 114.1239 / 110.9855
[2017-12-15 17:33:55] Epoch 0044 mean train/dev loss: 114.0753 / 109.7851
[2017-12-15 17:34:03] Epoch 0045 mean train/dev loss: 114.0495 / 109.0940
[2017-12-15 17:34:03] Learning rate decayed by 0.5000
[2017-12-15 17:34:10] Epoch 0046 mean train/dev loss: 113.8476 / 110.0194
[2017-12-15 17:34:18] Epoch 0047 mean train/dev loss: 113.7922 / 109.6932
[2017-12-15 17:34:25] Epoch 0048 mean train/dev loss: 113.7982 / 109.9766
[2017-12-15 17:34:32] Epoch 0049 mean train/dev loss: 113.8293 / 109.1763
[2017-12-15 17:34:39] Epoch 0050 mean train/dev loss: 113.8369 / 108.2823
[2017-12-15 17:34:39] Checkpointing model at epoch 50 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:34:40] Model Checkpointing finished.
[2017-12-15 17:34:47] Epoch 0051 mean train/dev loss: 113.8130 / 110.3727
[2017-12-15 17:34:54] Epoch 0052 mean train/dev loss: 113.7994 / 110.1852
[2017-12-15 17:35:02] Epoch 0053 mean train/dev loss: 113.8011 / 110.4065
[2017-12-15 17:35:09] Epoch 0054 mean train/dev loss: 113.7660 / 108.4511
[2017-12-15 17:35:17] Epoch 0055 mean train/dev loss: 113.8254 / 109.0380
[2017-12-15 17:35:24] Epoch 0056 mean train/dev loss: 113.8015 / 110.4261
[2017-12-15 17:35:32] Epoch 0057 mean train/dev loss: 113.7695 / 110.4782
[2017-12-15 17:35:39] Epoch 0058 mean train/dev loss: 113.7698 / 109.9735
[2017-12-15 17:35:47] Epoch 0059 mean train/dev loss: 113.8229 / 110.3550
[2017-12-15 17:35:54] Epoch 0060 mean train/dev loss: 113.7929 / 110.1712
[2017-12-15 17:35:54] Learning rate decayed by 0.5000
[2017-12-15 17:35:54] Checkpointing model at epoch 60 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:35:54] Model Checkpointing finished.
[2017-12-15 17:36:02] Epoch 0061 mean train/dev loss: 113.5938 / 108.9304
[2017-12-15 17:36:09] Epoch 0062 mean train/dev loss: 113.6352 / 110.1923
[2017-12-15 17:36:16] Epoch 0063 mean train/dev loss: 113.6212 / 109.5676
[2017-12-15 17:36:24] Epoch 0064 mean train/dev loss: 113.6398 / 109.3678
[2017-12-15 17:36:32] Epoch 0065 mean train/dev loss: 113.6405 / 109.7741
[2017-12-15 17:36:39] Epoch 0066 mean train/dev loss: 113.6624 / 110.0985
[2017-12-15 17:36:46] Epoch 0067 mean train/dev loss: 113.6244 / 109.4333
[2017-12-15 17:36:53] Epoch 0068 mean train/dev loss: 113.6305 / 109.1878
[2017-12-15 17:37:00] Epoch 0069 mean train/dev loss: 113.6127 / 110.4698
[2017-12-15 17:37:08] Epoch 0070 mean train/dev loss: 113.6504 / 110.2656
[2017-12-15 17:37:08] Checkpointing model at epoch 70 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:37:08] Model Checkpointing finished.
[2017-12-15 17:37:16] Epoch 0071 mean train/dev loss: 113.6356 / 109.6552
[2017-12-15 17:37:23] Epoch 0072 mean train/dev loss: 113.5913 / 109.6413
[2017-12-15 17:37:30] Epoch 0073 mean train/dev loss: 113.6167 / 110.0552
[2017-12-15 17:37:38] Epoch 0074 mean train/dev loss: 113.5992 / 109.5288
[2017-12-15 17:37:45] Epoch 0075 mean train/dev loss: 113.6225 / 109.8277
[2017-12-15 17:37:45] Learning rate decayed by 0.5000
[2017-12-15 17:37:53] Epoch 0076 mean train/dev loss: 113.5411 / 109.5365
[2017-12-15 17:38:00] Epoch 0077 mean train/dev loss: 113.5341 / 109.6911
[2017-12-15 17:38:07] Epoch 0078 mean train/dev loss: 113.5356 / 109.7606
[2017-12-15 17:38:14] Epoch 0079 mean train/dev loss: 113.5357 / 109.5352
[2017-12-15 17:38:22] Epoch 0080 mean train/dev loss: 113.5422 / 109.0564
[2017-12-15 17:38:22] Checkpointing model at epoch 80 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:38:22] Model Checkpointing finished.
[2017-12-15 17:38:30] Epoch 0081 mean train/dev loss: 113.5445 / 109.5862
[2017-12-15 17:38:37] Epoch 0082 mean train/dev loss: 113.5095 / 110.0547
[2017-12-15 17:38:44] Epoch 0083 mean train/dev loss: 113.5353 / 109.5702
[2017-12-15 17:38:52] Epoch 0084 mean train/dev loss: 113.5434 / 109.3252
[2017-12-15 17:38:59] Epoch 0085 mean train/dev loss: 113.5188 / 109.9960
[2017-12-15 17:39:05] Epoch 0086 mean train/dev loss: 113.5341 / 109.5537
[2017-12-15 17:39:11] Epoch 0087 mean train/dev loss: 113.5646 / 109.1917
[2017-12-15 17:39:16] Epoch 0088 mean train/dev loss: 113.5218 / 109.5892
[2017-12-15 17:39:21] Epoch 0089 mean train/dev loss: 113.5284 / 109.3795
[2017-12-15 17:39:25] Epoch 0090 mean train/dev loss: 113.5383 / 109.5424
[2017-12-15 17:39:25] Learning rate decayed by 0.5000
[2017-12-15 17:39:25] Checkpointing model at epoch 90 for ffn.hl_50.lr_0.01.wd_1.0
[2017-12-15 17:39:26] Model Checkpointing finished.
[2017-12-15 17:39:31] Epoch 0091 mean train/dev loss: 113.4662 / 109.1319
[2017-12-15 17:39:31] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:39:31] 
                       *** Training finished *** 
[2017-12-15 17:39:32] Dev MSE: 109.1319
[2017-12-15 17:39:36] Training MSE: 113.5153
[2017-12-15 17:39:37] Experiment ffn.hl_50.lr_0.01.wd_1.0 logging ended.
