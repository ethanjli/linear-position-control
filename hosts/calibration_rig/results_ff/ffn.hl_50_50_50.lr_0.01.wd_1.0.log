[2017-12-15 17:51:34] Experiment ffn.hl_50_50_50.lr_0.01.wd_1.0 logging started.
[2017-12-15 17:51:34] 
                       *** Starting Experiment ffn.hl_50_50_50.lr_0.01.wd_1.0 ***
                      
[2017-12-15 17:51:34] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 17:51:34] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 50)
                        (relu3): ReLU ()
                        (linear4): Linear (50 -> 1)
                      )
[2017-12-15 17:51:34]  *** Training on GPU ***
[2017-12-15 17:51:41] Epoch 0001 mean train/dev loss: 12959.1133 / 292.1123
[2017-12-15 17:51:50] Epoch 0002 mean train/dev loss: 136.4089 / 197.4581
[2017-12-15 17:51:59] Epoch 0003 mean train/dev loss: 121.8617 / 188.6393
[2017-12-15 17:52:07] Epoch 0004 mean train/dev loss: 117.1247 / 139.1615
[2017-12-15 17:52:16] Epoch 0005 mean train/dev loss: 114.8276 / 153.1337
[2017-12-15 17:52:24] Epoch 0006 mean train/dev loss: 114.4246 / 133.7984
[2017-12-15 17:52:32] Epoch 0007 mean train/dev loss: 113.4288 / 123.7214
[2017-12-15 17:52:41] Epoch 0008 mean train/dev loss: 111.4275 / 122.9114
[2017-12-15 17:52:49] Epoch 0009 mean train/dev loss: 111.3529 / 141.4037
[2017-12-15 17:52:58] Epoch 0010 mean train/dev loss: 108.5773 / 143.6307
[2017-12-15 17:52:58] Checkpointing model at epoch 10 for ffn.hl_50_50_50.lr_0.01.wd_1.0
[2017-12-15 17:52:58] Model Checkpointing finished.
[2017-12-15 17:53:06] Epoch 0011 mean train/dev loss: 110.7865 / 138.0226
[2017-12-15 17:53:15] Epoch 0012 mean train/dev loss: 105.6152 / 155.8300
[2017-12-15 17:53:23] Epoch 0013 mean train/dev loss: 104.6588 / 137.1295
[2017-12-15 17:53:32] Epoch 0014 mean train/dev loss: 104.7907 / 118.4407
[2017-12-15 17:53:41] Epoch 0015 mean train/dev loss: 100.8233 / 129.3209
[2017-12-15 17:53:41] Learning rate decayed by 0.5000
[2017-12-15 17:53:49] Epoch 0016 mean train/dev loss: 94.0676 / 118.3684
[2017-12-15 17:53:58] Epoch 0017 mean train/dev loss: 93.9535 / 110.6651
[2017-12-15 17:54:06] Epoch 0018 mean train/dev loss: 93.9845 / 117.8888
[2017-12-15 17:54:15] Epoch 0019 mean train/dev loss: 92.7199 / 133.1698
[2017-12-15 17:54:23] Epoch 0020 mean train/dev loss: 92.5383 / 109.8852
[2017-12-15 17:54:23] Checkpointing model at epoch 20 for ffn.hl_50_50_50.lr_0.01.wd_1.0
[2017-12-15 17:54:24] Model Checkpointing finished.
[2017-12-15 17:54:32] Epoch 0021 mean train/dev loss: 90.9650 / 108.3546
[2017-12-15 17:54:40] Epoch 0022 mean train/dev loss: 90.2886 / 111.9556
[2017-12-15 17:54:49] Epoch 0023 mean train/dev loss: 90.0242 / 121.2115
[2017-12-15 17:54:57] Epoch 0024 mean train/dev loss: 88.0614 / 120.0768
[2017-12-15 17:55:06] Epoch 0025 mean train/dev loss: 87.9409 / 128.2039
[2017-12-15 17:55:15] Epoch 0026 mean train/dev loss: 87.1019 / 126.8871
[2017-12-15 17:55:23] Epoch 0027 mean train/dev loss: 86.5840 / 119.2802
[2017-12-15 17:55:32] Epoch 0028 mean train/dev loss: 86.6392 / 124.4609
[2017-12-15 17:55:40] Epoch 0029 mean train/dev loss: 85.8652 / 122.0711
[2017-12-15 17:55:49] Epoch 0030 mean train/dev loss: 85.0054 / 139.5186
[2017-12-15 17:55:49] Learning rate decayed by 0.5000
[2017-12-15 17:55:49] Checkpointing model at epoch 30 for ffn.hl_50_50_50.lr_0.01.wd_1.0
[2017-12-15 17:55:49] Model Checkpointing finished.
[2017-12-15 17:55:57] Epoch 0031 mean train/dev loss: 82.3776 / 124.4215
[2017-12-15 17:56:06] Epoch 0032 mean train/dev loss: 82.2337 / 121.4265
[2017-12-15 17:56:15] Epoch 0033 mean train/dev loss: 82.6623 / 121.8259
[2017-12-15 17:56:23] Epoch 0034 mean train/dev loss: 82.0990 / 119.9075
[2017-12-15 17:56:32] Epoch 0035 mean train/dev loss: 82.1497 / 137.2821
[2017-12-15 17:56:40] Epoch 0036 mean train/dev loss: 81.9444 / 135.8811
[2017-12-15 17:56:49] Epoch 0037 mean train/dev loss: 81.7478 / 129.8940
[2017-12-15 17:56:58] Epoch 0038 mean train/dev loss: 82.1068 / 121.0945
[2017-12-15 17:57:06] Epoch 0039 mean train/dev loss: 81.8873 / 128.5169
[2017-12-15 17:57:14] Epoch 0040 mean train/dev loss: 81.3950 / 123.1866
[2017-12-15 17:57:14] Checkpointing model at epoch 40 for ffn.hl_50_50_50.lr_0.01.wd_1.0
[2017-12-15 17:57:15] Model Checkpointing finished.
[2017-12-15 17:57:23] Epoch 0041 mean train/dev loss: 81.4664 / 132.2825
[2017-12-15 17:57:31] Epoch 0042 mean train/dev loss: 81.7245 / 124.6806
[2017-12-15 17:57:40] Epoch 0043 mean train/dev loss: 81.3290 / 121.6720
[2017-12-15 17:57:49] Epoch 0044 mean train/dev loss: 81.2604 / 119.0104
[2017-12-15 17:57:57] Epoch 0045 mean train/dev loss: 81.0277 / 129.5523
[2017-12-15 17:57:57] Learning rate decayed by 0.5000
[2017-12-15 17:58:06] Epoch 0046 mean train/dev loss: 79.6558 / 122.3050
[2017-12-15 17:58:15] Epoch 0047 mean train/dev loss: 79.5666 / 124.2750
[2017-12-15 17:58:24] Epoch 0048 mean train/dev loss: 79.6452 / 119.7437
[2017-12-15 17:58:32] Epoch 0049 mean train/dev loss: 79.6427 / 117.8446
[2017-12-15 17:58:40] Epoch 0050 mean train/dev loss: 79.5716 / 122.3772
[2017-12-15 17:58:40] Checkpointing model at epoch 50 for ffn.hl_50_50_50.lr_0.01.wd_1.0
[2017-12-15 17:58:41] Model Checkpointing finished.
[2017-12-15 17:58:49] Epoch 0051 mean train/dev loss: 79.6051 / 124.4180
[2017-12-15 17:58:58] Epoch 0052 mean train/dev loss: 79.5349 / 123.7564
[2017-12-15 17:59:07] Epoch 0053 mean train/dev loss: 79.5984 / 114.6892
[2017-12-15 17:59:15] Epoch 0054 mean train/dev loss: 79.3298 / 113.0025
[2017-12-15 17:59:24] Epoch 0055 mean train/dev loss: 79.3080 / 116.8692
[2017-12-15 17:59:32] Epoch 0056 mean train/dev loss: 79.3200 / 118.7622
[2017-12-15 17:59:39] Epoch 0057 mean train/dev loss: 79.0653 / 121.0122
[2017-12-15 17:59:46] Epoch 0058 mean train/dev loss: 79.1541 / 118.7556
[2017-12-15 17:59:54] Epoch 0059 mean train/dev loss: 78.9753 / 121.0282
[2017-12-15 18:00:00] Epoch 0060 mean train/dev loss: 79.1361 / 118.3345
[2017-12-15 18:00:00] Learning rate decayed by 0.5000
[2017-12-15 18:00:00] Checkpointing model at epoch 60 for ffn.hl_50_50_50.lr_0.01.wd_1.0
[2017-12-15 18:00:01] Model Checkpointing finished.
[2017-12-15 18:00:08] Epoch 0061 mean train/dev loss: 78.3536 / 123.7120
[2017-12-15 18:00:15] Epoch 0062 mean train/dev loss: 78.3320 / 118.0334
[2017-12-15 18:00:15] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:00:15] 
                       *** Training finished *** 
[2017-12-15 18:00:17] Dev MSE: 118.0334
[2017-12-15 18:00:22] Training MSE: 78.0060
[2017-12-15 18:00:24] Experiment ffn.hl_50_50_50.lr_0.01.wd_1.0 logging ended.
