[2017-12-15 15:09:36] Experiment ffn.hl_50.lr_0.1.wd_10 logging started.
[2017-12-15 15:09:36] 
                       *** Starting Experiment ffn.hl_50.lr_0.1.wd_10 ***
                      
[2017-12-15 15:09:36] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 15:09:36] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 1)
                      )
[2017-12-15 15:09:36]  *** Training on GPU ***
[2017-12-15 15:09:43] Epoch 0001 mean train/dev loss: 10176.0447 / 319.9309
[2017-12-15 15:09:49] Epoch 0002 mean train/dev loss: 266.0274 / 231.8170
[2017-12-15 15:09:56] Epoch 0003 mean train/dev loss: 254.1635 / 229.0464
[2017-12-15 15:10:02] Epoch 0004 mean train/dev loss: 242.3854 / 270.1717
[2017-12-15 15:10:08] Epoch 0005 mean train/dev loss: 245.4099 / 246.3048
[2017-12-15 15:10:15] Epoch 0006 mean train/dev loss: 244.7693 / 240.1948
[2017-12-15 15:10:21] Epoch 0007 mean train/dev loss: 244.3739 / 225.1881
[2017-12-15 15:10:27] Epoch 0008 mean train/dev loss: 243.2820 / 248.8851
[2017-12-15 15:10:33] Epoch 0009 mean train/dev loss: 253.1578 / 258.0597
[2017-12-15 15:10:40] Epoch 0010 mean train/dev loss: 241.7438 / 246.6470
[2017-12-15 15:10:40] Checkpointing model at epoch 10 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:10:40] Model Checkpointing finished.
[2017-12-15 15:10:47] Epoch 0011 mean train/dev loss: 254.8539 / 260.2953
[2017-12-15 15:10:53] Epoch 0012 mean train/dev loss: 247.6590 / 269.5239
[2017-12-15 15:10:59] Epoch 0013 mean train/dev loss: 241.1459 / 260.4810
[2017-12-15 15:11:05] Epoch 0014 mean train/dev loss: 243.1479 / 233.5935
[2017-12-15 15:11:12] Epoch 0015 mean train/dev loss: 240.0827 / 274.5651
[2017-12-15 15:11:12] Learning rate decayed by 0.5000
[2017-12-15 15:11:18] Epoch 0016 mean train/dev loss: 228.5809 / 210.8524
[2017-12-15 15:11:24] Epoch 0017 mean train/dev loss: 230.3101 / 213.1033
[2017-12-15 15:11:30] Epoch 0018 mean train/dev loss: 230.0869 / 213.9136
[2017-12-15 15:11:36] Epoch 0019 mean train/dev loss: 230.4556 / 218.7735
[2017-12-15 15:11:43] Epoch 0020 mean train/dev loss: 230.5372 / 217.2030
[2017-12-15 15:11:43] Checkpointing model at epoch 20 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:11:43] Model Checkpointing finished.
[2017-12-15 15:11:49] Epoch 0021 mean train/dev loss: 229.8671 / 212.0056
[2017-12-15 15:11:56] Epoch 0022 mean train/dev loss: 230.3195 / 232.8033
[2017-12-15 15:12:02] Epoch 0023 mean train/dev loss: 228.9620 / 201.2560
[2017-12-15 15:12:09] Epoch 0024 mean train/dev loss: 228.9620 / 232.9755
[2017-12-15 15:12:15] Epoch 0025 mean train/dev loss: 230.1957 / 214.3204
[2017-12-15 15:12:21] Epoch 0026 mean train/dev loss: 228.8154 / 214.5972
[2017-12-15 15:12:27] Epoch 0027 mean train/dev loss: 228.9037 / 254.3105
[2017-12-15 15:12:33] Epoch 0028 mean train/dev loss: 229.0852 / 205.9852
[2017-12-15 15:12:40] Epoch 0029 mean train/dev loss: 229.4558 / 200.5292
[2017-12-15 15:12:47] Epoch 0030 mean train/dev loss: 227.9067 / 226.6675
[2017-12-15 15:12:47] Learning rate decayed by 0.5000
[2017-12-15 15:12:47] Checkpointing model at epoch 30 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:12:47] Model Checkpointing finished.
[2017-12-15 15:12:54] Epoch 0031 mean train/dev loss: 225.0755 / 220.6153
[2017-12-15 15:12:59] Epoch 0032 mean train/dev loss: 225.3742 / 210.9117
[2017-12-15 15:13:06] Epoch 0033 mean train/dev loss: 225.5597 / 219.5093
[2017-12-15 15:13:12] Epoch 0034 mean train/dev loss: 225.7773 / 205.0957
[2017-12-15 15:13:18] Epoch 0035 mean train/dev loss: 225.4366 / 219.9131
[2017-12-15 15:13:24] Epoch 0036 mean train/dev loss: 225.7799 / 202.0426
[2017-12-15 15:13:30] Epoch 0037 mean train/dev loss: 225.9746 / 212.2129
[2017-12-15 15:13:36] Epoch 0038 mean train/dev loss: 225.8141 / 218.3110
[2017-12-15 15:13:42] Epoch 0039 mean train/dev loss: 225.6895 / 219.7935
[2017-12-15 15:13:49] Epoch 0040 mean train/dev loss: 225.3498 / 222.6600
[2017-12-15 15:13:49] Checkpointing model at epoch 40 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:13:49] Model Checkpointing finished.
[2017-12-15 15:13:55] Epoch 0041 mean train/dev loss: 225.5995 / 197.7240
[2017-12-15 15:14:01] Epoch 0042 mean train/dev loss: 226.2609 / 224.0469
[2017-12-15 15:14:07] Epoch 0043 mean train/dev loss: 225.6797 / 229.7543
[2017-12-15 15:14:14] Epoch 0044 mean train/dev loss: 225.7737 / 226.4300
[2017-12-15 15:14:20] Epoch 0045 mean train/dev loss: 225.3827 / 226.3545
[2017-12-15 15:14:20] Learning rate decayed by 0.5000
[2017-12-15 15:14:27] Epoch 0046 mean train/dev loss: 223.7881 / 210.2380
[2017-12-15 15:14:33] Epoch 0047 mean train/dev loss: 223.7904 / 206.5793
[2017-12-15 15:14:40] Epoch 0048 mean train/dev loss: 224.2880 / 215.5292
[2017-12-15 15:14:47] Epoch 0049 mean train/dev loss: 223.9304 / 219.5948
[2017-12-15 15:14:53] Epoch 0050 mean train/dev loss: 224.5440 / 210.6812
[2017-12-15 15:14:53] Checkpointing model at epoch 50 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:14:54] Model Checkpointing finished.
[2017-12-15 15:15:00] Epoch 0051 mean train/dev loss: 223.9471 / 213.5805
[2017-12-15 15:15:06] Epoch 0052 mean train/dev loss: 224.0601 / 215.7149
[2017-12-15 15:15:12] Epoch 0053 mean train/dev loss: 224.2065 / 213.7598
[2017-12-15 15:15:19] Epoch 0054 mean train/dev loss: 223.8978 / 217.3266
[2017-12-15 15:15:25] Epoch 0055 mean train/dev loss: 224.0278 / 216.8964
[2017-12-15 15:15:32] Epoch 0056 mean train/dev loss: 224.0337 / 219.7834
[2017-12-15 15:15:37] Epoch 0057 mean train/dev loss: 224.1123 / 207.9036
[2017-12-15 15:15:44] Epoch 0058 mean train/dev loss: 224.2679 / 200.2496
[2017-12-15 15:15:50] Epoch 0059 mean train/dev loss: 224.1756 / 215.6268
[2017-12-15 15:15:56] Epoch 0060 mean train/dev loss: 223.8270 / 213.4448
[2017-12-15 15:15:56] Learning rate decayed by 0.5000
[2017-12-15 15:15:56] Checkpointing model at epoch 60 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:15:57] Model Checkpointing finished.
[2017-12-15 15:16:03] Epoch 0061 mean train/dev loss: 223.3198 / 211.6371
[2017-12-15 15:16:10] Epoch 0062 mean train/dev loss: 222.9970 / 210.7317
[2017-12-15 15:16:16] Epoch 0063 mean train/dev loss: 223.3115 / 205.4639
[2017-12-15 15:16:22] Epoch 0064 mean train/dev loss: 223.0703 / 204.2673
[2017-12-15 15:16:29] Epoch 0065 mean train/dev loss: 223.0846 / 206.3974
[2017-12-15 15:16:35] Epoch 0066 mean train/dev loss: 223.4007 / 203.7634
[2017-12-15 15:16:42] Epoch 0067 mean train/dev loss: 222.9828 / 212.3623
[2017-12-15 15:16:48] Epoch 0068 mean train/dev loss: 223.2287 / 214.3260
[2017-12-15 15:16:54] Epoch 0069 mean train/dev loss: 223.2509 / 219.2917
[2017-12-15 15:17:00] Epoch 0070 mean train/dev loss: 223.3049 / 206.9044
[2017-12-15 15:17:00] Checkpointing model at epoch 70 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:17:01] Model Checkpointing finished.
[2017-12-15 15:17:07] Epoch 0071 mean train/dev loss: 222.9181 / 215.0281
[2017-12-15 15:17:12] Epoch 0072 mean train/dev loss: 223.2932 / 215.3789
[2017-12-15 15:17:19] Epoch 0073 mean train/dev loss: 223.1803 / 206.7211
[2017-12-15 15:17:25] Epoch 0074 mean train/dev loss: 223.2188 / 197.4727
[2017-12-15 15:17:30] Epoch 0075 mean train/dev loss: 223.3764 / 204.0070
[2017-12-15 15:17:30] Learning rate decayed by 0.5000
[2017-12-15 15:17:35] Epoch 0076 mean train/dev loss: 222.6852 / 208.2610
[2017-12-15 15:17:40] Epoch 0077 mean train/dev loss: 222.7130 / 211.2482
[2017-12-15 15:17:45] Epoch 0078 mean train/dev loss: 222.6086 / 204.0745
[2017-12-15 15:17:50] Epoch 0079 mean train/dev loss: 222.6879 / 209.2445
[2017-12-15 15:17:55] Epoch 0080 mean train/dev loss: 222.7554 / 211.1399
[2017-12-15 15:17:55] Checkpointing model at epoch 80 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:17:55] Model Checkpointing finished.
[2017-12-15 15:18:00] Epoch 0081 mean train/dev loss: 222.5935 / 207.5889
[2017-12-15 15:18:04] Epoch 0082 mean train/dev loss: 222.6683 / 214.7796
[2017-12-15 15:18:09] Epoch 0083 mean train/dev loss: 222.6046 / 214.7042
[2017-12-15 15:18:14] Epoch 0084 mean train/dev loss: 222.7233 / 215.0702
[2017-12-15 15:18:19] Epoch 0085 mean train/dev loss: 222.7495 / 208.3915
[2017-12-15 15:18:24] Epoch 0086 mean train/dev loss: 222.6824 / 206.2150
[2017-12-15 15:18:28] Epoch 0087 mean train/dev loss: 222.5716 / 212.4689
[2017-12-15 15:18:33] Epoch 0088 mean train/dev loss: 222.8062 / 205.9918
[2017-12-15 15:18:38] Epoch 0089 mean train/dev loss: 222.5713 / 205.8155
[2017-12-15 15:18:43] Epoch 0090 mean train/dev loss: 222.6978 / 211.3611
[2017-12-15 15:18:43] Learning rate decayed by 0.5000
[2017-12-15 15:18:43] Checkpointing model at epoch 90 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:18:43] Model Checkpointing finished.
[2017-12-15 15:18:48] Epoch 0091 mean train/dev loss: 222.4083 / 211.5495
[2017-12-15 15:18:53] Epoch 0092 mean train/dev loss: 222.3903 / 209.5384
[2017-12-15 15:18:57] Epoch 0093 mean train/dev loss: 222.3644 / 207.8546
[2017-12-15 15:19:02] Epoch 0094 mean train/dev loss: 222.4604 / 209.2828
[2017-12-15 15:19:07] Epoch 0095 mean train/dev loss: 222.3396 / 208.6425
[2017-12-15 15:19:11] Epoch 0096 mean train/dev loss: 222.3551 / 212.5706
[2017-12-15 15:19:16] Epoch 0097 mean train/dev loss: 222.4313 / 209.1257
[2017-12-15 15:19:21] Epoch 0098 mean train/dev loss: 222.3413 / 211.2166
[2017-12-15 15:19:25] Epoch 0099 mean train/dev loss: 222.4465 / 210.8029
[2017-12-15 15:19:30] Epoch 0100 mean train/dev loss: 222.3163 / 210.0806
[2017-12-15 15:19:30] Checkpointing model at epoch 100 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:19:30] Model Checkpointing finished.
[2017-12-15 15:19:35] Epoch 0101 mean train/dev loss: 222.4334 / 211.8944
[2017-12-15 15:19:40] Epoch 0102 mean train/dev loss: 222.4650 / 207.2248
[2017-12-15 15:19:45] Epoch 0103 mean train/dev loss: 222.3381 / 213.2951
[2017-12-15 15:19:50] Epoch 0104 mean train/dev loss: 222.3791 / 214.8287
[2017-12-15 15:19:54] Epoch 0105 mean train/dev loss: 222.4534 / 211.0785
[2017-12-15 15:19:54] Learning rate decayed by 0.5000
[2017-12-15 15:19:59] Epoch 0106 mean train/dev loss: 222.1858 / 210.5812
[2017-12-15 15:20:04] Epoch 0107 mean train/dev loss: 222.3857 / 210.1597
[2017-12-15 15:20:08] Epoch 0108 mean train/dev loss: 222.2049 / 209.7300
[2017-12-15 15:20:13] Epoch 0109 mean train/dev loss: 222.2866 / 210.0169
[2017-12-15 15:20:18] Epoch 0110 mean train/dev loss: 222.1346 / 212.0810
[2017-12-15 15:20:18] Checkpointing model at epoch 110 for ffn.hl_50.lr_0.1.wd_10
[2017-12-15 15:20:18] Model Checkpointing finished.
[2017-12-15 15:20:23] Epoch 0111 mean train/dev loss: 222.1944 / 211.4957
[2017-12-15 15:20:27] Epoch 0112 mean train/dev loss: 222.4040 / 209.6935
[2017-12-15 15:20:32] Epoch 0113 mean train/dev loss: 222.2773 / 209.7760
[2017-12-15 15:20:37] Epoch 0114 mean train/dev loss: 222.1069 / 210.1497
[2017-12-15 15:20:42] Epoch 0115 mean train/dev loss: 222.2400 / 211.3967
[2017-12-15 15:20:42] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:20:42] 
                       *** Training finished *** 
[2017-12-15 15:20:42] Dev MSE: 211.3967
[2017-12-15 15:20:46] Training MSE: 223.3927
[2017-12-15 15:20:47] Experiment ffn.hl_50.lr_0.1.wd_10 logging ended.
