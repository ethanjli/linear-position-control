[2017-12-15 16:30:20] Experiment ffn.hl_linear.lr_0.01.wd_0.001 logging started.
[2017-12-15 16:30:20] 
                       *** Starting Experiment ffn.hl_linear.lr_0.01.wd_0.001 ***
                      
[2017-12-15 16:30:20] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] []  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 16:30:20] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 1)
                      )
[2017-12-15 16:30:20]  *** Training on GPU ***
[2017-12-15 16:30:26] Epoch 0001 mean train/dev loss: 329445.4296 / 326478.0000
[2017-12-15 16:30:31] Epoch 0002 mean train/dev loss: 318563.1997 / 315490.5000
[2017-12-15 16:30:37] Epoch 0003 mean train/dev loss: 308269.8407 / 305022.5312
[2017-12-15 16:30:44] Epoch 0004 mean train/dev loss: 298412.8969 / 295017.5625
[2017-12-15 16:30:52] Epoch 0005 mean train/dev loss: 288999.0951 / 285444.6250
[2017-12-15 16:30:58] Epoch 0006 mean train/dev loss: 280000.7950 / 276274.9062
[2017-12-15 16:31:05] Epoch 0007 mean train/dev loss: 271369.8193 / 267483.6562
[2017-12-15 16:31:11] Epoch 0008 mean train/dev loss: 263052.0430 / 259060.1719
[2017-12-15 16:31:18] Epoch 0009 mean train/dev loss: 255124.6516 / 250983.3125
[2017-12-15 16:31:24] Epoch 0010 mean train/dev loss: 247504.1226 / 243235.0469
[2017-12-15 16:31:24] Checkpointing model at epoch 10 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:31:24] Model Checkpointing finished.
[2017-12-15 16:31:30] Epoch 0011 mean train/dev loss: 240191.2819 / 235808.2188
[2017-12-15 16:31:36] Epoch 0012 mean train/dev loss: 233154.4948 / 228688.5000
[2017-12-15 16:31:40] Epoch 0013 mean train/dev loss: 226413.2355 / 221849.6562
[2017-12-15 16:31:47] Epoch 0014 mean train/dev loss: 219909.5045 / 215270.3281
[2017-12-15 16:31:54] Epoch 0015 mean train/dev loss: 213608.0762 / 208920.4062
[2017-12-15 16:31:54] Learning rate decayed by 0.5000
[2017-12-15 16:32:01] Epoch 0016 mean train/dev loss: 209033.1770 / 205813.5938
[2017-12-15 16:32:08] Epoch 0017 mean train/dev loss: 205988.1784 / 202734.2188
[2017-12-15 16:32:15] Epoch 0018 mean train/dev loss: 202972.9547 / 199689.2969
[2017-12-15 16:32:22] Epoch 0019 mean train/dev loss: 199985.0963 / 196684.8750
[2017-12-15 16:32:30] Epoch 0020 mean train/dev loss: 197064.2359 / 193718.9219
[2017-12-15 16:32:30] Checkpointing model at epoch 20 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:32:30] Model Checkpointing finished.
[2017-12-15 16:32:37] Epoch 0021 mean train/dev loss: 194120.8163 / 190786.2031
[2017-12-15 16:32:44] Epoch 0022 mean train/dev loss: 191232.3207 / 187888.5156
[2017-12-15 16:32:51] Epoch 0023 mean train/dev loss: 188381.3512 / 185024.5000
[2017-12-15 16:32:58] Epoch 0024 mean train/dev loss: 185544.0797 / 182189.7969
[2017-12-15 16:33:05] Epoch 0025 mean train/dev loss: 182724.5275 / 179390.3438
[2017-12-15 16:33:11] Epoch 0026 mean train/dev loss: 179947.9043 / 176624.1562
[2017-12-15 16:33:17] Epoch 0027 mean train/dev loss: 177174.2117 / 173884.2656
[2017-12-15 16:33:24] Epoch 0028 mean train/dev loss: 174465.2934 / 171174.9688
[2017-12-15 16:33:30] Epoch 0029 mean train/dev loss: 171777.5229 / 168492.4062
[2017-12-15 16:33:37] Epoch 0030 mean train/dev loss: 169082.8224 / 165837.8750
[2017-12-15 16:33:37] Learning rate decayed by 0.5000
[2017-12-15 16:33:37] Checkpointing model at epoch 30 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:33:37] Model Checkpointing finished.
[2017-12-15 16:33:44] Epoch 0031 mean train/dev loss: 167086.7158 / 164518.2031
[2017-12-15 16:33:50] Epoch 0032 mean train/dev loss: 165767.6035 / 163203.2344
[2017-12-15 16:33:57] Epoch 0033 mean train/dev loss: 164438.0377 / 161891.5938
[2017-12-15 16:34:04] Epoch 0034 mean train/dev loss: 163123.5516 / 160586.2656
[2017-12-15 16:34:11] Epoch 0035 mean train/dev loss: 161809.7060 / 159286.8594
[2017-12-15 16:34:16] Epoch 0036 mean train/dev loss: 160516.9613 / 157995.1875
[2017-12-15 16:34:22] Epoch 0037 mean train/dev loss: 159218.4786 / 156710.1250
[2017-12-15 16:34:28] Epoch 0038 mean train/dev loss: 157942.2192 / 155431.3594
[2017-12-15 16:34:34] Epoch 0039 mean train/dev loss: 156632.6135 / 154157.2188
[2017-12-15 16:34:41] Epoch 0040 mean train/dev loss: 155366.3379 / 152890.8125
[2017-12-15 16:34:41] Checkpointing model at epoch 40 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:34:42] Model Checkpointing finished.
[2017-12-15 16:34:47] Epoch 0041 mean train/dev loss: 154087.8467 / 151630.8125
[2017-12-15 16:34:54] Epoch 0042 mean train/dev loss: 152826.7352 / 150378.3906
[2017-12-15 16:34:59] Epoch 0043 mean train/dev loss: 151567.6757 / 149131.1562
[2017-12-15 16:35:07] Epoch 0044 mean train/dev loss: 150314.6611 / 147890.1719
[2017-12-15 16:35:14] Epoch 0045 mean train/dev loss: 149068.9050 / 146654.1562
[2017-12-15 16:35:14] Learning rate decayed by 0.5000
[2017-12-15 16:35:21] Epoch 0046 mean train/dev loss: 148123.5688 / 146039.0000
[2017-12-15 16:35:28] Epoch 0047 mean train/dev loss: 147509.3897 / 145423.8438
[2017-12-15 16:35:34] Epoch 0048 mean train/dev loss: 146886.8311 / 144809.6719
[2017-12-15 16:35:39] Epoch 0049 mean train/dev loss: 146279.5320 / 144196.5312
[2017-12-15 16:35:44] Epoch 0050 mean train/dev loss: 145654.0142 / 143584.1719
[2017-12-15 16:35:44] Checkpointing model at epoch 50 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:35:45] Model Checkpointing finished.
[2017-12-15 16:35:50] Epoch 0051 mean train/dev loss: 145043.3145 / 142974.7188
[2017-12-15 16:35:56] Epoch 0052 mean train/dev loss: 144417.6284 / 142365.8594
[2017-12-15 16:36:02] Epoch 0053 mean train/dev loss: 143795.4874 / 141758.0625
[2017-12-15 16:36:07] Epoch 0054 mean train/dev loss: 143200.1078 / 141152.3125
[2017-12-15 16:36:13] Epoch 0055 mean train/dev loss: 142588.2414 / 140547.2812
[2017-12-15 16:36:20] Epoch 0056 mean train/dev loss: 141974.6037 / 139943.0000
[2017-12-15 16:36:26] Epoch 0057 mean train/dev loss: 141349.7495 / 139341.2812
[2017-12-15 16:36:33] Epoch 0058 mean train/dev loss: 140766.2203 / 138740.1719
[2017-12-15 16:36:40] Epoch 0059 mean train/dev loss: 140148.5541 / 138140.8594
[2017-12-15 16:36:47] Epoch 0060 mean train/dev loss: 139550.7652 / 137542.8750
[2017-12-15 16:36:47] Learning rate decayed by 0.5000
[2017-12-15 16:36:47] Checkpointing model at epoch 60 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:36:47] Model Checkpointing finished.
[2017-12-15 16:36:54] Epoch 0061 mean train/dev loss: 139092.1930 / 137243.8438
[2017-12-15 16:37:01] Epoch 0062 mean train/dev loss: 138785.5068 / 136944.9219
[2017-12-15 16:37:07] Epoch 0063 mean train/dev loss: 138482.8305 / 136646.3281
[2017-12-15 16:37:13] Epoch 0064 mean train/dev loss: 138197.5550 / 136347.9062
[2017-12-15 16:37:21] Epoch 0065 mean train/dev loss: 137892.9450 / 136050.2031
[2017-12-15 16:37:28] Epoch 0066 mean train/dev loss: 137587.5070 / 135752.6250
[2017-12-15 16:37:35] Epoch 0067 mean train/dev loss: 137281.2634 / 135455.5938
[2017-12-15 16:37:41] Epoch 0068 mean train/dev loss: 136994.3772 / 135158.3594
[2017-12-15 16:37:48] Epoch 0069 mean train/dev loss: 136688.2480 / 134861.4688
[2017-12-15 16:37:55] Epoch 0070 mean train/dev loss: 136374.8816 / 134565.1719
[2017-12-15 16:37:55] Checkpointing model at epoch 70 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:37:56] Model Checkpointing finished.
[2017-12-15 16:38:03] Epoch 0071 mean train/dev loss: 136089.0568 / 134269.2969
[2017-12-15 16:38:10] Epoch 0072 mean train/dev loss: 135783.3211 / 133973.9219
[2017-12-15 16:38:18] Epoch 0073 mean train/dev loss: 135490.2073 / 133679.0156
[2017-12-15 16:38:25] Epoch 0074 mean train/dev loss: 135198.0831 / 133384.3281
[2017-12-15 16:38:33] Epoch 0075 mean train/dev loss: 134878.2523 / 133090.1719
[2017-12-15 16:38:33] Learning rate decayed by 0.5000
[2017-12-15 16:38:40] Epoch 0076 mean train/dev loss: 134672.4400 / 132945.3750
[2017-12-15 16:38:47] Epoch 0077 mean train/dev loss: 134521.9630 / 132800.1562
[2017-12-15 16:38:55] Epoch 0078 mean train/dev loss: 134377.4582 / 132655.3594
[2017-12-15 16:39:02] Epoch 0079 mean train/dev loss: 134233.8299 / 132510.1562
[2017-12-15 16:39:09] Epoch 0080 mean train/dev loss: 134080.7774 / 132365.4531
[2017-12-15 16:39:09] Checkpointing model at epoch 80 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:39:09] Model Checkpointing finished.
[2017-12-15 16:39:15] Epoch 0081 mean train/dev loss: 133937.1731 / 132220.6094
[2017-12-15 16:39:23] Epoch 0082 mean train/dev loss: 133791.0149 / 132075.4531
[2017-12-15 16:39:28] Epoch 0083 mean train/dev loss: 133652.4651 / 131931.0625
[2017-12-15 16:39:34] Epoch 0084 mean train/dev loss: 133489.2781 / 131786.1875
[2017-12-15 16:39:38] Epoch 0085 mean train/dev loss: 133350.0797 / 131641.2188
[2017-12-15 16:39:43] Epoch 0086 mean train/dev loss: 133206.7669 / 131496.4688
[2017-12-15 16:39:48] Epoch 0087 mean train/dev loss: 133064.1244 / 131351.9375
[2017-12-15 16:39:52] Epoch 0088 mean train/dev loss: 132903.9387 / 131207.4219
[2017-12-15 16:39:59] Epoch 0089 mean train/dev loss: 132764.6248 / 131062.8750
[2017-12-15 16:40:03] Epoch 0090 mean train/dev loss: 132617.7047 / 130918.3125
[2017-12-15 16:40:03] Learning rate decayed by 0.5000
[2017-12-15 16:40:03] Checkpointing model at epoch 90 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:40:04] Model Checkpointing finished.
[2017-12-15 16:40:08] Epoch 0091 mean train/dev loss: 132530.2776 / 130846.9688
[2017-12-15 16:40:13] Epoch 0092 mean train/dev loss: 132436.0945 / 130775.5469
[2017-12-15 16:40:18] Epoch 0093 mean train/dev loss: 132373.3751 / 130704.2344
[2017-12-15 16:40:23] Epoch 0094 mean train/dev loss: 132293.5874 / 130632.8672
[2017-12-15 16:40:27] Epoch 0095 mean train/dev loss: 132234.2695 / 130561.5078
[2017-12-15 16:40:32] Epoch 0096 mean train/dev loss: 132146.5813 / 130490.2812
[2017-12-15 16:40:37] Epoch 0097 mean train/dev loss: 132086.3549 / 130418.9375
[2017-12-15 16:40:41] Epoch 0098 mean train/dev loss: 132002.4004 / 130347.6641
[2017-12-15 16:40:46] Epoch 0099 mean train/dev loss: 131942.8113 / 130276.4453
[2017-12-15 16:40:51] Epoch 0100 mean train/dev loss: 131864.4631 / 130205.2656
[2017-12-15 16:40:51] Checkpointing model at epoch 100 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:40:52] Model Checkpointing finished.
[2017-12-15 16:40:59] Epoch 0101 mean train/dev loss: 131795.7444 / 130133.9531
[2017-12-15 16:41:04] Epoch 0102 mean train/dev loss: 131727.4015 / 130062.8125
[2017-12-15 16:41:09] Epoch 0103 mean train/dev loss: 131643.9753 / 129991.6406
[2017-12-15 16:41:14] Epoch 0104 mean train/dev loss: 131581.7368 / 129920.5391
[2017-12-15 16:41:19] Epoch 0105 mean train/dev loss: 131508.5857 / 129849.3672
[2017-12-15 16:41:19] Learning rate decayed by 0.5000
[2017-12-15 16:41:23] Epoch 0106 mean train/dev loss: 131440.3727 / 129813.8125
[2017-12-15 16:41:28] Epoch 0107 mean train/dev loss: 131413.7192 / 129778.2266
[2017-12-15 16:41:33] Epoch 0108 mean train/dev loss: 131385.0321 / 129742.6406
[2017-12-15 16:41:37] Epoch 0109 mean train/dev loss: 131350.9940 / 129707.0859
[2017-12-15 16:41:42] Epoch 0110 mean train/dev loss: 131309.6317 / 129671.5312
[2017-12-15 16:41:42] Checkpointing model at epoch 110 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:41:42] Model Checkpointing finished.
[2017-12-15 16:41:49] Epoch 0111 mean train/dev loss: 131269.2984 / 129636.0078
[2017-12-15 16:41:55] Epoch 0112 mean train/dev loss: 131221.0795 / 129600.5391
[2017-12-15 16:42:00] Epoch 0113 mean train/dev loss: 131195.6414 / 129564.9688
[2017-12-15 16:42:05] Epoch 0114 mean train/dev loss: 131153.6140 / 129529.4609
[2017-12-15 16:42:12] Epoch 0115 mean train/dev loss: 131116.1736 / 129493.9219
[2017-12-15 16:42:18] Epoch 0116 mean train/dev loss: 131085.4616 / 129458.3906
[2017-12-15 16:42:25] Epoch 0117 mean train/dev loss: 131035.2198 / 129422.8516
[2017-12-15 16:42:32] Epoch 0118 mean train/dev loss: 131019.4815 / 129387.3359
[2017-12-15 16:42:39] Epoch 0119 mean train/dev loss: 130982.8955 / 129351.7969
[2017-12-15 16:42:44] Epoch 0120 mean train/dev loss: 130961.4126 / 129316.3047
[2017-12-15 16:42:44] Learning rate decayed by 0.5000
[2017-12-15 16:42:44] Checkpointing model at epoch 120 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:42:45] Model Checkpointing finished.
[2017-12-15 16:42:49] Epoch 0121 mean train/dev loss: 130919.6098 / 129295.5547
[2017-12-15 16:42:54] Epoch 0122 mean train/dev loss: 130901.6465 / 129274.7969
[2017-12-15 16:42:59] Epoch 0123 mean train/dev loss: 130881.4015 / 129254.1016
[2017-12-15 16:43:03] Epoch 0124 mean train/dev loss: 130861.0068 / 129233.3594
[2017-12-15 16:43:08] Epoch 0125 mean train/dev loss: 130827.5492 / 129212.6484
[2017-12-15 16:43:15] Epoch 0126 mean train/dev loss: 130816.8076 / 129191.9219
[2017-12-15 16:43:23] Epoch 0127 mean train/dev loss: 130791.7554 / 129171.1562
[2017-12-15 16:43:29] Epoch 0128 mean train/dev loss: 130770.7509 / 129150.3906
[2017-12-15 16:43:34] Epoch 0129 mean train/dev loss: 130736.0163 / 129129.6797
[2017-12-15 16:43:38] Epoch 0130 mean train/dev loss: 130725.2720 / 129108.8828
[2017-12-15 16:43:38] Checkpointing model at epoch 130 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:43:39] Model Checkpointing finished.
[2017-12-15 16:43:43] Epoch 0131 mean train/dev loss: 130709.9251 / 129088.1562
[2017-12-15 16:43:49] Epoch 0132 mean train/dev loss: 130687.5231 / 129067.3906
[2017-12-15 16:43:56] Epoch 0133 mean train/dev loss: 130670.2656 / 129046.6641
[2017-12-15 16:44:02] Epoch 0134 mean train/dev loss: 130661.7541 / 129025.9375
[2017-12-15 16:44:07] Epoch 0135 mean train/dev loss: 130613.0658 / 129005.2344
[2017-12-15 16:44:07] Learning rate decayed by 0.5000
[2017-12-15 16:44:14] Epoch 0136 mean train/dev loss: 130611.5782 / 128998.0234
[2017-12-15 16:44:21] Epoch 0137 mean train/dev loss: 130604.0274 / 128990.8281
[2017-12-15 16:44:25] Epoch 0138 mean train/dev loss: 130595.8806 / 128983.6016
[2017-12-15 16:44:30] Epoch 0139 mean train/dev loss: 130583.2870 / 128976.3828
[2017-12-15 16:44:36] Epoch 0140 mean train/dev loss: 130574.8074 / 128969.1875
[2017-12-15 16:44:36] Checkpointing model at epoch 140 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:44:36] Model Checkpointing finished.
[2017-12-15 16:44:43] Epoch 0141 mean train/dev loss: 130577.4421 / 128961.9844
[2017-12-15 16:44:48] Epoch 0142 mean train/dev loss: 130559.4849 / 128954.7891
[2017-12-15 16:44:52] Epoch 0143 mean train/dev loss: 130547.7618 / 128947.5625
[2017-12-15 16:44:57] Epoch 0144 mean train/dev loss: 130553.4703 / 128940.3750
[2017-12-15 16:45:04] Epoch 0145 mean train/dev loss: 130555.8088 / 128933.1719
[2017-12-15 16:45:08] Epoch 0146 mean train/dev loss: 130545.7441 / 128925.9609
[2017-12-15 16:45:13] Epoch 0147 mean train/dev loss: 130536.2163 / 128918.7578
[2017-12-15 16:45:19] Epoch 0148 mean train/dev loss: 130524.0841 / 128911.5625
[2017-12-15 16:45:26] Epoch 0149 mean train/dev loss: 130509.6369 / 128904.3594
[2017-12-15 16:45:31] Epoch 0150 mean train/dev loss: 130506.2052 / 128897.1484
[2017-12-15 16:45:31] Learning rate decayed by 0.5000
[2017-12-15 16:45:31] Checkpointing model at epoch 150 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:45:32] Model Checkpointing finished.
[2017-12-15 16:45:36] Epoch 0151 mean train/dev loss: 130500.0948 / 128890.2188
[2017-12-15 16:45:41] Epoch 0152 mean train/dev loss: 130491.3289 / 128883.3203
[2017-12-15 16:45:48] Epoch 0153 mean train/dev loss: 130490.2795 / 128876.4766
[2017-12-15 16:45:56] Epoch 0154 mean train/dev loss: 130480.4431 / 128869.5938
[2017-12-15 16:46:03] Epoch 0155 mean train/dev loss: 130457.7402 / 128862.7031
[2017-12-15 16:46:10] Epoch 0156 mean train/dev loss: 130453.3211 / 128855.8047
[2017-12-15 16:46:15] Epoch 0157 mean train/dev loss: 130472.9830 / 128848.9297
[2017-12-15 16:46:23] Epoch 0158 mean train/dev loss: 130456.7965 / 128842.0469
[2017-12-15 16:46:30] Epoch 0159 mean train/dev loss: 130441.8308 / 128835.2188
[2017-12-15 16:46:36] Epoch 0160 mean train/dev loss: 130440.2137 / 128828.3281
[2017-12-15 16:46:36] Checkpointing model at epoch 160 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:46:36] Model Checkpointing finished.
[2017-12-15 16:46:41] Epoch 0161 mean train/dev loss: 130435.5332 / 128821.4297
[2017-12-15 16:46:48] Epoch 0162 mean train/dev loss: 130427.4382 / 128814.5547
[2017-12-15 16:46:55] Epoch 0163 mean train/dev loss: 130408.6126 / 128807.6719
[2017-12-15 16:47:01] Epoch 0164 mean train/dev loss: 130415.9830 / 128800.7734
[2017-12-15 16:47:09] Epoch 0165 mean train/dev loss: 130414.5169 / 128793.8828
[2017-12-15 16:47:09] Learning rate decayed by 0.5000
[2017-12-15 16:47:17] Epoch 0166 mean train/dev loss: 130394.7494 / 128793.8750
[2017-12-15 16:47:23] Epoch 0167 mean train/dev loss: 130396.7954 / 128793.8750
[2017-12-15 16:47:29] Epoch 0168 mean train/dev loss: 130399.9749 / 128793.8750
[2017-12-15 16:47:36] Epoch 0169 mean train/dev loss: 130399.9960 / 128793.8594
[2017-12-15 16:47:42] Epoch 0170 mean train/dev loss: 130406.7531 / 128793.8594
[2017-12-15 16:47:42] Checkpointing model at epoch 170 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:47:43] Model Checkpointing finished.
[2017-12-15 16:47:50] Epoch 0171 mean train/dev loss: 130395.0210 / 128793.8281
[2017-12-15 16:47:55] Epoch 0172 mean train/dev loss: 130393.5993 / 128793.8438
[2017-12-15 16:48:00] Epoch 0173 mean train/dev loss: 130393.7674 / 128793.8438
[2017-12-15 16:48:07] Epoch 0174 mean train/dev loss: 130405.8857 / 128793.8438
[2017-12-15 16:48:15] Epoch 0175 mean train/dev loss: 130420.6146 / 128793.8438
[2017-12-15 16:48:21] Epoch 0176 mean train/dev loss: 130396.6193 / 128793.8203
[2017-12-15 16:48:27] Epoch 0177 mean train/dev loss: 130409.1969 / 128793.8281
[2017-12-15 16:48:31] Epoch 0178 mean train/dev loss: 130396.0808 / 128793.8281
[2017-12-15 16:48:36] Epoch 0179 mean train/dev loss: 130407.6047 / 128793.8438
[2017-12-15 16:48:41] Epoch 0180 mean train/dev loss: 130395.6514 / 128793.8281
[2017-12-15 16:48:41] Learning rate decayed by 0.5000
[2017-12-15 16:48:41] Checkpointing model at epoch 180 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:48:41] Model Checkpointing finished.
[2017-12-15 16:48:46] Epoch 0181 mean train/dev loss: 130396.2113 / 128793.8281
[2017-12-15 16:48:51] Epoch 0182 mean train/dev loss: 130401.2424 / 128793.8281
[2017-12-15 16:48:56] Epoch 0183 mean train/dev loss: 130397.7959 / 128793.8281
[2017-12-15 16:49:01] Epoch 0184 mean train/dev loss: 130402.5132 / 128793.8281
[2017-12-15 16:49:08] Epoch 0185 mean train/dev loss: 130399.8387 / 128793.8281
[2017-12-15 16:49:16] Epoch 0186 mean train/dev loss: 130400.2221 / 128793.8203
[2017-12-15 16:49:21] Epoch 0187 mean train/dev loss: 130397.4876 / 128793.8203
[2017-12-15 16:49:28] Epoch 0188 mean train/dev loss: 130386.7517 / 128793.8281
[2017-12-15 16:49:35] Epoch 0189 mean train/dev loss: 130406.9299 / 128793.8203
[2017-12-15 16:49:40] Epoch 0190 mean train/dev loss: 130405.6836 / 128793.8203
[2017-12-15 16:49:40] Checkpointing model at epoch 190 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:49:41] Model Checkpointing finished.
[2017-12-15 16:49:48] Epoch 0191 mean train/dev loss: 130400.0378 / 128793.8203
[2017-12-15 16:49:52] Epoch 0192 mean train/dev loss: 130400.0590 / 128793.8203
[2017-12-15 16:49:57] Epoch 0193 mean train/dev loss: 130404.1485 / 128793.8203
[2017-12-15 16:50:04] Epoch 0194 mean train/dev loss: 130396.5687 / 128793.8203
[2017-12-15 16:50:09] Epoch 0195 mean train/dev loss: 130413.2934 / 128793.8281
[2017-12-15 16:50:09] Learning rate decayed by 0.5000
[2017-12-15 16:50:17] Epoch 0196 mean train/dev loss: 130404.9107 / 128793.8203
[2017-12-15 16:50:23] Epoch 0197 mean train/dev loss: 130407.2065 / 128793.8047
[2017-12-15 16:50:29] Epoch 0198 mean train/dev loss: 130417.0648 / 128793.8203
[2017-12-15 16:50:36] Epoch 0199 mean train/dev loss: 130403.7454 / 128793.8047
[2017-12-15 16:50:41] Epoch 0200 mean train/dev loss: 130417.7312 / 128793.8281
[2017-12-15 16:50:41] Checkpointing model at epoch 200 for ffn.hl_linear.lr_0.01.wd_0.001
[2017-12-15 16:50:42] Model Checkpointing finished.
[2017-12-15 16:50:42] 
                       *** Training finished *** 
[2017-12-15 16:50:43] Dev MSE: 128793.8281
[2017-12-15 16:50:47] Training MSE: 130401.3047
[2017-12-15 16:50:48] Experiment ffn.hl_linear.lr_0.01.wd_0.001 logging ended.
