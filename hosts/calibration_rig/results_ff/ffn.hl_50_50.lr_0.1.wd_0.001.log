[2017-12-15 15:20:49] Experiment ffn.hl_50_50.lr_0.1.wd_0.001 logging started.
[2017-12-15 15:20:49] 
                       *** Starting Experiment ffn.hl_50_50.lr_0.1.wd_0.001 ***
                      
[2017-12-15 15:20:49] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 15:20:52] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 1)
                      )
[2017-12-15 15:20:52]  *** Training on GPU ***
[2017-12-15 15:21:00] Epoch 0001 mean train/dev loss: 4370.9361 / 638.3247
[2017-12-15 15:21:08] Epoch 0002 mean train/dev loss: 145.2979 / 144.1127
[2017-12-15 15:21:16] Epoch 0003 mean train/dev loss: 128.3240 / 162.2854
[2017-12-15 15:21:24] Epoch 0004 mean train/dev loss: 132.2234 / 409.7401
[2017-12-15 15:21:32] Epoch 0005 mean train/dev loss: 152.7392 / 363.3931
[2017-12-15 15:21:39] Epoch 0006 mean train/dev loss: 146.5470 / 208.3232
[2017-12-15 15:21:47] Epoch 0007 mean train/dev loss: 140.8952 / 211.2903
[2017-12-15 15:21:55] Epoch 0008 mean train/dev loss: 126.4612 / 180.3712
[2017-12-15 15:22:02] Epoch 0009 mean train/dev loss: 117.6375 / 192.9121
[2017-12-15 15:22:10] Epoch 0010 mean train/dev loss: 124.3115 / 125.6079
[2017-12-15 15:22:10] Checkpointing model at epoch 10 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:22:11] Model Checkpointing finished.
[2017-12-15 15:22:19] Epoch 0011 mean train/dev loss: 95.6155 / 110.0115
[2017-12-15 15:22:27] Epoch 0012 mean train/dev loss: 95.7005 / 159.6004
[2017-12-15 15:22:35] Epoch 0013 mean train/dev loss: 93.5151 / 160.3915
[2017-12-15 15:22:42] Epoch 0014 mean train/dev loss: 92.7432 / 142.1100
[2017-12-15 15:22:50] Epoch 0015 mean train/dev loss: 89.8782 / 135.4285
[2017-12-15 15:22:50] Learning rate decayed by 0.5000
[2017-12-15 15:22:58] Epoch 0016 mean train/dev loss: 72.7619 / 94.1628
[2017-12-15 15:23:06] Epoch 0017 mean train/dev loss: 72.5774 / 115.6144
[2017-12-15 15:23:14] Epoch 0018 mean train/dev loss: 71.9146 / 90.9696
[2017-12-15 15:23:22] Epoch 0019 mean train/dev loss: 69.6924 / 112.2046
[2017-12-15 15:23:30] Epoch 0020 mean train/dev loss: 67.3103 / 94.7971
[2017-12-15 15:23:30] Checkpointing model at epoch 20 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:23:30] Model Checkpointing finished.
[2017-12-15 15:23:38] Epoch 0021 mean train/dev loss: 65.6238 / 102.4418
[2017-12-15 15:23:46] Epoch 0022 mean train/dev loss: 63.4534 / 88.4304
[2017-12-15 15:23:54] Epoch 0023 mean train/dev loss: 62.3488 / 93.3158
[2017-12-15 15:24:01] Epoch 0024 mean train/dev loss: 61.8580 / 116.2021
[2017-12-15 15:24:09] Epoch 0025 mean train/dev loss: 61.3971 / 78.9932
[2017-12-15 15:24:17] Epoch 0026 mean train/dev loss: 60.7813 / 96.1651
[2017-12-15 15:24:26] Epoch 0027 mean train/dev loss: 61.3587 / 105.9722
[2017-12-15 15:24:33] Epoch 0028 mean train/dev loss: 59.1218 / 102.1320
[2017-12-15 15:24:41] Epoch 0029 mean train/dev loss: 59.5861 / 91.2283
[2017-12-15 15:24:49] Epoch 0030 mean train/dev loss: 59.6409 / 104.4341
[2017-12-15 15:24:49] Learning rate decayed by 0.5000
[2017-12-15 15:24:49] Checkpointing model at epoch 30 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:24:49] Model Checkpointing finished.
[2017-12-15 15:24:57] Epoch 0031 mean train/dev loss: 52.3143 / 82.9572
[2017-12-15 15:25:06] Epoch 0032 mean train/dev loss: 52.2497 / 101.4161
[2017-12-15 15:25:13] Epoch 0033 mean train/dev loss: 51.9875 / 79.9299
[2017-12-15 15:25:21] Epoch 0034 mean train/dev loss: 51.9455 / 100.7591
[2017-12-15 15:25:29] Epoch 0035 mean train/dev loss: 51.1385 / 97.1128
[2017-12-15 15:25:37] Epoch 0036 mean train/dev loss: 51.4223 / 89.1338
[2017-12-15 15:25:45] Epoch 0037 mean train/dev loss: 51.2321 / 87.2212
[2017-12-15 15:25:53] Epoch 0038 mean train/dev loss: 50.9238 / 106.0733
[2017-12-15 15:26:01] Epoch 0039 mean train/dev loss: 50.5094 / 87.4428
[2017-12-15 15:26:09] Epoch 0040 mean train/dev loss: 50.1694 / 88.3221
[2017-12-15 15:26:09] Checkpointing model at epoch 40 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:26:09] Model Checkpointing finished.
[2017-12-15 15:26:17] Epoch 0041 mean train/dev loss: 49.2985 / 86.1645
[2017-12-15 15:26:25] Epoch 0042 mean train/dev loss: 48.8685 / 84.6162
[2017-12-15 15:26:33] Epoch 0043 mean train/dev loss: 48.6588 / 84.3828
[2017-12-15 15:26:41] Epoch 0044 mean train/dev loss: 48.0335 / 90.2016
[2017-12-15 15:26:48] Epoch 0045 mean train/dev loss: 47.8242 / 103.9836
[2017-12-15 15:26:48] Learning rate decayed by 0.5000
[2017-12-15 15:26:56] Epoch 0046 mean train/dev loss: 44.4054 / 87.2621
[2017-12-15 15:27:04] Epoch 0047 mean train/dev loss: 44.2976 / 90.8255
[2017-12-15 15:27:12] Epoch 0048 mean train/dev loss: 44.4940 / 87.5544
[2017-12-15 15:27:19] Epoch 0049 mean train/dev loss: 44.0083 / 90.7326
[2017-12-15 15:27:27] Epoch 0050 mean train/dev loss: 43.9703 / 80.1221
[2017-12-15 15:27:27] Checkpointing model at epoch 50 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:27:28] Model Checkpointing finished.
[2017-12-15 15:27:35] Epoch 0051 mean train/dev loss: 43.9512 / 85.9400
[2017-12-15 15:27:43] Epoch 0052 mean train/dev loss: 43.8263 / 88.6671
[2017-12-15 15:27:52] Epoch 0053 mean train/dev loss: 43.6007 / 85.0539
[2017-12-15 15:28:00] Epoch 0054 mean train/dev loss: 43.6820 / 81.8413
[2017-12-15 15:28:08] Epoch 0055 mean train/dev loss: 43.4407 / 77.0690
[2017-12-15 15:28:15] Epoch 0056 mean train/dev loss: 43.4033 / 81.8010
[2017-12-15 15:28:23] Epoch 0057 mean train/dev loss: 43.1058 / 85.5811
[2017-12-15 15:28:30] Epoch 0058 mean train/dev loss: 43.3138 / 76.0975
[2017-12-15 15:28:38] Epoch 0059 mean train/dev loss: 42.9751 / 77.4794
[2017-12-15 15:28:46] Epoch 0060 mean train/dev loss: 43.2801 / 83.7592
[2017-12-15 15:28:46] Learning rate decayed by 0.5000
[2017-12-15 15:28:46] Checkpointing model at epoch 60 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:28:47] Model Checkpointing finished.
[2017-12-15 15:28:54] Epoch 0061 mean train/dev loss: 41.2548 / 83.4508
[2017-12-15 15:29:03] Epoch 0062 mean train/dev loss: 41.2583 / 79.9884
[2017-12-15 15:29:11] Epoch 0063 mean train/dev loss: 41.2576 / 79.7361
[2017-12-15 15:29:18] Epoch 0064 mean train/dev loss: 41.2195 / 83.9193
[2017-12-15 15:29:26] Epoch 0065 mean train/dev loss: 41.1860 / 73.6834
[2017-12-15 15:29:34] Epoch 0066 mean train/dev loss: 41.0010 / 76.8883
[2017-12-15 15:29:42] Epoch 0067 mean train/dev loss: 40.8596 / 83.4046
[2017-12-15 15:29:50] Epoch 0068 mean train/dev loss: 40.5581 / 73.4741
[2017-12-15 15:29:58] Epoch 0069 mean train/dev loss: 40.3549 / 74.5958
[2017-12-15 15:30:06] Epoch 0070 mean train/dev loss: 39.9816 / 83.7510
[2017-12-15 15:30:06] Checkpointing model at epoch 70 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:30:06] Model Checkpointing finished.
[2017-12-15 15:30:14] Epoch 0071 mean train/dev loss: 39.8433 / 80.1731
[2017-12-15 15:30:22] Epoch 0072 mean train/dev loss: 39.7942 / 78.5667
[2017-12-15 15:30:30] Epoch 0073 mean train/dev loss: 39.6659 / 80.8711
[2017-12-15 15:30:38] Epoch 0074 mean train/dev loss: 39.4627 / 74.1339
[2017-12-15 15:30:45] Epoch 0075 mean train/dev loss: 39.4064 / 74.9953
[2017-12-15 15:30:45] Learning rate decayed by 0.5000
[2017-12-15 15:30:54] Epoch 0076 mean train/dev loss: 38.3269 / 73.1425
[2017-12-15 15:31:02] Epoch 0077 mean train/dev loss: 38.3202 / 72.5947
[2017-12-15 15:31:09] Epoch 0078 mean train/dev loss: 38.3041 / 76.2757
[2017-12-15 15:31:17] Epoch 0079 mean train/dev loss: 38.2595 / 74.9159
[2017-12-15 15:31:25] Epoch 0080 mean train/dev loss: 38.1855 / 73.5473
[2017-12-15 15:31:25] Checkpointing model at epoch 80 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:31:26] Model Checkpointing finished.
[2017-12-15 15:31:34] Epoch 0081 mean train/dev loss: 38.2080 / 77.0534
[2017-12-15 15:31:42] Epoch 0082 mean train/dev loss: 38.1321 / 77.6554
[2017-12-15 15:31:50] Epoch 0083 mean train/dev loss: 38.0729 / 73.3427
[2017-12-15 15:31:58] Epoch 0084 mean train/dev loss: 37.9156 / 69.5206
[2017-12-15 15:32:06] Epoch 0085 mean train/dev loss: 37.9610 / 72.6505
[2017-12-15 15:32:14] Epoch 0086 mean train/dev loss: 37.8439 / 73.1491
[2017-12-15 15:32:22] Epoch 0087 mean train/dev loss: 37.8238 / 73.3339
[2017-12-15 15:32:29] Epoch 0088 mean train/dev loss: 37.7729 / 77.6395
[2017-12-15 15:32:37] Epoch 0089 mean train/dev loss: 37.7825 / 74.0480
[2017-12-15 15:32:45] Epoch 0090 mean train/dev loss: 37.6445 / 73.2936
[2017-12-15 15:32:45] Learning rate decayed by 0.5000
[2017-12-15 15:32:45] Checkpointing model at epoch 90 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:32:46] Model Checkpointing finished.
[2017-12-15 15:32:54] Epoch 0091 mean train/dev loss: 37.1012 / 73.4021
[2017-12-15 15:33:02] Epoch 0092 mean train/dev loss: 37.1283 / 71.4581
[2017-12-15 15:33:10] Epoch 0093 mean train/dev loss: 37.0725 / 73.3406
[2017-12-15 15:33:17] Epoch 0094 mean train/dev loss: 37.0464 / 72.9124
[2017-12-15 15:33:24] Epoch 0095 mean train/dev loss: 37.0492 / 72.1550
[2017-12-15 15:33:31] Epoch 0096 mean train/dev loss: 37.0334 / 72.8365
[2017-12-15 15:33:38] Epoch 0097 mean train/dev loss: 36.9771 / 69.6119
[2017-12-15 15:33:44] Epoch 0098 mean train/dev loss: 36.9005 / 71.5020
[2017-12-15 15:33:51] Epoch 0099 mean train/dev loss: 36.9493 / 70.3315
[2017-12-15 15:33:58] Epoch 0100 mean train/dev loss: 36.9035 / 70.3144
[2017-12-15 15:33:58] Checkpointing model at epoch 100 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:33:58] Model Checkpointing finished.
[2017-12-15 15:34:05] Epoch 0101 mean train/dev loss: 36.8486 / 71.6326
[2017-12-15 15:34:12] Epoch 0102 mean train/dev loss: 36.8457 / 71.8417
[2017-12-15 15:34:18] Epoch 0103 mean train/dev loss: 36.8248 / 68.1297
[2017-12-15 15:34:25] Epoch 0104 mean train/dev loss: 36.7936 / 73.4513
[2017-12-15 15:34:32] Epoch 0105 mean train/dev loss: 36.7449 / 72.9623
[2017-12-15 15:34:32] Learning rate decayed by 0.5000
[2017-12-15 15:34:39] Epoch 0106 mean train/dev loss: 36.4592 / 72.6412
[2017-12-15 15:34:44] Epoch 0107 mean train/dev loss: 36.4542 / 71.3848
[2017-12-15 15:34:50] Epoch 0108 mean train/dev loss: 36.4377 / 71.1239
[2017-12-15 15:34:55] Epoch 0109 mean train/dev loss: 36.4533 / 69.6559
[2017-12-15 15:35:00] Epoch 0110 mean train/dev loss: 36.4150 / 70.8800
[2017-12-15 15:35:00] Checkpointing model at epoch 110 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:35:01] Model Checkpointing finished.
[2017-12-15 15:35:06] Epoch 0111 mean train/dev loss: 36.4110 / 71.3809
[2017-12-15 15:35:11] Epoch 0112 mean train/dev loss: 36.3866 / 72.0239
[2017-12-15 15:35:17] Epoch 0113 mean train/dev loss: 36.3769 / 70.5828
[2017-12-15 15:35:22] Epoch 0114 mean train/dev loss: 36.4110 / 70.5146
[2017-12-15 15:35:27] Epoch 0115 mean train/dev loss: 36.3521 / 71.2658
[2017-12-15 15:35:33] Epoch 0116 mean train/dev loss: 36.3227 / 70.3617
[2017-12-15 15:35:38] Epoch 0117 mean train/dev loss: 36.3318 / 69.9850
[2017-12-15 15:35:43] Epoch 0118 mean train/dev loss: 36.3210 / 72.5087
[2017-12-15 15:35:48] Epoch 0119 mean train/dev loss: 36.2840 / 73.6210
[2017-12-15 15:35:54] Epoch 0120 mean train/dev loss: 36.3331 / 70.0426
[2017-12-15 15:35:54] Learning rate decayed by 0.5000
[2017-12-15 15:35:54] Checkpointing model at epoch 120 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:35:54] Model Checkpointing finished.
[2017-12-15 15:35:59] Epoch 0121 mean train/dev loss: 36.0952 / 68.3843
[2017-12-15 15:36:05] Epoch 0122 mean train/dev loss: 36.1084 / 69.6354
[2017-12-15 15:36:10] Epoch 0123 mean train/dev loss: 36.1030 / 70.6918
[2017-12-15 15:36:15] Epoch 0124 mean train/dev loss: 36.1000 / 70.3997
[2017-12-15 15:36:21] Epoch 0125 mean train/dev loss: 36.0748 / 69.1651
[2017-12-15 15:36:26] Epoch 0126 mean train/dev loss: 36.0703 / 69.8182
[2017-12-15 15:36:32] Epoch 0127 mean train/dev loss: 36.0755 / 69.8356
[2017-12-15 15:36:37] Epoch 0128 mean train/dev loss: 36.0647 / 71.3335
[2017-12-15 15:36:42] Epoch 0129 mean train/dev loss: 36.0566 / 70.2272
[2017-12-15 15:36:47] Epoch 0130 mean train/dev loss: 36.0456 / 70.3395
[2017-12-15 15:36:47] Checkpointing model at epoch 130 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:36:48] Model Checkpointing finished.
[2017-12-15 15:36:53] Epoch 0131 mean train/dev loss: 36.0359 / 68.4883
[2017-12-15 15:36:58] Epoch 0132 mean train/dev loss: 36.0297 / 70.6553
[2017-12-15 15:37:03] Epoch 0133 mean train/dev loss: 36.0171 / 70.3947
[2017-12-15 15:37:08] Epoch 0134 mean train/dev loss: 36.0176 / 71.0995
[2017-12-15 15:37:13] Epoch 0135 mean train/dev loss: 36.0039 / 69.7576
[2017-12-15 15:37:13] Learning rate decayed by 0.5000
[2017-12-15 15:37:18] Epoch 0136 mean train/dev loss: 35.9108 / 69.0296
[2017-12-15 15:37:24] Epoch 0137 mean train/dev loss: 35.9072 / 70.9091
[2017-12-15 15:37:29] Epoch 0138 mean train/dev loss: 35.9037 / 69.6707
[2017-12-15 15:37:34] Epoch 0139 mean train/dev loss: 35.9016 / 70.3276
[2017-12-15 15:37:39] Epoch 0140 mean train/dev loss: 35.8908 / 70.1827
[2017-12-15 15:37:39] Checkpointing model at epoch 140 for ffn.hl_50_50.lr_0.1.wd_0.001
[2017-12-15 15:37:40] Model Checkpointing finished.
[2017-12-15 15:37:45] Epoch 0141 mean train/dev loss: 35.8803 / 69.3247
[2017-12-15 15:37:50] Epoch 0142 mean train/dev loss: 35.8837 / 69.9760
[2017-12-15 15:37:55] Epoch 0143 mean train/dev loss: 35.8944 / 70.1912
[2017-12-15 15:38:00] Epoch 0144 mean train/dev loss: 35.8712 / 69.3915
[2017-12-15 15:38:00] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:38:00] 
                       *** Training finished *** 
[2017-12-15 15:38:01] Dev MSE: 69.3915
[2017-12-15 15:38:06] Training MSE: 35.8563
[2017-12-15 15:38:09] Experiment ffn.hl_50_50.lr_0.1.wd_0.001 logging ended.
