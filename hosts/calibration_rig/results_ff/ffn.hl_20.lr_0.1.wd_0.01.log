[2017-12-15 14:32:05] Experiment ffn.hl_20.lr_0.1.wd_0.01 logging started.
[2017-12-15 14:32:05] 
                       *** Starting Experiment ffn.hl_20.lr_0.1.wd_0.01 ***
                      
[2017-12-15 14:32:05] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 14:32:08] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 1)
                      )
[2017-12-15 14:32:08]  *** Training on GPU ***
[2017-12-15 14:32:14] Epoch 0001 mean train/dev loss: 15254.1328 / 553.3144
[2017-12-15 14:32:20] Epoch 0002 mean train/dev loss: 198.8205 / 327.0730
[2017-12-15 14:32:25] Epoch 0003 mean train/dev loss: 147.0845 / 183.5677
[2017-12-15 14:32:31] Epoch 0004 mean train/dev loss: 131.8465 / 194.2372
[2017-12-15 14:32:37] Epoch 0005 mean train/dev loss: 124.4088 / 195.3453
[2017-12-15 14:32:43] Epoch 0006 mean train/dev loss: 122.2434 / 146.2349
[2017-12-15 14:32:49] Epoch 0007 mean train/dev loss: 118.6472 / 158.4247
[2017-12-15 14:32:55] Epoch 0008 mean train/dev loss: 116.0217 / 165.6958
[2017-12-15 14:33:00] Epoch 0009 mean train/dev loss: 116.2036 / 147.6034
[2017-12-15 14:33:06] Epoch 0010 mean train/dev loss: 115.9565 / 151.9262
[2017-12-15 14:33:06] Checkpointing model at epoch 10 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:33:06] Model Checkpointing finished.
[2017-12-15 14:33:12] Epoch 0011 mean train/dev loss: 115.8921 / 131.6836
[2017-12-15 14:33:18] Epoch 0012 mean train/dev loss: 114.7911 / 145.5806
[2017-12-15 14:33:24] Epoch 0013 mean train/dev loss: 115.1787 / 113.2003
[2017-12-15 14:33:30] Epoch 0014 mean train/dev loss: 113.8838 / 122.6604
[2017-12-15 14:33:36] Epoch 0015 mean train/dev loss: 117.7316 / 126.4310
[2017-12-15 14:33:36] Learning rate decayed by 0.5000
[2017-12-15 14:33:42] Epoch 0016 mean train/dev loss: 109.7309 / 113.5776
[2017-12-15 14:33:48] Epoch 0017 mean train/dev loss: 109.7525 / 114.5832
[2017-12-15 14:33:53] Epoch 0018 mean train/dev loss: 110.2058 / 117.0335
[2017-12-15 14:34:00] Epoch 0019 mean train/dev loss: 110.7939 / 116.9968
[2017-12-15 14:34:06] Epoch 0020 mean train/dev loss: 110.0690 / 114.4432
[2017-12-15 14:34:06] Checkpointing model at epoch 20 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:34:06] Model Checkpointing finished.
[2017-12-15 14:34:12] Epoch 0021 mean train/dev loss: 110.0200 / 110.6967
[2017-12-15 14:34:18] Epoch 0022 mean train/dev loss: 110.0228 / 124.8701
[2017-12-15 14:34:23] Epoch 0023 mean train/dev loss: 110.0993 / 122.0965
[2017-12-15 14:34:29] Epoch 0024 mean train/dev loss: 109.7567 / 113.8971
[2017-12-15 14:34:35] Epoch 0025 mean train/dev loss: 110.1527 / 109.0005
[2017-12-15 14:34:42] Epoch 0026 mean train/dev loss: 109.6245 / 107.5832
[2017-12-15 14:34:48] Epoch 0027 mean train/dev loss: 108.8934 / 114.1992
[2017-12-15 14:34:53] Epoch 0028 mean train/dev loss: 105.9849 / 106.6888
[2017-12-15 14:34:59] Epoch 0029 mean train/dev loss: 105.6134 / 120.3309
[2017-12-15 14:35:06] Epoch 0030 mean train/dev loss: 105.0276 / 120.3086
[2017-12-15 14:35:06] Learning rate decayed by 0.5000
[2017-12-15 14:35:06] Checkpointing model at epoch 30 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:35:06] Model Checkpointing finished.
[2017-12-15 14:35:12] Epoch 0031 mean train/dev loss: 102.9724 / 107.9628
[2017-12-15 14:35:17] Epoch 0032 mean train/dev loss: 102.5185 / 113.9349
[2017-12-15 14:35:23] Epoch 0033 mean train/dev loss: 102.4802 / 107.8946
[2017-12-15 14:35:30] Epoch 0034 mean train/dev loss: 102.1625 / 112.6696
[2017-12-15 14:35:36] Epoch 0035 mean train/dev loss: 101.6223 / 112.8100
[2017-12-15 14:35:42] Epoch 0036 mean train/dev loss: 101.1453 / 122.6428
[2017-12-15 14:35:48] Epoch 0037 mean train/dev loss: 100.9034 / 114.0980
[2017-12-15 14:35:54] Epoch 0038 mean train/dev loss: 100.7007 / 125.6946
[2017-12-15 14:36:00] Epoch 0039 mean train/dev loss: 100.1919 / 111.2757
[2017-12-15 14:36:06] Epoch 0040 mean train/dev loss: 100.0752 / 114.4550
[2017-12-15 14:36:06] Checkpointing model at epoch 40 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:36:06] Model Checkpointing finished.
[2017-12-15 14:36:12] Epoch 0041 mean train/dev loss: 99.8367 / 116.9572
[2017-12-15 14:36:18] Epoch 0042 mean train/dev loss: 99.4773 / 109.7061
[2017-12-15 14:36:23] Epoch 0043 mean train/dev loss: 99.4593 / 108.8646
[2017-12-15 14:36:29] Epoch 0044 mean train/dev loss: 98.1867 / 130.4957
[2017-12-15 14:36:35] Epoch 0045 mean train/dev loss: 96.6567 / 118.5439
[2017-12-15 14:36:35] Learning rate decayed by 0.5000
[2017-12-15 14:36:41] Epoch 0046 mean train/dev loss: 94.8759 / 109.2571
[2017-12-15 14:36:47] Epoch 0047 mean train/dev loss: 94.5373 / 104.5653
[2017-12-15 14:36:53] Epoch 0048 mean train/dev loss: 94.1842 / 111.4389
[2017-12-15 14:37:00] Epoch 0049 mean train/dev loss: 93.8002 / 109.7143
[2017-12-15 14:37:06] Epoch 0050 mean train/dev loss: 93.2594 / 107.9403
[2017-12-15 14:37:06] Checkpointing model at epoch 50 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:37:06] Model Checkpointing finished.
[2017-12-15 14:37:12] Epoch 0051 mean train/dev loss: 91.8365 / 114.1785
[2017-12-15 14:37:18] Epoch 0052 mean train/dev loss: 90.6499 / 129.6149
[2017-12-15 14:37:24] Epoch 0053 mean train/dev loss: 89.9026 / 112.2859
[2017-12-15 14:37:30] Epoch 0054 mean train/dev loss: 89.0375 / 114.5475
[2017-12-15 14:37:36] Epoch 0055 mean train/dev loss: 88.3065 / 116.3685
[2017-12-15 14:37:42] Epoch 0056 mean train/dev loss: 87.5839 / 108.8835
[2017-12-15 14:37:47] Epoch 0057 mean train/dev loss: 87.1386 / 112.6704
[2017-12-15 14:37:53] Epoch 0058 mean train/dev loss: 86.5769 / 114.4518
[2017-12-15 14:37:59] Epoch 0059 mean train/dev loss: 84.5229 / 101.4516
[2017-12-15 14:38:05] Epoch 0060 mean train/dev loss: 81.4712 / 99.4573
[2017-12-15 14:38:05] Learning rate decayed by 0.5000
[2017-12-15 14:38:05] Checkpointing model at epoch 60 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:38:05] Model Checkpointing finished.
[2017-12-15 14:38:11] Epoch 0061 mean train/dev loss: 79.3300 / 105.5134
[2017-12-15 14:38:17] Epoch 0062 mean train/dev loss: 78.6285 / 94.3254
[2017-12-15 14:38:23] Epoch 0063 mean train/dev loss: 78.1415 / 101.7267
[2017-12-15 14:38:30] Epoch 0064 mean train/dev loss: 77.6092 / 98.1009
[2017-12-15 14:38:37] Epoch 0065 mean train/dev loss: 77.2401 / 102.9199
[2017-12-15 14:38:43] Epoch 0066 mean train/dev loss: 76.8701 / 98.3711
[2017-12-15 14:38:49] Epoch 0067 mean train/dev loss: 76.3614 / 102.1591
[2017-12-15 14:38:55] Epoch 0068 mean train/dev loss: 76.1150 / 103.3266
[2017-12-15 14:39:02] Epoch 0069 mean train/dev loss: 75.7547 / 97.4063
[2017-12-15 14:39:07] Epoch 0070 mean train/dev loss: 75.6007 / 103.0465
[2017-12-15 14:39:07] Checkpointing model at epoch 70 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:39:07] Model Checkpointing finished.
[2017-12-15 14:39:14] Epoch 0071 mean train/dev loss: 75.3642 / 95.5495
[2017-12-15 14:39:20] Epoch 0072 mean train/dev loss: 75.1824 / 107.3025
[2017-12-15 14:39:26] Epoch 0073 mean train/dev loss: 75.0069 / 102.8317
[2017-12-15 14:39:32] Epoch 0074 mean train/dev loss: 74.8642 / 104.3793
[2017-12-15 14:39:37] Epoch 0075 mean train/dev loss: 74.7499 / 115.5292
[2017-12-15 14:39:37] Learning rate decayed by 0.5000
[2017-12-15 14:39:43] Epoch 0076 mean train/dev loss: 74.2343 / 101.8087
[2017-12-15 14:39:49] Epoch 0077 mean train/dev loss: 74.1948 / 98.9028
[2017-12-15 14:39:55] Epoch 0078 mean train/dev loss: 74.1514 / 97.5141
[2017-12-15 14:40:01] Epoch 0079 mean train/dev loss: 74.0797 / 100.9727
[2017-12-15 14:40:07] Epoch 0080 mean train/dev loss: 74.0085 / 100.4782
[2017-12-15 14:40:07] Checkpointing model at epoch 80 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:40:07] Model Checkpointing finished.
[2017-12-15 14:40:13] Epoch 0081 mean train/dev loss: 73.9633 / 100.4220
[2017-12-15 14:40:20] Epoch 0082 mean train/dev loss: 73.8601 / 103.1318
[2017-12-15 14:40:26] Epoch 0083 mean train/dev loss: 73.8178 / 102.3339
[2017-12-15 14:40:32] Epoch 0084 mean train/dev loss: 73.7707 / 103.9508
[2017-12-15 14:40:38] Epoch 0085 mean train/dev loss: 73.6516 / 103.1053
[2017-12-15 14:40:44] Epoch 0086 mean train/dev loss: 73.5910 / 103.3295
[2017-12-15 14:40:49] Epoch 0087 mean train/dev loss: 73.5549 / 102.8783
[2017-12-15 14:40:53] Epoch 0088 mean train/dev loss: 73.4835 / 103.3909
[2017-12-15 14:40:58] Epoch 0089 mean train/dev loss: 73.4501 / 102.0165
[2017-12-15 14:41:03] Epoch 0090 mean train/dev loss: 73.3158 / 103.5215
[2017-12-15 14:41:03] Learning rate decayed by 0.5000
[2017-12-15 14:41:03] Checkpointing model at epoch 90 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:41:03] Model Checkpointing finished.
[2017-12-15 14:41:08] Epoch 0091 mean train/dev loss: 73.1040 / 98.6312
[2017-12-15 14:41:12] Epoch 0092 mean train/dev loss: 73.0648 / 102.5991
[2017-12-15 14:41:17] Epoch 0093 mean train/dev loss: 72.9881 / 103.7039
[2017-12-15 14:41:22] Epoch 0094 mean train/dev loss: 72.9809 / 101.1656
[2017-12-15 14:41:27] Epoch 0095 mean train/dev loss: 72.9167 / 106.6749
[2017-12-15 14:41:32] Epoch 0096 mean train/dev loss: 72.9337 / 98.6449
[2017-12-15 14:41:37] Epoch 0097 mean train/dev loss: 72.9074 / 103.4831
[2017-12-15 14:41:42] Epoch 0098 mean train/dev loss: 72.8686 / 107.4248
[2017-12-15 14:41:47] Epoch 0099 mean train/dev loss: 72.8557 / 102.3576
[2017-12-15 14:41:52] Epoch 0100 mean train/dev loss: 72.7832 / 103.0362
[2017-12-15 14:41:52] Checkpointing model at epoch 100 for ffn.hl_20.lr_0.1.wd_0.01
[2017-12-15 14:41:52] Model Checkpointing finished.
[2017-12-15 14:41:57] Epoch 0101 mean train/dev loss: 72.7808 / 99.1926
[2017-12-15 14:42:01] Epoch 0102 mean train/dev loss: 72.7505 / 99.3448
[2017-12-15 14:42:06] Epoch 0103 mean train/dev loss: 72.6957 / 97.9437
[2017-12-15 14:42:06] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:42:06] 
                       *** Training finished *** 
[2017-12-15 14:42:07] Dev MSE: 97.9437
[2017-12-15 14:42:11] Training MSE: 72.8255
[2017-12-15 14:42:13] Experiment ffn.hl_20.lr_0.1.wd_0.01 logging ended.
