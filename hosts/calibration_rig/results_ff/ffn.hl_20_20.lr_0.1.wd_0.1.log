[2017-12-15 14:42:15] Experiment ffn.hl_20_20.lr_0.1.wd_0.1 logging started.
[2017-12-15 14:42:15] 
                       *** Starting Experiment ffn.hl_20_20.lr_0.1.wd_0.1 ***
                      
[2017-12-15 14:42:15] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 14:42:15] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 1)
                      )
[2017-12-15 14:42:15]  *** Training on GPU ***
[2017-12-15 14:42:22] Epoch 0001 mean train/dev loss: 6278.5432 / 414.1380
[2017-12-15 14:42:29] Epoch 0002 mean train/dev loss: 224.8889 / 727.1970
[2017-12-15 14:42:36] Epoch 0003 mean train/dev loss: 142.4663 / 173.1630
[2017-12-15 14:42:42] Epoch 0004 mean train/dev loss: 132.8357 / 161.3711
[2017-12-15 14:42:49] Epoch 0005 mean train/dev loss: 134.8033 / 119.6778
[2017-12-15 14:42:56] Epoch 0006 mean train/dev loss: 156.0235 / 135.3442
[2017-12-15 14:43:03] Epoch 0007 mean train/dev loss: 132.2454 / 130.2088
[2017-12-15 14:43:10] Epoch 0008 mean train/dev loss: 143.0357 / 136.0045
[2017-12-15 14:43:17] Epoch 0009 mean train/dev loss: 126.3886 / 134.6709
[2017-12-15 14:43:23] Epoch 0010 mean train/dev loss: 131.4031 / 174.9653
[2017-12-15 14:43:23] Checkpointing model at epoch 10 for ffn.hl_20_20.lr_0.1.wd_0.1
[2017-12-15 14:43:24] Model Checkpointing finished.
[2017-12-15 14:43:31] Epoch 0011 mean train/dev loss: 136.2362 / 118.9295
[2017-12-15 14:43:37] Epoch 0012 mean train/dev loss: 125.0299 / 208.0189
[2017-12-15 14:43:44] Epoch 0013 mean train/dev loss: 116.4678 / 144.0818
[2017-12-15 14:43:51] Epoch 0014 mean train/dev loss: 129.2516 / 204.0096
[2017-12-15 14:43:58] Epoch 0015 mean train/dev loss: 111.7487 / 111.7617
[2017-12-15 14:43:58] Learning rate decayed by 0.5000
[2017-12-15 14:44:05] Epoch 0016 mean train/dev loss: 94.0772 / 133.5630
[2017-12-15 14:44:12] Epoch 0017 mean train/dev loss: 91.6408 / 157.3448
[2017-12-15 14:44:19] Epoch 0018 mean train/dev loss: 92.3443 / 119.3587
[2017-12-15 14:44:26] Epoch 0019 mean train/dev loss: 91.1087 / 151.5146
[2017-12-15 14:44:33] Epoch 0020 mean train/dev loss: 92.3870 / 136.1858
[2017-12-15 14:44:33] Checkpointing model at epoch 20 for ffn.hl_20_20.lr_0.1.wd_0.1
[2017-12-15 14:44:33] Model Checkpointing finished.
[2017-12-15 14:44:40] Epoch 0021 mean train/dev loss: 91.2983 / 122.5725
[2017-12-15 14:44:46] Epoch 0022 mean train/dev loss: 89.7195 / 136.5695
[2017-12-15 14:44:53] Epoch 0023 mean train/dev loss: 92.3194 / 153.4349
[2017-12-15 14:45:00] Epoch 0024 mean train/dev loss: 87.8088 / 131.5238
[2017-12-15 14:45:08] Epoch 0025 mean train/dev loss: 88.5360 / 122.4309
[2017-12-15 14:45:15] Epoch 0026 mean train/dev loss: 89.7949 / 153.6812
[2017-12-15 14:45:22] Epoch 0027 mean train/dev loss: 88.2603 / 117.3199
[2017-12-15 14:45:28] Epoch 0028 mean train/dev loss: 86.3381 / 118.4558
[2017-12-15 14:45:35] Epoch 0029 mean train/dev loss: 89.9898 / 147.1462
[2017-12-15 14:45:42] Epoch 0030 mean train/dev loss: 87.8952 / 106.6569
[2017-12-15 14:45:42] Learning rate decayed by 0.5000
[2017-12-15 14:45:42] Checkpointing model at epoch 30 for ffn.hl_20_20.lr_0.1.wd_0.1
[2017-12-15 14:45:43] Model Checkpointing finished.
[2017-12-15 14:45:50] Epoch 0031 mean train/dev loss: 78.1309 / 115.0608
[2017-12-15 14:45:57] Epoch 0032 mean train/dev loss: 77.9281 / 113.4167
[2017-12-15 14:46:04] Epoch 0033 mean train/dev loss: 78.9212 / 107.2482
[2017-12-15 14:46:11] Epoch 0034 mean train/dev loss: 78.7443 / 128.9997
[2017-12-15 14:46:18] Epoch 0035 mean train/dev loss: 77.7154 / 138.9328
[2017-12-15 14:46:25] Epoch 0036 mean train/dev loss: 78.3361 / 100.0091
[2017-12-15 14:46:32] Epoch 0037 mean train/dev loss: 78.1522 / 119.0095
[2017-12-15 14:46:39] Epoch 0038 mean train/dev loss: 77.4071 / 85.1537
[2017-12-15 14:46:45] Epoch 0039 mean train/dev loss: 78.0677 / 106.8138
[2017-12-15 14:46:52] Epoch 0040 mean train/dev loss: 77.1297 / 106.7116
[2017-12-15 14:46:52] Checkpointing model at epoch 40 for ffn.hl_20_20.lr_0.1.wd_0.1
[2017-12-15 14:46:52] Model Checkpointing finished.
[2017-12-15 14:46:59] Epoch 0041 mean train/dev loss: 77.7884 / 110.7409
[2017-12-15 14:47:06] Epoch 0042 mean train/dev loss: 77.7994 / 113.8875
[2017-12-15 14:47:12] Epoch 0043 mean train/dev loss: 77.5077 / 108.4024
[2017-12-15 14:47:19] Epoch 0044 mean train/dev loss: 76.8427 / 109.2220
[2017-12-15 14:47:26] Epoch 0045 mean train/dev loss: 77.9133 / 101.4743
[2017-12-15 14:47:26] Learning rate decayed by 0.5000
[2017-12-15 14:47:32] Epoch 0046 mean train/dev loss: 73.2246 / 116.6558
[2017-12-15 14:47:39] Epoch 0047 mean train/dev loss: 73.7170 / 109.0033
[2017-12-15 14:47:46] Epoch 0048 mean train/dev loss: 73.1412 / 101.6741
[2017-12-15 14:47:54] Epoch 0049 mean train/dev loss: 73.0111 / 105.6309
[2017-12-15 14:48:00] Epoch 0050 mean train/dev loss: 73.1344 / 106.2646
[2017-12-15 14:48:00] Checkpointing model at epoch 50 for ffn.hl_20_20.lr_0.1.wd_0.1
[2017-12-15 14:48:00] Model Checkpointing finished.
[2017-12-15 14:48:07] Epoch 0051 mean train/dev loss: 72.8449 / 91.5035
[2017-12-15 14:48:14] Epoch 0052 mean train/dev loss: 72.9259 / 109.8829
[2017-12-15 14:48:21] Epoch 0053 mean train/dev loss: 73.5263 / 112.9737
[2017-12-15 14:48:28] Epoch 0054 mean train/dev loss: 72.8230 / 107.4628
[2017-12-15 14:48:35] Epoch 0055 mean train/dev loss: 72.9175 / 90.4246
[2017-12-15 14:48:42] Epoch 0056 mean train/dev loss: 72.4931 / 104.2217
[2017-12-15 14:48:49] Epoch 0057 mean train/dev loss: 72.2828 / 95.5753
[2017-12-15 14:48:55] Epoch 0058 mean train/dev loss: 72.5879 / 103.9419
[2017-12-15 14:49:02] Epoch 0059 mean train/dev loss: 72.8518 / 94.3682
[2017-12-15 14:49:09] Epoch 0060 mean train/dev loss: 72.5047 / 108.7582
[2017-12-15 14:49:09] Learning rate decayed by 0.5000
[2017-12-15 14:49:09] Checkpointing model at epoch 60 for ffn.hl_20_20.lr_0.1.wd_0.1
[2017-12-15 14:49:09] Model Checkpointing finished.
[2017-12-15 14:49:16] Epoch 0061 mean train/dev loss: 70.1356 / 103.0932
[2017-12-15 14:49:23] Epoch 0062 mean train/dev loss: 70.2474 / 96.0753
[2017-12-15 14:49:29] Epoch 0063 mean train/dev loss: 70.2131 / 102.3011
[2017-12-15 14:49:36] Epoch 0064 mean train/dev loss: 70.4077 / 96.4189
[2017-12-15 14:49:43] Epoch 0065 mean train/dev loss: 70.3436 / 100.5543
[2017-12-15 14:49:50] Epoch 0066 mean train/dev loss: 70.0932 / 102.5588
[2017-12-15 14:49:57] Epoch 0067 mean train/dev loss: 70.1540 / 92.2551
[2017-12-15 14:50:04] Epoch 0068 mean train/dev loss: 69.9285 / 91.5521
[2017-12-15 14:50:10] Epoch 0069 mean train/dev loss: 69.8572 / 101.9195
[2017-12-15 14:50:16] Epoch 0070 mean train/dev loss: 69.6448 / 95.4850
[2017-12-15 14:50:16] Checkpointing model at epoch 70 for ffn.hl_20_20.lr_0.1.wd_0.1
[2017-12-15 14:50:17] Model Checkpointing finished.
[2017-12-15 14:50:23] Epoch 0071 mean train/dev loss: 69.8703 / 101.2035
[2017-12-15 14:50:30] Epoch 0072 mean train/dev loss: 70.0218 / 96.6602
[2017-12-15 14:50:37] Epoch 0073 mean train/dev loss: 69.8606 / 97.0010
[2017-12-15 14:50:44] Epoch 0074 mean train/dev loss: 69.7124 / 91.7083
[2017-12-15 14:50:51] Epoch 0075 mean train/dev loss: 69.7999 / 94.6785
[2017-12-15 14:50:51] Learning rate decayed by 0.5000
[2017-12-15 14:50:57] Epoch 0076 mean train/dev loss: 68.3397 / 95.6470
[2017-12-15 14:51:04] Epoch 0077 mean train/dev loss: 68.0358 / 91.5535
[2017-12-15 14:51:11] Epoch 0078 mean train/dev loss: 67.9848 / 91.5827
[2017-12-15 14:51:18] Epoch 0079 mean train/dev loss: 67.6737 / 98.9477
[2017-12-15 14:51:18] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:51:18] 
                       *** Training finished *** 
[2017-12-15 14:51:19] Dev MSE: 98.9477
[2017-12-15 14:51:25] Training MSE: 67.9483
[2017-12-15 14:51:27] Experiment ffn.hl_20_20.lr_0.1.wd_0.1 logging ended.
