[2017-12-15 16:01:48] Experiment ffn.hl_100.lr_0.1.wd_10 logging started.
[2017-12-15 16:01:48] 
                       *** Starting Experiment ffn.hl_100.lr_0.1.wd_10 ***
                      
[2017-12-15 16:01:48] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 16:01:48] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 1)
                      )
[2017-12-15 16:01:48]  *** Training on GPU ***
[2017-12-15 16:01:56] Epoch 0001 mean train/dev loss: 8192.7162 / 306.1879
[2017-12-15 16:02:03] Epoch 0002 mean train/dev loss: 273.3552 / 257.8307
[2017-12-15 16:02:11] Epoch 0003 mean train/dev loss: 249.0180 / 260.1204
[2017-12-15 16:02:18] Epoch 0004 mean train/dev loss: 246.3849 / 259.5299
[2017-12-15 16:02:25] Epoch 0005 mean train/dev loss: 247.7950 / 245.1584
[2017-12-15 16:02:32] Epoch 0006 mean train/dev loss: 260.1049 / 288.3581
[2017-12-15 16:02:39] Epoch 0007 mean train/dev loss: 243.9372 / 219.2014
[2017-12-15 16:02:47] Epoch 0008 mean train/dev loss: 251.5917 / 236.1231
[2017-12-15 16:02:54] Epoch 0009 mean train/dev loss: 247.5128 / 209.9928
[2017-12-15 16:03:01] Epoch 0010 mean train/dev loss: 252.6676 / 317.8989
[2017-12-15 16:03:01] Checkpointing model at epoch 10 for ffn.hl_100.lr_0.1.wd_10
[2017-12-15 16:03:02] Model Checkpointing finished.
[2017-12-15 16:03:09] Epoch 0011 mean train/dev loss: 262.2071 / 223.6520
[2017-12-15 16:03:16] Epoch 0012 mean train/dev loss: 249.7538 / 228.6661
[2017-12-15 16:03:24] Epoch 0013 mean train/dev loss: 246.5765 / 220.0550
[2017-12-15 16:03:31] Epoch 0014 mean train/dev loss: 245.9030 / 230.0917
[2017-12-15 16:03:38] Epoch 0015 mean train/dev loss: 242.6567 / 246.9513
[2017-12-15 16:03:38] Learning rate decayed by 0.5000
[2017-12-15 16:03:45] Epoch 0016 mean train/dev loss: 229.5909 / 224.5552
[2017-12-15 16:03:52] Epoch 0017 mean train/dev loss: 230.7462 / 228.6758
[2017-12-15 16:04:00] Epoch 0018 mean train/dev loss: 232.7528 / 221.9575
[2017-12-15 16:04:07] Epoch 0019 mean train/dev loss: 231.4798 / 253.7485
[2017-12-15 16:04:15] Epoch 0020 mean train/dev loss: 234.6982 / 200.2014
[2017-12-15 16:04:15] Checkpointing model at epoch 20 for ffn.hl_100.lr_0.1.wd_10
[2017-12-15 16:04:15] Model Checkpointing finished.
[2017-12-15 16:04:22] Epoch 0021 mean train/dev loss: 232.4340 / 239.7042
[2017-12-15 16:04:29] Epoch 0022 mean train/dev loss: 233.3957 / 216.1317
[2017-12-15 16:04:37] Epoch 0023 mean train/dev loss: 232.5779 / 216.7242
[2017-12-15 16:04:44] Epoch 0024 mean train/dev loss: 232.7618 / 217.2987
[2017-12-15 16:04:51] Epoch 0025 mean train/dev loss: 232.0755 / 235.0389
[2017-12-15 16:04:59] Epoch 0026 mean train/dev loss: 235.0439 / 205.6132
[2017-12-15 16:05:05] Epoch 0027 mean train/dev loss: 231.0985 / 221.7316
[2017-12-15 16:05:13] Epoch 0028 mean train/dev loss: 231.1377 / 254.1697
[2017-12-15 16:05:20] Epoch 0029 mean train/dev loss: 231.0491 / 252.1893
[2017-12-15 16:05:27] Epoch 0030 mean train/dev loss: 231.8610 / 218.6779
[2017-12-15 16:05:27] Learning rate decayed by 0.5000
[2017-12-15 16:05:27] Checkpointing model at epoch 30 for ffn.hl_100.lr_0.1.wd_10
[2017-12-15 16:05:27] Model Checkpointing finished.
[2017-12-15 16:05:35] Epoch 0031 mean train/dev loss: 225.9087 / 226.1826
[2017-12-15 16:05:42] Epoch 0032 mean train/dev loss: 226.2810 / 208.7753
[2017-12-15 16:05:49] Epoch 0033 mean train/dev loss: 227.1011 / 224.4338
[2017-12-15 16:05:56] Epoch 0034 mean train/dev loss: 227.1662 / 206.8800
[2017-12-15 16:06:03] Epoch 0035 mean train/dev loss: 226.5600 / 210.5320
[2017-12-15 16:06:10] Epoch 0036 mean train/dev loss: 226.5185 / 201.2575
[2017-12-15 16:06:17] Epoch 0037 mean train/dev loss: 226.4386 / 211.0261
[2017-12-15 16:06:25] Epoch 0038 mean train/dev loss: 226.1384 / 221.9124
[2017-12-15 16:06:32] Epoch 0039 mean train/dev loss: 227.2358 / 218.4892
[2017-12-15 16:06:40] Epoch 0040 mean train/dev loss: 227.8163 / 220.3497
[2017-12-15 16:06:40] Checkpointing model at epoch 40 for ffn.hl_100.lr_0.1.wd_10
[2017-12-15 16:06:40] Model Checkpointing finished.
[2017-12-15 16:06:47] Epoch 0041 mean train/dev loss: 226.6222 / 217.1897
[2017-12-15 16:06:55] Epoch 0042 mean train/dev loss: 226.7667 / 215.2607
[2017-12-15 16:07:02] Epoch 0043 mean train/dev loss: 225.9226 / 214.2312
[2017-12-15 16:07:09] Epoch 0044 mean train/dev loss: 226.4859 / 236.2379
[2017-12-15 16:07:16] Epoch 0045 mean train/dev loss: 227.1519 / 216.0621
[2017-12-15 16:07:16] Learning rate decayed by 0.5000
[2017-12-15 16:07:24] Epoch 0046 mean train/dev loss: 223.9455 / 213.9866
[2017-12-15 16:07:31] Epoch 0047 mean train/dev loss: 224.2449 / 220.4921
[2017-12-15 16:07:38] Epoch 0048 mean train/dev loss: 224.1984 / 218.2321
[2017-12-15 16:07:45] Epoch 0049 mean train/dev loss: 225.0245 / 213.2849
[2017-12-15 16:07:53] Epoch 0050 mean train/dev loss: 224.4971 / 208.3267
[2017-12-15 16:07:53] Checkpointing model at epoch 50 for ffn.hl_100.lr_0.1.wd_10
[2017-12-15 16:07:53] Model Checkpointing finished.
[2017-12-15 16:08:00] Epoch 0051 mean train/dev loss: 224.8278 / 212.6384
[2017-12-15 16:08:07] Epoch 0052 mean train/dev loss: 224.4051 / 212.7819
[2017-12-15 16:08:15] Epoch 0053 mean train/dev loss: 224.5686 / 224.9060
[2017-12-15 16:08:22] Epoch 0054 mean train/dev loss: 224.6435 / 212.5515
[2017-12-15 16:08:30] Epoch 0055 mean train/dev loss: 224.7887 / 208.7639
[2017-12-15 16:08:37] Epoch 0056 mean train/dev loss: 224.6024 / 215.2333
[2017-12-15 16:08:44] Epoch 0057 mean train/dev loss: 224.4341 / 203.9948
[2017-12-15 16:08:51] Epoch 0058 mean train/dev loss: 224.4311 / 218.6345
[2017-12-15 16:08:57] Epoch 0059 mean train/dev loss: 224.7241 / 213.6519
[2017-12-15 16:09:03] Epoch 0060 mean train/dev loss: 224.6122 / 213.5465
[2017-12-15 16:09:03] Learning rate decayed by 0.5000
[2017-12-15 16:09:03] Checkpointing model at epoch 60 for ffn.hl_100.lr_0.1.wd_10
[2017-12-15 16:09:04] Model Checkpointing finished.
[2017-12-15 16:09:10] Epoch 0061 mean train/dev loss: 223.3282 / 210.3650
[2017-12-15 16:09:10] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 16:09:10] 
                       *** Training finished *** 
[2017-12-15 16:09:11] Dev MSE: 210.3650
[2017-12-15 16:09:16] Training MSE: 224.2026
[2017-12-15 16:09:17] Experiment ffn.hl_100.lr_0.1.wd_10 logging ended.
