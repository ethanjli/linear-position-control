[2017-12-15 14:03:59] Experiment ffn.hl_linear.lr_0.1.wd_0.001 logging started.
[2017-12-15 14:03:59] 
                       *** Starting Experiment ffn.hl_linear.lr_0.1.wd_0.001 ***
                      
[2017-12-15 14:03:59] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] []  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 14:04:02] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 1)
                      )
[2017-12-15 14:04:02]  *** Training on GPU ***
[2017-12-15 14:04:09] Epoch 0001 mean train/dev loss: 286387.8249 / 244944.9688
[2017-12-15 14:04:15] Epoch 0002 mean train/dev loss: 214175.3464 / 184658.7656
[2017-12-15 14:04:22] Epoch 0003 mean train/dev loss: 162272.8650 / 138465.4688
[2017-12-15 14:04:29] Epoch 0004 mean train/dev loss: 120452.2052 / 101058.0547
[2017-12-15 14:04:36] Epoch 0005 mean train/dev loss: 86563.3065 / 71094.7656
[2017-12-15 14:04:42] Epoch 0006 mean train/dev loss: 59674.1280 / 47656.1523
[2017-12-15 14:04:48] Epoch 0007 mean train/dev loss: 38997.5527 / 30055.1660
[2017-12-15 14:04:55] Epoch 0008 mean train/dev loss: 23787.9948 / 17465.2891
[2017-12-15 14:05:01] Epoch 0009 mean train/dev loss: 13253.7251 / 9106.0801
[2017-12-15 14:05:07] Epoch 0010 mean train/dev loss: 6556.6532 / 4119.6738
[2017-12-15 14:05:07] Checkpointing model at epoch 10 for ffn.hl_linear.lr_0.1.wd_0.001
[2017-12-15 14:05:08] Model Checkpointing finished.
[2017-12-15 14:05:14] Epoch 0011 mean train/dev loss: 2783.4804 / 1556.9569
[2017-12-15 14:05:21] Epoch 0012 mean train/dev loss: 993.8334 / 497.4847
[2017-12-15 14:05:27] Epoch 0013 mean train/dev loss: 328.4303 / 178.1284
[2017-12-15 14:05:34] Epoch 0014 mean train/dev loss: 151.7346 / 118.7397
[2017-12-15 14:05:40] Epoch 0015 mean train/dev loss: 121.6341 / 113.2614
[2017-12-15 14:05:40] Learning rate decayed by 0.5000
[2017-12-15 14:05:47] Epoch 0016 mean train/dev loss: 118.6814 / 113.4547
[2017-12-15 14:05:54] Epoch 0017 mean train/dev loss: 118.4052 / 113.8131
[2017-12-15 14:06:00] Epoch 0018 mean train/dev loss: 118.3464 / 113.9080
[2017-12-15 14:06:07] Epoch 0019 mean train/dev loss: 118.3498 / 113.8763
[2017-12-15 14:06:13] Epoch 0020 mean train/dev loss: 118.3758 / 113.2864
[2017-12-15 14:06:13] Checkpointing model at epoch 20 for ffn.hl_linear.lr_0.1.wd_0.001
[2017-12-15 14:06:14] Model Checkpointing finished.
[2017-12-15 14:06:21] Epoch 0021 mean train/dev loss: 118.3588 / 113.4696
[2017-12-15 14:06:27] Epoch 0022 mean train/dev loss: 118.3557 / 114.2449
[2017-12-15 14:06:34] Epoch 0023 mean train/dev loss: 118.3750 / 113.3033
[2017-12-15 14:06:41] Epoch 0024 mean train/dev loss: 118.3902 / 113.6650
[2017-12-15 14:06:47] Epoch 0025 mean train/dev loss: 118.3871 / 113.3285
[2017-12-15 14:06:53] Epoch 0026 mean train/dev loss: 118.3726 / 113.1588
[2017-12-15 14:07:00] Epoch 0027 mean train/dev loss: 118.3540 / 113.2921
[2017-12-15 14:07:06] Epoch 0028 mean train/dev loss: 118.3897 / 113.6832
[2017-12-15 14:07:13] Epoch 0029 mean train/dev loss: 118.3906 / 113.3883
[2017-12-15 14:07:20] Epoch 0030 mean train/dev loss: 118.3591 / 113.3692
[2017-12-15 14:07:20] Learning rate decayed by 0.5000
[2017-12-15 14:07:20] Checkpointing model at epoch 30 for ffn.hl_linear.lr_0.1.wd_0.001
[2017-12-15 14:07:20] Model Checkpointing finished.
[2017-12-15 14:07:27] Epoch 0031 mean train/dev loss: 118.3450 / 113.6875
[2017-12-15 14:07:34] Epoch 0032 mean train/dev loss: 118.3537 / 113.3000
[2017-12-15 14:07:40] Epoch 0033 mean train/dev loss: 118.3486 / 113.3231
[2017-12-15 14:07:47] Epoch 0034 mean train/dev loss: 118.3403 / 113.5985
[2017-12-15 14:07:53] Epoch 0035 mean train/dev loss: 118.3243 / 113.3609
[2017-12-15 14:08:00] Epoch 0036 mean train/dev loss: 118.3470 / 113.0984
[2017-12-15 14:08:07] Epoch 0037 mean train/dev loss: 118.3755 / 113.4610
[2017-12-15 14:08:14] Epoch 0038 mean train/dev loss: 118.3295 / 113.4473
[2017-12-15 14:08:21] Epoch 0039 mean train/dev loss: 118.3476 / 113.1277
[2017-12-15 14:08:27] Epoch 0040 mean train/dev loss: 118.3306 / 113.6532
[2017-12-15 14:08:27] Checkpointing model at epoch 40 for ffn.hl_linear.lr_0.1.wd_0.001
[2017-12-15 14:08:28] Model Checkpointing finished.
[2017-12-15 14:08:35] Epoch 0041 mean train/dev loss: 118.3862 / 113.2381
[2017-12-15 14:08:41] Epoch 0042 mean train/dev loss: 118.3190 / 113.7802
[2017-12-15 14:08:48] Epoch 0043 mean train/dev loss: 118.3628 / 113.5072
[2017-12-15 14:08:55] Epoch 0044 mean train/dev loss: 118.3095 / 113.7365
[2017-12-15 14:09:01] Epoch 0045 mean train/dev loss: 118.3441 / 113.7286
[2017-12-15 14:09:01] Learning rate decayed by 0.5000
[2017-12-15 14:09:08] Epoch 0046 mean train/dev loss: 118.3211 / 113.4588
[2017-12-15 14:09:15] Epoch 0047 mean train/dev loss: 118.3230 / 113.5338
[2017-12-15 14:09:21] Epoch 0048 mean train/dev loss: 118.3315 / 113.6360
[2017-12-15 14:09:28] Epoch 0049 mean train/dev loss: 118.3197 / 113.5223
[2017-12-15 14:09:35] Epoch 0050 mean train/dev loss: 118.2987 / 113.5353
[2017-12-15 14:09:35] Checkpointing model at epoch 50 for ffn.hl_linear.lr_0.1.wd_0.001
[2017-12-15 14:09:35] Model Checkpointing finished.
[2017-12-15 14:09:42] Epoch 0051 mean train/dev loss: 118.3174 / 113.5495
[2017-12-15 14:09:48] Epoch 0052 mean train/dev loss: 118.3053 / 113.2766
[2017-12-15 14:09:56] Epoch 0053 mean train/dev loss: 118.3021 / 113.6266
[2017-12-15 14:10:02] Epoch 0054 mean train/dev loss: 118.3456 / 113.5065
[2017-12-15 14:10:09] Epoch 0055 mean train/dev loss: 118.3112 / 113.4532
[2017-12-15 14:10:15] Epoch 0056 mean train/dev loss: 118.3150 / 113.5763
[2017-12-15 14:10:21] Epoch 0057 mean train/dev loss: 118.3275 / 113.8252
[2017-12-15 14:10:28] Epoch 0058 mean train/dev loss: 118.2875 / 113.5411
[2017-12-15 14:10:34] Epoch 0059 mean train/dev loss: 118.3343 / 113.5610
[2017-12-15 14:10:41] Epoch 0060 mean train/dev loss: 118.3237 / 113.4158
[2017-12-15 14:10:41] Learning rate decayed by 0.5000
[2017-12-15 14:10:41] Checkpointing model at epoch 60 for ffn.hl_linear.lr_0.1.wd_0.001
[2017-12-15 14:10:41] Model Checkpointing finished.
[2017-12-15 14:10:48] Epoch 0061 mean train/dev loss: 118.3053 / 113.5946
[2017-12-15 14:10:54] Epoch 0062 mean train/dev loss: 118.3053 / 113.5994
[2017-12-15 14:11:01] Epoch 0063 mean train/dev loss: 118.3041 / 113.5987
[2017-12-15 14:11:08] Epoch 0064 mean train/dev loss: 118.3169 / 113.8667
[2017-12-15 14:11:15] Epoch 0065 mean train/dev loss: 118.3066 / 113.5560
[2017-12-15 14:11:21] Epoch 0066 mean train/dev loss: 118.2930 / 113.6517
[2017-12-15 14:11:28] Epoch 0067 mean train/dev loss: 118.3190 / 113.6961
[2017-12-15 14:11:35] Epoch 0068 mean train/dev loss: 118.2978 / 113.5852
[2017-12-15 14:11:42] Epoch 0069 mean train/dev loss: 118.3203 / 113.7286
[2017-12-15 14:11:49] Epoch 0070 mean train/dev loss: 118.2721 / 113.6984
[2017-12-15 14:11:49] Checkpointing model at epoch 70 for ffn.hl_linear.lr_0.1.wd_0.001
[2017-12-15 14:11:49] Model Checkpointing finished.
[2017-12-15 14:11:56] Epoch 0071 mean train/dev loss: 118.3208 / 113.5252
[2017-12-15 14:12:03] Epoch 0072 mean train/dev loss: 118.3120 / 113.6470
[2017-12-15 14:12:09] Epoch 0073 mean train/dev loss: 118.3167 / 113.5388
[2017-12-15 14:12:17] Epoch 0074 mean train/dev loss: 118.2911 / 113.5855
[2017-12-15 14:12:23] Epoch 0075 mean train/dev loss: 118.2938 / 113.4385
[2017-12-15 14:12:23] Learning rate decayed by 0.5000
[2017-12-15 14:12:30] Epoch 0076 mean train/dev loss: 118.2917 / 113.5605
[2017-12-15 14:12:36] Epoch 0077 mean train/dev loss: 118.3280 / 113.6332
[2017-12-15 14:12:36] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:12:36] 
                       *** Training finished *** 
[2017-12-15 14:12:38] Dev MSE: 113.6332
[2017-12-15 14:12:44] Training MSE: 118.3080
[2017-12-15 14:12:45] Experiment ffn.hl_linear.lr_0.1.wd_0.001 logging ended.
