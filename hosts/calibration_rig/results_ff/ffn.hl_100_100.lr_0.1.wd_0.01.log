[2017-12-15 16:13:33] Experiment ffn.hl_100_100.lr_0.1.wd_0.01 logging started.
[2017-12-15 16:13:33] 
                       *** Starting Experiment ffn.hl_100_100.lr_0.1.wd_0.01 ***
                      
[2017-12-15 16:13:33] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100, 100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 16:13:33] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 100)
                        (relu2): ReLU ()
                        (linear3): Linear (100 -> 1)
                      )
[2017-12-15 16:13:33]  *** Training on GPU ***
[2017-12-15 16:13:41] Epoch 0001 mean train/dev loss: 3393.8345 / 218.7887
[2017-12-15 16:13:49] Epoch 0002 mean train/dev loss: 145.5445 / 971.7381
[2017-12-15 16:13:57] Epoch 0003 mean train/dev loss: 176.7607 / 235.9945
[2017-12-15 16:14:05] Epoch 0004 mean train/dev loss: 212.1023 / 197.4954
[2017-12-15 16:14:13] Epoch 0005 mean train/dev loss: 185.8844 / 218.3833
[2017-12-15 16:14:22] Epoch 0006 mean train/dev loss: 145.5611 / 203.7827
[2017-12-15 16:14:30] Epoch 0007 mean train/dev loss: 236.6374 / 210.5359
[2017-12-15 16:14:38] Epoch 0008 mean train/dev loss: 115.6905 / 277.9485
[2017-12-15 16:14:46] Epoch 0009 mean train/dev loss: 264.9028 / 144.9411
[2017-12-15 16:14:54] Epoch 0010 mean train/dev loss: 98.0937 / 145.1004
[2017-12-15 16:14:54] Checkpointing model at epoch 10 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:14:54] Model Checkpointing finished.
[2017-12-15 16:15:02] Epoch 0011 mean train/dev loss: 99.3688 / 350.1711
[2017-12-15 16:15:11] Epoch 0012 mean train/dev loss: 130.2126 / 198.8528
[2017-12-15 16:15:19] Epoch 0013 mean train/dev loss: 124.1495 / 366.3684
[2017-12-15 16:15:27] Epoch 0014 mean train/dev loss: 143.6810 / 134.9610
[2017-12-15 16:15:35] Epoch 0015 mean train/dev loss: 94.2986 / 132.6444
[2017-12-15 16:15:35] Learning rate decayed by 0.5000
[2017-12-15 16:15:44] Epoch 0016 mean train/dev loss: 78.9770 / 131.0810
[2017-12-15 16:15:52] Epoch 0017 mean train/dev loss: 80.1696 / 147.3309
[2017-12-15 16:16:00] Epoch 0018 mean train/dev loss: 81.0838 / 164.8001
[2017-12-15 16:16:08] Epoch 0019 mean train/dev loss: 83.0144 / 142.3887
[2017-12-15 16:16:16] Epoch 0020 mean train/dev loss: 91.6044 / 141.7807
[2017-12-15 16:16:16] Checkpointing model at epoch 20 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:16:16] Model Checkpointing finished.
[2017-12-15 16:16:24] Epoch 0021 mean train/dev loss: 89.4413 / 156.8488
[2017-12-15 16:16:32] Epoch 0022 mean train/dev loss: 81.3882 / 144.1431
[2017-12-15 16:16:40] Epoch 0023 mean train/dev loss: 78.8777 / 115.8212
[2017-12-15 16:16:48] Epoch 0024 mean train/dev loss: 78.7206 / 146.5320
[2017-12-15 16:16:56] Epoch 0025 mean train/dev loss: 80.5869 / 121.6750
[2017-12-15 16:17:04] Epoch 0026 mean train/dev loss: 81.2316 / 130.1969
[2017-12-15 16:17:12] Epoch 0027 mean train/dev loss: 77.7973 / 128.7662
[2017-12-15 16:17:20] Epoch 0028 mean train/dev loss: 76.5501 / 116.6925
[2017-12-15 16:17:28] Epoch 0029 mean train/dev loss: 76.8050 / 127.1093
[2017-12-15 16:17:35] Epoch 0030 mean train/dev loss: 73.8282 / 127.3228
[2017-12-15 16:17:35] Learning rate decayed by 0.5000
[2017-12-15 16:17:35] Checkpointing model at epoch 30 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:17:36] Model Checkpointing finished.
[2017-12-15 16:17:44] Epoch 0031 mean train/dev loss: 65.4576 / 126.0398
[2017-12-15 16:17:52] Epoch 0032 mean train/dev loss: 63.9711 / 123.7891
[2017-12-15 16:18:00] Epoch 0033 mean train/dev loss: 63.3452 / 105.4655
[2017-12-15 16:18:08] Epoch 0034 mean train/dev loss: 63.2338 / 127.2120
[2017-12-15 16:18:16] Epoch 0035 mean train/dev loss: 63.1478 / 139.1178
[2017-12-15 16:18:24] Epoch 0036 mean train/dev loss: 63.5350 / 92.8896
[2017-12-15 16:18:32] Epoch 0037 mean train/dev loss: 62.4914 / 117.8582
[2017-12-15 16:18:40] Epoch 0038 mean train/dev loss: 63.0836 / 101.0837
[2017-12-15 16:18:48] Epoch 0039 mean train/dev loss: 62.2662 / 109.7675
[2017-12-15 16:18:56] Epoch 0040 mean train/dev loss: 61.6584 / 106.5324
[2017-12-15 16:18:56] Checkpointing model at epoch 40 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:18:57] Model Checkpointing finished.
[2017-12-15 16:19:05] Epoch 0041 mean train/dev loss: 62.6880 / 108.8194
[2017-12-15 16:19:13] Epoch 0042 mean train/dev loss: 60.9573 / 94.4534
[2017-12-15 16:19:21] Epoch 0043 mean train/dev loss: 61.3908 / 91.2138
[2017-12-15 16:19:29] Epoch 0044 mean train/dev loss: 61.7474 / 97.0235
[2017-12-15 16:19:37] Epoch 0045 mean train/dev loss: 61.9198 / 105.4695
[2017-12-15 16:19:37] Learning rate decayed by 0.5000
[2017-12-15 16:19:45] Epoch 0046 mean train/dev loss: 58.3888 / 111.2423
[2017-12-15 16:19:53] Epoch 0047 mean train/dev loss: 58.3804 / 96.6908
[2017-12-15 16:20:01] Epoch 0048 mean train/dev loss: 58.1280 / 96.1800
[2017-12-15 16:20:09] Epoch 0049 mean train/dev loss: 58.3378 / 88.6572
[2017-12-15 16:20:17] Epoch 0050 mean train/dev loss: 58.3358 / 92.6870
[2017-12-15 16:20:17] Checkpointing model at epoch 50 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:20:18] Model Checkpointing finished.
[2017-12-15 16:20:26] Epoch 0051 mean train/dev loss: 58.0292 / 97.3641
[2017-12-15 16:20:34] Epoch 0052 mean train/dev loss: 57.7557 / 92.0286
[2017-12-15 16:20:42] Epoch 0053 mean train/dev loss: 57.3349 / 83.6473
[2017-12-15 16:20:50] Epoch 0054 mean train/dev loss: 57.0596 / 77.0077
[2017-12-15 16:20:58] Epoch 0055 mean train/dev loss: 57.0429 / 87.8608
[2017-12-15 16:21:06] Epoch 0056 mean train/dev loss: 56.7782 / 93.0165
[2017-12-15 16:21:15] Epoch 0057 mean train/dev loss: 56.9915 / 96.8947
[2017-12-15 16:21:22] Epoch 0058 mean train/dev loss: 56.6613 / 85.7200
[2017-12-15 16:21:30] Epoch 0059 mean train/dev loss: 56.4412 / 94.4069
[2017-12-15 16:21:38] Epoch 0060 mean train/dev loss: 56.5036 / 102.4801
[2017-12-15 16:21:38] Learning rate decayed by 0.5000
[2017-12-15 16:21:38] Checkpointing model at epoch 60 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:21:39] Model Checkpointing finished.
[2017-12-15 16:21:47] Epoch 0061 mean train/dev loss: 54.5407 / 96.5881
[2017-12-15 16:21:55] Epoch 0062 mean train/dev loss: 54.4475 / 102.2703
[2017-12-15 16:22:03] Epoch 0063 mean train/dev loss: 54.0808 / 88.5635
[2017-12-15 16:22:11] Epoch 0064 mean train/dev loss: 53.3662 / 88.3294
[2017-12-15 16:22:19] Epoch 0065 mean train/dev loss: 53.5898 / 89.0525
[2017-12-15 16:22:27] Epoch 0066 mean train/dev loss: 53.2333 / 85.0545
[2017-12-15 16:22:35] Epoch 0067 mean train/dev loss: 53.1402 / 93.8530
[2017-12-15 16:22:44] Epoch 0068 mean train/dev loss: 52.9393 / 95.3493
[2017-12-15 16:22:52] Epoch 0069 mean train/dev loss: 53.0711 / 88.0726
[2017-12-15 16:23:00] Epoch 0070 mean train/dev loss: 53.1110 / 94.4318
[2017-12-15 16:23:00] Checkpointing model at epoch 70 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:23:01] Model Checkpointing finished.
[2017-12-15 16:23:09] Epoch 0071 mean train/dev loss: 52.8635 / 95.6433
[2017-12-15 16:23:17] Epoch 0072 mean train/dev loss: 52.7255 / 99.6539
[2017-12-15 16:23:24] Epoch 0073 mean train/dev loss: 53.0720 / 100.7282
[2017-12-15 16:23:32] Epoch 0074 mean train/dev loss: 52.5639 / 83.1833
[2017-12-15 16:23:40] Epoch 0075 mean train/dev loss: 52.5859 / 101.6336
[2017-12-15 16:23:40] Learning rate decayed by 0.5000
[2017-12-15 16:23:48] Epoch 0076 mean train/dev loss: 51.6687 / 93.7737
[2017-12-15 16:23:56] Epoch 0077 mean train/dev loss: 51.4707 / 96.3233
[2017-12-15 16:24:05] Epoch 0078 mean train/dev loss: 51.6046 / 94.0930
[2017-12-15 16:24:13] Epoch 0079 mean train/dev loss: 51.4662 / 94.3247
[2017-12-15 16:24:21] Epoch 0080 mean train/dev loss: 51.5068 / 88.2966
[2017-12-15 16:24:21] Checkpointing model at epoch 80 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:24:21] Model Checkpointing finished.
[2017-12-15 16:24:29] Epoch 0081 mean train/dev loss: 51.5282 / 93.7902
[2017-12-15 16:24:37] Epoch 0082 mean train/dev loss: 51.3829 / 91.0970
[2017-12-15 16:24:45] Epoch 0083 mean train/dev loss: 51.4454 / 95.7655
[2017-12-15 16:24:53] Epoch 0084 mean train/dev loss: 51.3546 / 91.5292
[2017-12-15 16:25:01] Epoch 0085 mean train/dev loss: 51.3596 / 90.3867
[2017-12-15 16:25:09] Epoch 0086 mean train/dev loss: 51.2596 / 94.0899
[2017-12-15 16:25:17] Epoch 0087 mean train/dev loss: 51.3976 / 97.4047
[2017-12-15 16:25:25] Epoch 0088 mean train/dev loss: 51.2198 / 94.8398
[2017-12-15 16:25:33] Epoch 0089 mean train/dev loss: 51.1998 / 94.3168
[2017-12-15 16:25:41] Epoch 0090 mean train/dev loss: 51.2650 / 90.7360
[2017-12-15 16:25:41] Learning rate decayed by 0.5000
[2017-12-15 16:25:41] Checkpointing model at epoch 90 for ffn.hl_100_100.lr_0.1.wd_0.01
[2017-12-15 16:25:41] Model Checkpointing finished.
[2017-12-15 16:25:49] Epoch 0091 mean train/dev loss: 50.6896 / 91.5082
[2017-12-15 16:25:57] Epoch 0092 mean train/dev loss: 50.6606 / 97.5857
[2017-12-15 16:26:06] Epoch 0093 mean train/dev loss: 50.7021 / 94.4945
[2017-12-15 16:26:14] Epoch 0094 mean train/dev loss: 50.6655 / 91.6911
[2017-12-15 16:26:22] Epoch 0095 mean train/dev loss: 50.7077 / 90.2191
[2017-12-15 16:26:22] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 16:26:22] 
                       *** Training finished *** 
[2017-12-15 16:26:24] Dev MSE: 90.2191
[2017-12-15 16:26:31] Training MSE: 50.4577
[2017-12-15 16:26:35] Experiment ffn.hl_100_100.lr_0.1.wd_0.01 logging ended.
