[2017-12-15 14:56:58] Experiment ffn.hl_20_20_20.lr_0.1.wd_0.01 logging started.
[2017-12-15 14:56:58] 
                       *** Starting Experiment ffn.hl_20_20_20.lr_0.1.wd_0.01 ***
                      
[2017-12-15 14:56:58] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 14:56:58] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 20)
                        (relu3): ReLU ()
                        (linear4): Linear (20 -> 1)
                      )
[2017-12-15 14:56:58]  *** Training on GPU ***
[2017-12-15 14:57:06] Epoch 0001 mean train/dev loss: 4573.1309 / 192.5684
[2017-12-15 14:57:13] Epoch 0002 mean train/dev loss: 129.4414 / 153.7642
[2017-12-15 14:57:20] Epoch 0003 mean train/dev loss: 776.8616 / 219.1619
[2017-12-15 14:57:27] Epoch 0004 mean train/dev loss: 119.7551 / 143.5815
[2017-12-15 14:57:34] Epoch 0005 mean train/dev loss: 131.6446 / 129.8682
[2017-12-15 14:57:42] Epoch 0006 mean train/dev loss: 159.2670 / 272.7463
[2017-12-15 14:57:49] Epoch 0007 mean train/dev loss: 152.9029 / 197.1233
[2017-12-15 14:57:56] Epoch 0008 mean train/dev loss: 153.6632 / 269.6403
[2017-12-15 14:58:03] Epoch 0009 mean train/dev loss: 141.6163 / 160.1458
[2017-12-15 14:58:10] Epoch 0010 mean train/dev loss: 141.9212 / 164.2124
[2017-12-15 14:58:10] Checkpointing model at epoch 10 for ffn.hl_20_20_20.lr_0.1.wd_0.01
[2017-12-15 14:58:10] Model Checkpointing finished.
[2017-12-15 14:58:17] Epoch 0011 mean train/dev loss: 266.3323 / 1316.8967
[2017-12-15 14:58:25] Epoch 0012 mean train/dev loss: 185.6707 / 146.3799
[2017-12-15 14:58:33] Epoch 0013 mean train/dev loss: 105.3406 / 191.9371
[2017-12-15 14:58:40] Epoch 0014 mean train/dev loss: 121.0245 / 124.5368
[2017-12-15 14:58:48] Epoch 0015 mean train/dev loss: 108.1522 / 125.8513
[2017-12-15 14:58:48] Learning rate decayed by 0.5000
[2017-12-15 14:58:55] Epoch 0016 mean train/dev loss: 83.8806 / 110.6776
[2017-12-15 14:59:02] Epoch 0017 mean train/dev loss: 83.7337 / 103.1907
[2017-12-15 14:59:09] Epoch 0018 mean train/dev loss: 86.1517 / 122.2840
[2017-12-15 14:59:16] Epoch 0019 mean train/dev loss: 80.9410 / 115.7038
[2017-12-15 14:59:24] Epoch 0020 mean train/dev loss: 81.8635 / 96.7748
[2017-12-15 14:59:24] Checkpointing model at epoch 20 for ffn.hl_20_20_20.lr_0.1.wd_0.01
[2017-12-15 14:59:24] Model Checkpointing finished.
[2017-12-15 14:59:32] Epoch 0021 mean train/dev loss: 78.8262 / 87.6118
[2017-12-15 14:59:39] Epoch 0022 mean train/dev loss: 78.0961 / 91.2312
[2017-12-15 14:59:46] Epoch 0023 mean train/dev loss: 78.1855 / 93.4316
[2017-12-15 14:59:53] Epoch 0024 mean train/dev loss: 74.7616 / 87.7224
[2017-12-15 15:00:01] Epoch 0025 mean train/dev loss: 72.4436 / 82.3405
[2017-12-15 15:00:08] Epoch 0026 mean train/dev loss: 73.0121 / 118.7810
[2017-12-15 15:00:15] Epoch 0027 mean train/dev loss: 68.6277 / 75.6846
[2017-12-15 15:00:22] Epoch 0028 mean train/dev loss: 71.1433 / 121.1119
[2017-12-15 15:00:29] Epoch 0029 mean train/dev loss: 70.9249 / 126.5914
[2017-12-15 15:00:37] Epoch 0030 mean train/dev loss: 71.2166 / 88.7742
[2017-12-15 15:00:37] Learning rate decayed by 0.5000
[2017-12-15 15:00:37] Checkpointing model at epoch 30 for ffn.hl_20_20_20.lr_0.1.wd_0.01
[2017-12-15 15:00:37] Model Checkpointing finished.
[2017-12-15 15:00:44] Epoch 0031 mean train/dev loss: 61.5573 / 89.5928
[2017-12-15 15:00:51] Epoch 0032 mean train/dev loss: 61.8886 / 82.1294
[2017-12-15 15:00:58] Epoch 0033 mean train/dev loss: 61.1375 / 73.5942
[2017-12-15 15:01:05] Epoch 0034 mean train/dev loss: 60.9257 / 88.7148
[2017-12-15 15:01:13] Epoch 0035 mean train/dev loss: 59.4745 / 82.0228
[2017-12-15 15:01:21] Epoch 0036 mean train/dev loss: 58.8751 / 80.3137
[2017-12-15 15:01:28] Epoch 0037 mean train/dev loss: 58.0869 / 83.6625
[2017-12-15 15:01:36] Epoch 0038 mean train/dev loss: 57.4299 / 89.5591
[2017-12-15 15:01:43] Epoch 0039 mean train/dev loss: 56.8641 / 69.3077
[2017-12-15 15:01:50] Epoch 0040 mean train/dev loss: 57.0460 / 80.8331
[2017-12-15 15:01:50] Checkpointing model at epoch 40 for ffn.hl_20_20_20.lr_0.1.wd_0.01
[2017-12-15 15:01:50] Model Checkpointing finished.
[2017-12-15 15:01:57] Epoch 0041 mean train/dev loss: 56.2201 / 78.0770
[2017-12-15 15:02:04] Epoch 0042 mean train/dev loss: 56.0775 / 78.0629
[2017-12-15 15:02:12] Epoch 0043 mean train/dev loss: 57.4097 / 83.0432
[2017-12-15 15:02:19] Epoch 0044 mean train/dev loss: 56.0840 / 89.1368
[2017-12-15 15:02:26] Epoch 0045 mean train/dev loss: 56.7259 / 77.8578
[2017-12-15 15:02:26] Learning rate decayed by 0.5000
[2017-12-15 15:02:33] Epoch 0046 mean train/dev loss: 52.6230 / 81.1005
[2017-12-15 15:02:40] Epoch 0047 mean train/dev loss: 52.7937 / 86.4466
[2017-12-15 15:02:47] Epoch 0048 mean train/dev loss: 53.3651 / 82.8330
[2017-12-15 15:02:54] Epoch 0049 mean train/dev loss: 53.0773 / 81.0549
[2017-12-15 15:03:01] Epoch 0050 mean train/dev loss: 53.0662 / 79.6152
[2017-12-15 15:03:01] Checkpointing model at epoch 50 for ffn.hl_20_20_20.lr_0.1.wd_0.01
[2017-12-15 15:03:01] Model Checkpointing finished.
[2017-12-15 15:03:08] Epoch 0051 mean train/dev loss: 53.3576 / 76.5556
[2017-12-15 15:03:15] Epoch 0052 mean train/dev loss: 52.8777 / 84.8244
[2017-12-15 15:03:23] Epoch 0053 mean train/dev loss: 53.1525 / 80.2315
[2017-12-15 15:03:31] Epoch 0054 mean train/dev loss: 52.7952 / 82.6138
[2017-12-15 15:03:38] Epoch 0055 mean train/dev loss: 53.2241 / 78.8490
[2017-12-15 15:03:45] Epoch 0056 mean train/dev loss: 52.7085 / 76.6873
[2017-12-15 15:03:53] Epoch 0057 mean train/dev loss: 52.9809 / 83.9071
[2017-12-15 15:04:00] Epoch 0058 mean train/dev loss: 52.9511 / 83.6285
[2017-12-15 15:04:08] Epoch 0059 mean train/dev loss: 52.3874 / 76.7581
[2017-12-15 15:04:15] Epoch 0060 mean train/dev loss: 52.2419 / 80.4103
[2017-12-15 15:04:15] Learning rate decayed by 0.5000
[2017-12-15 15:04:15] Checkpointing model at epoch 60 for ffn.hl_20_20_20.lr_0.1.wd_0.01
[2017-12-15 15:04:15] Model Checkpointing finished.
[2017-12-15 15:04:22] Epoch 0061 mean train/dev loss: 50.4086 / 75.4951
[2017-12-15 15:04:30] Epoch 0062 mean train/dev loss: 50.4575 / 74.8974
[2017-12-15 15:04:37] Epoch 0063 mean train/dev loss: 50.5479 / 72.9867
[2017-12-15 15:04:45] Epoch 0064 mean train/dev loss: 50.3837 / 72.6653
[2017-12-15 15:04:53] Epoch 0065 mean train/dev loss: 50.4895 / 72.3243
[2017-12-15 15:05:00] Epoch 0066 mean train/dev loss: 50.2964 / 74.6401
[2017-12-15 15:05:07] Epoch 0067 mean train/dev loss: 50.4979 / 79.8121
[2017-12-15 15:05:14] Epoch 0068 mean train/dev loss: 50.3264 / 76.2770
[2017-12-15 15:05:21] Epoch 0069 mean train/dev loss: 50.3909 / 73.4370
[2017-12-15 15:05:29] Epoch 0070 mean train/dev loss: 50.1438 / 80.7412
[2017-12-15 15:05:29] Checkpointing model at epoch 70 for ffn.hl_20_20_20.lr_0.1.wd_0.01
[2017-12-15 15:05:29] Model Checkpointing finished.
[2017-12-15 15:05:36] Epoch 0071 mean train/dev loss: 50.3372 / 73.0901
[2017-12-15 15:05:43] Epoch 0072 mean train/dev loss: 50.2658 / 84.0234
[2017-12-15 15:05:51] Epoch 0073 mean train/dev loss: 50.3999 / 74.7993
[2017-12-15 15:05:59] Epoch 0074 mean train/dev loss: 50.0965 / 77.6352
[2017-12-15 15:06:06] Epoch 0075 mean train/dev loss: 50.2614 / 75.8888
[2017-12-15 15:06:06] Learning rate decayed by 0.5000
[2017-12-15 15:06:13] Epoch 0076 mean train/dev loss: 49.3102 / 73.5687
[2017-12-15 15:06:20] Epoch 0077 mean train/dev loss: 49.2136 / 74.4277
[2017-12-15 15:06:26] Epoch 0078 mean train/dev loss: 49.2998 / 75.2946
[2017-12-15 15:06:31] Epoch 0079 mean train/dev loss: 49.2543 / 73.7539
[2017-12-15 15:06:37] Epoch 0080 mean train/dev loss: 49.2768 / 80.7103
[2017-12-15 15:06:37] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:06:37] 
                       *** Training finished *** 
[2017-12-15 15:06:38] Dev MSE: 80.7103
[2017-12-15 15:06:43] Training MSE: 49.5484
[2017-12-15 15:06:45] Experiment ffn.hl_20_20_20.lr_0.1.wd_0.01 logging ended.
