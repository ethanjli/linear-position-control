[2017-12-15 18:07:54] Experiment ffn.hl_100.lr_0.01.wd_0.01 logging started.
[2017-12-15 18:07:54] 
                       *** Starting Experiment ffn.hl_100.lr_0.01.wd_0.01 ***
                      
[2017-12-15 18:07:54] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 18:07:54] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 1)
                      )
[2017-12-15 18:07:54]  *** Training on GPU ***
[2017-12-15 18:08:01] Epoch 0001 mean train/dev loss: 51522.7439 / 1234.0573
[2017-12-15 18:08:09] Epoch 0002 mean train/dev loss: 459.8623 / 485.0915
[2017-12-15 18:08:17] Epoch 0003 mean train/dev loss: 231.6024 / 242.7000
[2017-12-15 18:08:24] Epoch 0004 mean train/dev loss: 161.8098 / 193.7129
[2017-12-15 18:08:32] Epoch 0005 mean train/dev loss: 136.0480 / 166.3354
[2017-12-15 18:08:39] Epoch 0006 mean train/dev loss: 123.8872 / 142.6148
[2017-12-15 18:08:46] Epoch 0007 mean train/dev loss: 116.9237 / 130.6451
[2017-12-15 18:08:54] Epoch 0008 mean train/dev loss: 112.8961 / 129.4165
[2017-12-15 18:09:01] Epoch 0009 mean train/dev loss: 110.2323 / 123.7009
[2017-12-15 18:09:09] Epoch 0010 mean train/dev loss: 107.7045 / 123.7675
[2017-12-15 18:09:09] Checkpointing model at epoch 10 for ffn.hl_100.lr_0.01.wd_0.01
[2017-12-15 18:09:09] Model Checkpointing finished.
[2017-12-15 18:09:17] Epoch 0011 mean train/dev loss: 105.0172 / 113.4860
[2017-12-15 18:09:24] Epoch 0012 mean train/dev loss: 101.6187 / 128.2865
[2017-12-15 18:09:31] Epoch 0013 mean train/dev loss: 99.5242 / 113.4315
[2017-12-15 18:09:39] Epoch 0014 mean train/dev loss: 98.2002 / 114.4260
[2017-12-15 18:09:47] Epoch 0015 mean train/dev loss: 97.1290 / 115.5457
[2017-12-15 18:09:47] Learning rate decayed by 0.5000
[2017-12-15 18:09:54] Epoch 0016 mean train/dev loss: 94.8941 / 119.9226
[2017-12-15 18:10:02] Epoch 0017 mean train/dev loss: 94.3966 / 113.2988
[2017-12-15 18:10:09] Epoch 0018 mean train/dev loss: 93.7786 / 131.1766
[2017-12-15 18:10:16] Epoch 0019 mean train/dev loss: 93.0768 / 118.0202
[2017-12-15 18:10:24] Epoch 0020 mean train/dev loss: 92.3149 / 119.6728
[2017-12-15 18:10:24] Checkpointing model at epoch 20 for ffn.hl_100.lr_0.01.wd_0.01
[2017-12-15 18:10:24] Model Checkpointing finished.
[2017-12-15 18:10:31] Epoch 0021 mean train/dev loss: 91.7904 / 126.2805
[2017-12-15 18:10:39] Epoch 0022 mean train/dev loss: 91.0510 / 124.8185
[2017-12-15 18:10:47] Epoch 0023 mean train/dev loss: 90.5332 / 117.4598
[2017-12-15 18:10:54] Epoch 0024 mean train/dev loss: 89.8231 / 117.7999
[2017-12-15 18:11:01] Epoch 0025 mean train/dev loss: 89.2625 / 123.9930
[2017-12-15 18:11:08] Epoch 0026 mean train/dev loss: 88.6630 / 112.5236
[2017-12-15 18:11:15] Epoch 0027 mean train/dev loss: 88.1804 / 110.7431
[2017-12-15 18:11:23] Epoch 0028 mean train/dev loss: 87.8722 / 108.7196
[2017-12-15 18:11:30] Epoch 0029 mean train/dev loss: 87.1654 / 119.7894
[2017-12-15 18:11:38] Epoch 0030 mean train/dev loss: 86.8139 / 103.3310
[2017-12-15 18:11:38] Learning rate decayed by 0.5000
[2017-12-15 18:11:38] Checkpointing model at epoch 30 for ffn.hl_100.lr_0.01.wd_0.01
[2017-12-15 18:11:38] Model Checkpointing finished.
[2017-12-15 18:11:45] Epoch 0031 mean train/dev loss: 85.8659 / 110.3196
[2017-12-15 18:11:52] Epoch 0032 mean train/dev loss: 85.6994 / 109.9946
[2017-12-15 18:12:00] Epoch 0033 mean train/dev loss: 85.6019 / 111.6159
[2017-12-15 18:12:07] Epoch 0034 mean train/dev loss: 85.3722 / 107.7859
[2017-12-15 18:12:15] Epoch 0035 mean train/dev loss: 85.0681 / 112.4988
[2017-12-15 18:12:22] Epoch 0036 mean train/dev loss: 84.8072 / 112.5986
[2017-12-15 18:12:30] Epoch 0037 mean train/dev loss: 84.6964 / 110.5517
[2017-12-15 18:12:37] Epoch 0038 mean train/dev loss: 84.5083 / 107.8748
[2017-12-15 18:12:44] Epoch 0039 mean train/dev loss: 84.2147 / 108.7871
[2017-12-15 18:12:51] Epoch 0040 mean train/dev loss: 84.0561 / 110.4664
[2017-12-15 18:12:51] Checkpointing model at epoch 40 for ffn.hl_100.lr_0.01.wd_0.01
[2017-12-15 18:12:51] Model Checkpointing finished.
[2017-12-15 18:12:59] Epoch 0041 mean train/dev loss: 83.8935 / 112.7276
[2017-12-15 18:13:07] Epoch 0042 mean train/dev loss: 83.7948 / 112.0853
[2017-12-15 18:13:14] Epoch 0043 mean train/dev loss: 83.5255 / 109.5551
[2017-12-15 18:13:22] Epoch 0044 mean train/dev loss: 83.3618 / 115.3989
[2017-12-15 18:13:29] Epoch 0045 mean train/dev loss: 83.2587 / 114.5618
[2017-12-15 18:13:29] Learning rate decayed by 0.5000
[2017-12-15 18:13:36] Epoch 0046 mean train/dev loss: 82.8063 / 112.5138
[2017-12-15 18:13:44] Epoch 0047 mean train/dev loss: 82.7177 / 110.7331
[2017-12-15 18:13:52] Epoch 0048 mean train/dev loss: 82.6622 / 110.9506
[2017-12-15 18:13:59] Epoch 0049 mean train/dev loss: 82.5720 / 115.1260
[2017-12-15 18:14:06] Epoch 0050 mean train/dev loss: 82.5179 / 113.7657
[2017-12-15 18:14:06] Checkpointing model at epoch 50 for ffn.hl_100.lr_0.01.wd_0.01
[2017-12-15 18:14:06] Model Checkpointing finished.
[2017-12-15 18:14:14] Epoch 0051 mean train/dev loss: 82.4221 / 113.3699
[2017-12-15 18:14:20] Epoch 0052 mean train/dev loss: 82.3533 / 113.4209
[2017-12-15 18:14:26] Epoch 0053 mean train/dev loss: 82.2485 / 119.1895
[2017-12-15 18:14:32] Epoch 0054 mean train/dev loss: 82.1748 / 116.2977
[2017-12-15 18:14:39] Epoch 0055 mean train/dev loss: 82.1957 / 117.8119
[2017-12-15 18:14:45] Epoch 0056 mean train/dev loss: 82.0797 / 115.2564
[2017-12-15 18:14:51] Epoch 0057 mean train/dev loss: 82.0163 / 114.6627
[2017-12-15 18:14:57] Epoch 0058 mean train/dev loss: 81.9950 / 114.1501
[2017-12-15 18:15:04] Epoch 0059 mean train/dev loss: 81.8836 / 116.7071
[2017-12-15 18:15:10] Epoch 0060 mean train/dev loss: 81.8053 / 115.0923
[2017-12-15 18:15:10] Learning rate decayed by 0.5000
[2017-12-15 18:15:10] Checkpointing model at epoch 60 for ffn.hl_100.lr_0.01.wd_0.01
[2017-12-15 18:15:11] Model Checkpointing finished.
[2017-12-15 18:15:17] Epoch 0061 mean train/dev loss: 81.6057 / 118.7421
[2017-12-15 18:15:23] Epoch 0062 mean train/dev loss: 81.5640 / 116.6744
[2017-12-15 18:15:28] Epoch 0063 mean train/dev loss: 81.5527 / 115.3851
[2017-12-15 18:15:34] Epoch 0064 mean train/dev loss: 81.5157 / 115.2943
[2017-12-15 18:15:41] Epoch 0065 mean train/dev loss: 81.4888 / 112.0528
[2017-12-15 18:15:47] Epoch 0066 mean train/dev loss: 81.4979 / 114.3108
[2017-12-15 18:15:53] Epoch 0067 mean train/dev loss: 81.4522 / 114.7957
[2017-12-15 18:15:59] Epoch 0068 mean train/dev loss: 81.4071 / 113.1969
[2017-12-15 18:16:05] Epoch 0069 mean train/dev loss: 81.3728 / 114.2132
[2017-12-15 18:16:11] Epoch 0070 mean train/dev loss: 81.2938 / 113.1220
[2017-12-15 18:16:11] Checkpointing model at epoch 70 for ffn.hl_100.lr_0.01.wd_0.01
[2017-12-15 18:16:12] Model Checkpointing finished.
[2017-12-15 18:16:18] Epoch 0071 mean train/dev loss: 81.3108 / 116.9438
[2017-12-15 18:16:18] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:16:18] 
                       *** Training finished *** 
[2017-12-15 18:16:19] Dev MSE: 116.9438
[2017-12-15 18:16:25] Training MSE: 81.2266
[2017-12-15 18:16:26] Experiment ffn.hl_100.lr_0.01.wd_0.01 logging ended.
