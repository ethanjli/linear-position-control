[2017-12-15 15:38:11] Experiment ffn.hl_50_50_50.lr_0.1.wd_0.1 logging started.
[2017-12-15 15:38:11] 
                       *** Starting Experiment ffn.hl_50_50_50.lr_0.1.wd_0.1 ***
                      
[2017-12-15 15:38:11] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 15:38:11] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 50)
                        (relu3): ReLU ()
                        (linear4): Linear (50 -> 1)
                      )
[2017-12-15 15:38:11]  *** Training on GPU ***
[2017-12-15 15:38:20] Epoch 0001 mean train/dev loss: 4333.7133 / 199.9565
[2017-12-15 15:38:28] Epoch 0002 mean train/dev loss: 187.7076 / 280.4582
[2017-12-15 15:38:37] Epoch 0003 mean train/dev loss: 14693.7242 / 774.2950
[2017-12-15 15:38:45] Epoch 0004 mean train/dev loss: 331.3510 / 392.7287
[2017-12-15 15:38:53] Epoch 0005 mean train/dev loss: 152.4000 / 131.6994
[2017-12-15 15:39:02] Epoch 0006 mean train/dev loss: 149.4688 / 140.2147
[2017-12-15 15:39:09] Epoch 0007 mean train/dev loss: 133.0888 / 123.5893
[2017-12-15 15:39:17] Epoch 0008 mean train/dev loss: 130.9765 / 131.6873
[2017-12-15 15:39:25] Epoch 0009 mean train/dev loss: 232.1275 / 114.7692
[2017-12-15 15:39:34] Epoch 0010 mean train/dev loss: 114.8564 / 159.3415
[2017-12-15 15:39:34] Checkpointing model at epoch 10 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:39:34] Model Checkpointing finished.
[2017-12-15 15:39:42] Epoch 0011 mean train/dev loss: 268.4840 / 136.5476
[2017-12-15 15:39:51] Epoch 0012 mean train/dev loss: 127.4149 / 121.1686
[2017-12-15 15:40:00] Epoch 0013 mean train/dev loss: 230.4088 / 123.5530
[2017-12-15 15:40:08] Epoch 0014 mean train/dev loss: 115.5421 / 202.5401
[2017-12-15 15:40:17] Epoch 0015 mean train/dev loss: 291.6780 / 125.5107
[2017-12-15 15:40:17] Learning rate decayed by 0.5000
[2017-12-15 15:40:25] Epoch 0016 mean train/dev loss: 104.5443 / 117.0574
[2017-12-15 15:40:33] Epoch 0017 mean train/dev loss: 104.2444 / 150.0840
[2017-12-15 15:40:42] Epoch 0018 mean train/dev loss: 104.0190 / 115.4906
[2017-12-15 15:40:50] Epoch 0019 mean train/dev loss: 109.1033 / 205.0300
[2017-12-15 15:40:59] Epoch 0020 mean train/dev loss: 113.3214 / 113.3637
[2017-12-15 15:40:59] Checkpointing model at epoch 20 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:40:59] Model Checkpointing finished.
[2017-12-15 15:41:07] Epoch 0021 mean train/dev loss: 110.9264 / 152.3509
[2017-12-15 15:41:15] Epoch 0022 mean train/dev loss: 106.7915 / 127.8056
[2017-12-15 15:41:24] Epoch 0023 mean train/dev loss: 98.9210 / 124.6754
[2017-12-15 15:41:32] Epoch 0024 mean train/dev loss: 99.6795 / 164.5353
[2017-12-15 15:41:40] Epoch 0025 mean train/dev loss: 106.8877 / 146.7125
[2017-12-15 15:41:48] Epoch 0026 mean train/dev loss: 99.7641 / 130.6815
[2017-12-15 15:41:57] Epoch 0027 mean train/dev loss: 99.6575 / 182.2151
[2017-12-15 15:42:05] Epoch 0028 mean train/dev loss: 107.3984 / 236.2722
[2017-12-15 15:42:13] Epoch 0029 mean train/dev loss: 98.7034 / 104.1493
[2017-12-15 15:42:22] Epoch 0030 mean train/dev loss: 98.2339 / 131.5939
[2017-12-15 15:42:22] Learning rate decayed by 0.5000
[2017-12-15 15:42:22] Checkpointing model at epoch 30 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:42:22] Model Checkpointing finished.
[2017-12-15 15:42:31] Epoch 0031 mean train/dev loss: 86.0779 / 133.9773
[2017-12-15 15:42:39] Epoch 0032 mean train/dev loss: 87.0956 / 118.8535
[2017-12-15 15:42:47] Epoch 0033 mean train/dev loss: 88.5942 / 127.3068
[2017-12-15 15:42:56] Epoch 0034 mean train/dev loss: 85.9639 / 132.1378
[2017-12-15 15:43:04] Epoch 0035 mean train/dev loss: 86.2212 / 114.8968
[2017-12-15 15:43:12] Epoch 0036 mean train/dev loss: 85.7184 / 138.6544
[2017-12-15 15:43:21] Epoch 0037 mean train/dev loss: 84.1098 / 163.1357
[2017-12-15 15:43:29] Epoch 0038 mean train/dev loss: 84.0837 / 113.7256
[2017-12-15 15:43:37] Epoch 0039 mean train/dev loss: 82.7282 / 163.0728
[2017-12-15 15:43:46] Epoch 0040 mean train/dev loss: 82.8870 / 157.1641
[2017-12-15 15:43:46] Checkpointing model at epoch 40 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:43:46] Model Checkpointing finished.
[2017-12-15 15:43:54] Epoch 0041 mean train/dev loss: 82.4204 / 135.2655
[2017-12-15 15:44:02] Epoch 0042 mean train/dev loss: 83.1878 / 152.1703
[2017-12-15 15:44:11] Epoch 0043 mean train/dev loss: 84.2757 / 133.6129
[2017-12-15 15:44:19] Epoch 0044 mean train/dev loss: 82.4579 / 98.1132
[2017-12-15 15:44:27] Epoch 0045 mean train/dev loss: 81.0572 / 116.3770
[2017-12-15 15:44:27] Learning rate decayed by 0.5000
[2017-12-15 15:44:36] Epoch 0046 mean train/dev loss: 76.6017 / 134.6695
[2017-12-15 15:44:44] Epoch 0047 mean train/dev loss: 75.7627 / 133.2533
[2017-12-15 15:44:52] Epoch 0048 mean train/dev loss: 75.4182 / 116.9997
[2017-12-15 15:45:01] Epoch 0049 mean train/dev loss: 75.0879 / 118.6572
[2017-12-15 15:45:09] Epoch 0050 mean train/dev loss: 74.6980 / 124.3590
[2017-12-15 15:45:09] Checkpointing model at epoch 50 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:45:09] Model Checkpointing finished.
[2017-12-15 15:45:18] Epoch 0051 mean train/dev loss: 74.2718 / 168.2749
[2017-12-15 15:45:26] Epoch 0052 mean train/dev loss: 73.7612 / 123.9591
[2017-12-15 15:45:35] Epoch 0053 mean train/dev loss: 73.3352 / 105.2019
[2017-12-15 15:45:43] Epoch 0054 mean train/dev loss: 73.6604 / 158.7357
[2017-12-15 15:45:51] Epoch 0055 mean train/dev loss: 72.7110 / 111.5499
[2017-12-15 15:46:00] Epoch 0056 mean train/dev loss: 72.2166 / 106.7422
[2017-12-15 15:46:08] Epoch 0057 mean train/dev loss: 72.4395 / 113.4409
[2017-12-15 15:46:17] Epoch 0058 mean train/dev loss: 72.4860 / 99.0326
[2017-12-15 15:46:25] Epoch 0059 mean train/dev loss: 71.2347 / 98.3754
[2017-12-15 15:46:33] Epoch 0060 mean train/dev loss: 71.0100 / 112.2071
[2017-12-15 15:46:33] Learning rate decayed by 0.5000
[2017-12-15 15:46:33] Checkpointing model at epoch 60 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:46:34] Model Checkpointing finished.
[2017-12-15 15:46:43] Epoch 0061 mean train/dev loss: 67.5334 / 102.2309
[2017-12-15 15:46:51] Epoch 0062 mean train/dev loss: 67.3448 / 98.6655
[2017-12-15 15:46:59] Epoch 0063 mean train/dev loss: 66.7790 / 110.4955
[2017-12-15 15:47:07] Epoch 0064 mean train/dev loss: 66.1950 / 95.6174
[2017-12-15 15:47:16] Epoch 0065 mean train/dev loss: 66.0669 / 85.5715
[2017-12-15 15:47:24] Epoch 0066 mean train/dev loss: 65.5624 / 98.9957
[2017-12-15 15:47:32] Epoch 0067 mean train/dev loss: 64.7999 / 96.8772
[2017-12-15 15:47:41] Epoch 0068 mean train/dev loss: 64.2233 / 102.3242
[2017-12-15 15:47:49] Epoch 0069 mean train/dev loss: 63.4813 / 96.8974
[2017-12-15 15:47:58] Epoch 0070 mean train/dev loss: 63.2701 / 88.3105
[2017-12-15 15:47:58] Checkpointing model at epoch 70 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:47:58] Model Checkpointing finished.
[2017-12-15 15:48:06] Epoch 0071 mean train/dev loss: 62.9664 / 119.2794
[2017-12-15 15:48:14] Epoch 0072 mean train/dev loss: 62.2410 / 94.0989
[2017-12-15 15:48:20] Epoch 0073 mean train/dev loss: 62.2343 / 89.1931
[2017-12-15 15:48:28] Epoch 0074 mean train/dev loss: 61.8478 / 86.7395
[2017-12-15 15:48:35] Epoch 0075 mean train/dev loss: 61.8128 / 119.1707
[2017-12-15 15:48:35] Learning rate decayed by 0.5000
[2017-12-15 15:48:42] Epoch 0076 mean train/dev loss: 60.0437 / 92.3308
[2017-12-15 15:48:49] Epoch 0077 mean train/dev loss: 59.8752 / 90.9299
[2017-12-15 15:48:56] Epoch 0078 mean train/dev loss: 59.8596 / 94.7728
[2017-12-15 15:49:03] Epoch 0079 mean train/dev loss: 59.8152 / 93.8036
[2017-12-15 15:49:10] Epoch 0080 mean train/dev loss: 59.7582 / 92.3227
[2017-12-15 15:49:10] Checkpointing model at epoch 80 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:49:10] Model Checkpointing finished.
[2017-12-15 15:49:17] Epoch 0081 mean train/dev loss: 59.8259 / 100.7381
[2017-12-15 15:49:25] Epoch 0082 mean train/dev loss: 59.6444 / 94.5706
[2017-12-15 15:49:32] Epoch 0083 mean train/dev loss: 59.4845 / 95.6786
[2017-12-15 15:49:40] Epoch 0084 mean train/dev loss: 59.5142 / 87.4122
[2017-12-15 15:49:46] Epoch 0085 mean train/dev loss: 59.2360 / 85.7579
[2017-12-15 15:49:54] Epoch 0086 mean train/dev loss: 59.4447 / 90.2151
[2017-12-15 15:50:01] Epoch 0087 mean train/dev loss: 59.3664 / 79.5889
[2017-12-15 15:50:08] Epoch 0088 mean train/dev loss: 59.3059 / 86.9160
[2017-12-15 15:50:15] Epoch 0089 mean train/dev loss: 59.0903 / 82.3566
[2017-12-15 15:50:23] Epoch 0090 mean train/dev loss: 59.2135 / 95.7689
[2017-12-15 15:50:23] Learning rate decayed by 0.5000
[2017-12-15 15:50:23] Checkpointing model at epoch 90 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:50:23] Model Checkpointing finished.
[2017-12-15 15:50:30] Epoch 0091 mean train/dev loss: 58.3105 / 88.6074
[2017-12-15 15:50:37] Epoch 0092 mean train/dev loss: 58.1956 / 88.0524
[2017-12-15 15:50:44] Epoch 0093 mean train/dev loss: 58.2099 / 84.0773
[2017-12-15 15:50:51] Epoch 0094 mean train/dev loss: 58.1362 / 92.4802
[2017-12-15 15:50:58] Epoch 0095 mean train/dev loss: 58.1112 / 87.2490
[2017-12-15 15:51:05] Epoch 0096 mean train/dev loss: 58.1746 / 86.2918
[2017-12-15 15:51:13] Epoch 0097 mean train/dev loss: 58.0118 / 89.0996
[2017-12-15 15:51:20] Epoch 0098 mean train/dev loss: 58.0518 / 88.5764
[2017-12-15 15:51:28] Epoch 0099 mean train/dev loss: 57.9913 / 89.8485
[2017-12-15 15:51:35] Epoch 0100 mean train/dev loss: 57.9328 / 84.9395
[2017-12-15 15:51:35] Checkpointing model at epoch 100 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:51:35] Model Checkpointing finished.
[2017-12-15 15:51:41] Epoch 0101 mean train/dev loss: 58.0073 / 85.5526
[2017-12-15 15:51:49] Epoch 0102 mean train/dev loss: 57.8799 / 82.7730
[2017-12-15 15:51:55] Epoch 0103 mean train/dev loss: 57.8070 / 86.9280
[2017-12-15 15:52:02] Epoch 0104 mean train/dev loss: 57.7543 / 94.0798
[2017-12-15 15:52:09] Epoch 0105 mean train/dev loss: 57.7076 / 82.7929
[2017-12-15 15:52:09] Learning rate decayed by 0.5000
[2017-12-15 15:52:17] Epoch 0106 mean train/dev loss: 57.2330 / 89.1743
[2017-12-15 15:52:24] Epoch 0107 mean train/dev loss: 57.1913 / 83.5917
[2017-12-15 15:52:31] Epoch 0108 mean train/dev loss: 57.1408 / 84.6349
[2017-12-15 15:52:38] Epoch 0109 mean train/dev loss: 57.1152 / 88.8561
[2017-12-15 15:52:46] Epoch 0110 mean train/dev loss: 57.0738 / 87.3332
[2017-12-15 15:52:46] Checkpointing model at epoch 110 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:52:46] Model Checkpointing finished.
[2017-12-15 15:52:53] Epoch 0111 mean train/dev loss: 57.1016 / 82.3103
[2017-12-15 15:53:00] Epoch 0112 mean train/dev loss: 57.0350 / 85.5285
[2017-12-15 15:53:08] Epoch 0113 mean train/dev loss: 57.0332 / 85.9495
[2017-12-15 15:53:15] Epoch 0114 mean train/dev loss: 57.0024 / 88.3641
[2017-12-15 15:53:22] Epoch 0115 mean train/dev loss: 56.9936 / 87.4955
[2017-12-15 15:53:29] Epoch 0116 mean train/dev loss: 56.9543 / 85.7628
[2017-12-15 15:53:37] Epoch 0117 mean train/dev loss: 56.9589 / 90.8495
[2017-12-15 15:53:44] Epoch 0118 mean train/dev loss: 56.9325 / 89.0454
[2017-12-15 15:53:50] Epoch 0119 mean train/dev loss: 56.8818 / 86.0166
[2017-12-15 15:53:55] Epoch 0120 mean train/dev loss: 56.7795 / 89.9085
[2017-12-15 15:53:55] Learning rate decayed by 0.5000
[2017-12-15 15:53:55] Checkpointing model at epoch 120 for ffn.hl_50_50_50.lr_0.1.wd_0.1
[2017-12-15 15:53:56] Model Checkpointing finished.
[2017-12-15 15:54:01] Epoch 0121 mean train/dev loss: 56.5086 / 83.9903
[2017-12-15 15:54:07] Epoch 0122 mean train/dev loss: 56.4698 / 86.3937
[2017-12-15 15:54:13] Epoch 0123 mean train/dev loss: 56.5171 / 84.8477
[2017-12-15 15:54:18] Epoch 0124 mean train/dev loss: 56.4267 / 89.0585
[2017-12-15 15:54:24] Epoch 0125 mean train/dev loss: 56.4057 / 87.1593
[2017-12-15 15:54:30] Epoch 0126 mean train/dev loss: 56.3645 / 84.9548
[2017-12-15 15:54:35] Epoch 0127 mean train/dev loss: 56.3478 / 88.9593
[2017-12-15 15:54:41] Epoch 0128 mean train/dev loss: 56.3285 / 85.0732
[2017-12-15 15:54:41] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:54:41] 
                       *** Training finished *** 
[2017-12-15 15:54:42] Dev MSE: 85.0732
[2017-12-15 15:54:47] Training MSE: 56.2823
[2017-12-15 15:54:49] Experiment ffn.hl_50_50_50.lr_0.1.wd_0.1 logging ended.
