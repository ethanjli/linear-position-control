[2017-12-15 18:20:00] Experiment ffn.hl_100_100.lr_0.01.wd_0.001 logging started.
[2017-12-15 18:20:00] 
                       *** Starting Experiment ffn.hl_100_100.lr_0.01.wd_0.001 ***
                      
[2017-12-15 18:20:00] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100, 100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 18:20:00] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 100)
                        (relu2): ReLU ()
                        (linear3): Linear (100 -> 1)
                      )
[2017-12-15 18:20:00]  *** Training on GPU ***
[2017-12-15 18:20:08] Epoch 0001 mean train/dev loss: 14325.9133 / 210.3727
[2017-12-15 18:20:16] Epoch 0002 mean train/dev loss: 124.1144 / 148.2402
[2017-12-15 18:20:24] Epoch 0003 mean train/dev loss: 113.0631 / 127.1274
[2017-12-15 18:20:32] Epoch 0004 mean train/dev loss: 110.1841 / 144.6244
[2017-12-15 18:20:41] Epoch 0005 mean train/dev loss: 109.0456 / 111.9308
[2017-12-15 18:20:49] Epoch 0006 mean train/dev loss: 105.2281 / 128.4073
[2017-12-15 18:20:57] Epoch 0007 mean train/dev loss: 105.5955 / 122.7972
[2017-12-15 18:21:05] Epoch 0008 mean train/dev loss: 101.1786 / 119.5576
[2017-12-15 18:21:13] Epoch 0009 mean train/dev loss: 101.3764 / 224.4883
[2017-12-15 18:21:21] Epoch 0010 mean train/dev loss: 95.0355 / 126.1090
[2017-12-15 18:21:21] Checkpointing model at epoch 10 for ffn.hl_100_100.lr_0.01.wd_0.001
[2017-12-15 18:21:22] Model Checkpointing finished.
[2017-12-15 18:21:30] Epoch 0011 mean train/dev loss: 97.2689 / 162.6546
[2017-12-15 18:21:38] Epoch 0012 mean train/dev loss: 89.9645 / 119.1634
[2017-12-15 18:21:46] Epoch 0013 mean train/dev loss: 87.9075 / 182.2069
[2017-12-15 18:21:54] Epoch 0014 mean train/dev loss: 87.3500 / 123.9265
[2017-12-15 18:22:02] Epoch 0015 mean train/dev loss: 84.8565 / 113.0867
[2017-12-15 18:22:02] Learning rate decayed by 0.5000
[2017-12-15 18:22:10] Epoch 0016 mean train/dev loss: 76.7566 / 122.2967
[2017-12-15 18:22:18] Epoch 0017 mean train/dev loss: 75.5802 / 116.8634
[2017-12-15 18:22:27] Epoch 0018 mean train/dev loss: 74.9148 / 121.5774
[2017-12-15 18:22:35] Epoch 0019 mean train/dev loss: 75.5002 / 114.6725
[2017-12-15 18:22:42] Epoch 0020 mean train/dev loss: 75.0233 / 127.2644
[2017-12-15 18:22:42] Checkpointing model at epoch 20 for ffn.hl_100_100.lr_0.01.wd_0.001
[2017-12-15 18:22:43] Model Checkpointing finished.
[2017-12-15 18:22:51] Epoch 0021 mean train/dev loss: 74.6157 / 121.8511
[2017-12-15 18:22:59] Epoch 0022 mean train/dev loss: 74.5597 / 116.4260
[2017-12-15 18:23:07] Epoch 0023 mean train/dev loss: 74.3067 / 114.2768
[2017-12-15 18:23:16] Epoch 0024 mean train/dev loss: 75.1056 / 120.9898
[2017-12-15 18:23:23] Epoch 0025 mean train/dev loss: 73.0358 / 110.5435
[2017-12-15 18:23:31] Epoch 0026 mean train/dev loss: 72.7503 / 93.0534
[2017-12-15 18:23:39] Epoch 0027 mean train/dev loss: 72.4491 / 84.6874
[2017-12-15 18:23:47] Epoch 0028 mean train/dev loss: 71.7343 / 137.5896
[2017-12-15 18:23:55] Epoch 0029 mean train/dev loss: 70.9942 / 106.1905
[2017-12-15 18:24:03] Epoch 0030 mean train/dev loss: 71.3663 / 100.5267
[2017-12-15 18:24:03] Learning rate decayed by 0.5000
[2017-12-15 18:24:03] Checkpointing model at epoch 30 for ffn.hl_100_100.lr_0.01.wd_0.001
[2017-12-15 18:24:04] Model Checkpointing finished.
[2017-12-15 18:24:12] Epoch 0031 mean train/dev loss: 67.6518 / 98.9866
[2017-12-15 18:24:20] Epoch 0032 mean train/dev loss: 67.8840 / 100.2119
[2017-12-15 18:24:28] Epoch 0033 mean train/dev loss: 67.7926 / 105.3107
[2017-12-15 18:24:36] Epoch 0034 mean train/dev loss: 67.7465 / 90.4021
[2017-12-15 18:24:44] Epoch 0035 mean train/dev loss: 67.1192 / 91.5793
[2017-12-15 18:24:52] Epoch 0036 mean train/dev loss: 67.1815 / 92.4980
[2017-12-15 18:25:00] Epoch 0037 mean train/dev loss: 67.2997 / 96.6036
[2017-12-15 18:25:08] Epoch 0038 mean train/dev loss: 66.8353 / 109.1409
[2017-12-15 18:25:16] Epoch 0039 mean train/dev loss: 66.5838 / 91.0995
[2017-12-15 18:25:24] Epoch 0040 mean train/dev loss: 66.6966 / 107.6793
[2017-12-15 18:25:24] Checkpointing model at epoch 40 for ffn.hl_100_100.lr_0.01.wd_0.001
[2017-12-15 18:25:25] Model Checkpointing finished.
[2017-12-15 18:25:33] Epoch 0041 mean train/dev loss: 66.4332 / 116.2023
[2017-12-15 18:25:41] Epoch 0042 mean train/dev loss: 66.0545 / 101.4762
[2017-12-15 18:25:49] Epoch 0043 mean train/dev loss: 66.2589 / 108.8174
[2017-12-15 18:25:57] Epoch 0044 mean train/dev loss: 65.7839 / 98.2496
[2017-12-15 18:26:05] Epoch 0045 mean train/dev loss: 65.0024 / 108.3526
[2017-12-15 18:26:05] Learning rate decayed by 0.5000
[2017-12-15 18:26:13] Epoch 0046 mean train/dev loss: 63.5004 / 100.6801
[2017-12-15 18:26:21] Epoch 0047 mean train/dev loss: 63.4891 / 93.6866
[2017-12-15 18:26:29] Epoch 0048 mean train/dev loss: 63.3175 / 95.7786
[2017-12-15 18:26:37] Epoch 0049 mean train/dev loss: 63.2311 / 89.9780
[2017-12-15 18:26:45] Epoch 0050 mean train/dev loss: 63.2598 / 100.2260
[2017-12-15 18:26:45] Checkpointing model at epoch 50 for ffn.hl_100_100.lr_0.01.wd_0.001
[2017-12-15 18:26:45] Model Checkpointing finished.
[2017-12-15 18:26:54] Epoch 0051 mean train/dev loss: 63.0205 / 94.1904
[2017-12-15 18:27:01] Epoch 0052 mean train/dev loss: 62.6534 / 97.6748
[2017-12-15 18:27:09] Epoch 0053 mean train/dev loss: 62.6031 / 101.2574
[2017-12-15 18:27:17] Epoch 0054 mean train/dev loss: 62.5635 / 100.3255
[2017-12-15 18:27:25] Epoch 0055 mean train/dev loss: 62.2379 / 99.1708
[2017-12-15 18:27:33] Epoch 0056 mean train/dev loss: 62.0476 / 103.3650
[2017-12-15 18:27:41] Epoch 0057 mean train/dev loss: 61.8341 / 98.4137
[2017-12-15 18:27:49] Epoch 0058 mean train/dev loss: 61.7182 / 99.6467
[2017-12-15 18:27:57] Epoch 0059 mean train/dev loss: 61.4593 / 99.8985
[2017-12-15 18:28:05] Epoch 0060 mean train/dev loss: 61.3961 / 97.2632
[2017-12-15 18:28:05] Learning rate decayed by 0.5000
[2017-12-15 18:28:05] Checkpointing model at epoch 60 for ffn.hl_100_100.lr_0.01.wd_0.001
[2017-12-15 18:28:05] Model Checkpointing finished.
[2017-12-15 18:28:13] Epoch 0061 mean train/dev loss: 60.3502 / 97.0745
[2017-12-15 18:28:22] Epoch 0062 mean train/dev loss: 60.2523 / 101.9519
[2017-12-15 18:28:30] Epoch 0063 mean train/dev loss: 60.1713 / 94.6642
[2017-12-15 18:28:38] Epoch 0064 mean train/dev loss: 60.0187 / 99.0202
[2017-12-15 18:28:45] Epoch 0065 mean train/dev loss: 59.9280 / 96.7658
[2017-12-15 18:28:54] Epoch 0066 mean train/dev loss: 59.8540 / 99.4166
[2017-12-15 18:29:02] Epoch 0067 mean train/dev loss: 59.7056 / 100.6469
[2017-12-15 18:29:10] Epoch 0068 mean train/dev loss: 59.5956 / 103.2465
[2017-12-15 18:29:10] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:29:10] 
                       *** Training finished *** 
[2017-12-15 18:29:11] Dev MSE: 103.2465
[2017-12-15 18:29:19] Training MSE: 59.6685
[2017-12-15 18:29:21] Experiment ffn.hl_100_100.lr_0.01.wd_0.001 logging ended.
