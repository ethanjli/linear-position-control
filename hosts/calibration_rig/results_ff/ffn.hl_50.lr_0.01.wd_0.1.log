[2017-12-15 17:28:30] Experiment ffn.hl_50.lr_0.01.wd_0.1 logging started.
[2017-12-15 17:28:30] 
                       *** Starting Experiment ffn.hl_50.lr_0.01.wd_0.1 ***
                      
[2017-12-15 17:28:30] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 17:28:30] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 1)
                      )
[2017-12-15 17:28:30]  *** Training on GPU ***
[2017-12-15 17:28:38] Epoch 0001 mean train/dev loss: 69886.6202 / 1974.0543
[2017-12-15 17:28:46] Epoch 0002 mean train/dev loss: 623.1748 / 926.2704
[2017-12-15 17:28:53] Epoch 0003 mean train/dev loss: 312.0245 / 489.4884
[2017-12-15 17:29:01] Epoch 0004 mean train/dev loss: 208.5595 / 371.6555
[2017-12-15 17:29:09] Epoch 0005 mean train/dev loss: 163.1993 / 281.7887
[2017-12-15 17:29:16] Epoch 0006 mean train/dev loss: 142.0946 / 247.0737
[2017-12-15 17:29:24] Epoch 0007 mean train/dev loss: 131.0560 / 208.8790
[2017-12-15 17:29:32] Epoch 0008 mean train/dev loss: 123.3764 / 176.2091
[2017-12-15 17:29:39] Epoch 0009 mean train/dev loss: 118.2020 / 156.6973
[2017-12-15 17:29:47] Epoch 0010 mean train/dev loss: 115.1295 / 150.4151
[2017-12-15 17:29:47] Checkpointing model at epoch 10 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:29:47] Model Checkpointing finished.
[2017-12-15 17:29:55] Epoch 0011 mean train/dev loss: 112.9745 / 140.7938
[2017-12-15 17:30:02] Epoch 0012 mean train/dev loss: 111.2383 / 129.3528
[2017-12-15 17:30:09] Epoch 0013 mean train/dev loss: 109.7108 / 129.7060
[2017-12-15 17:30:16] Epoch 0014 mean train/dev loss: 108.5309 / 137.4868
[2017-12-15 17:30:23] Epoch 0015 mean train/dev loss: 107.4608 / 119.5116
[2017-12-15 17:30:23] Learning rate decayed by 0.5000
[2017-12-15 17:30:31] Epoch 0016 mean train/dev loss: 105.6251 / 119.9355
[2017-12-15 17:30:38] Epoch 0017 mean train/dev loss: 105.3725 / 116.1877
[2017-12-15 17:30:46] Epoch 0018 mean train/dev loss: 105.0310 / 117.5583
[2017-12-15 17:30:54] Epoch 0019 mean train/dev loss: 104.8113 / 114.4497
[2017-12-15 17:31:01] Epoch 0020 mean train/dev loss: 104.5809 / 113.1823
[2017-12-15 17:31:01] Checkpointing model at epoch 20 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:31:01] Model Checkpointing finished.
[2017-12-15 17:31:09] Epoch 0021 mean train/dev loss: 104.5520 / 115.1409
[2017-12-15 17:31:16] Epoch 0022 mean train/dev loss: 104.4163 / 115.8266
[2017-12-15 17:31:23] Epoch 0023 mean train/dev loss: 104.4743 / 114.0381
[2017-12-15 17:31:31] Epoch 0024 mean train/dev loss: 103.9903 / 110.3051
[2017-12-15 17:31:38] Epoch 0025 mean train/dev loss: 103.9711 / 114.6895
[2017-12-15 17:31:45] Epoch 0026 mean train/dev loss: 104.0536 / 108.8202
[2017-12-15 17:31:53] Epoch 0027 mean train/dev loss: 103.8413 / 107.3760
[2017-12-15 17:32:00] Epoch 0028 mean train/dev loss: 103.8762 / 106.6039
[2017-12-15 17:32:07] Epoch 0029 mean train/dev loss: 103.6390 / 104.7375
[2017-12-15 17:32:14] Epoch 0030 mean train/dev loss: 103.7549 / 111.3241
[2017-12-15 17:32:14] Learning rate decayed by 0.5000
[2017-12-15 17:32:14] Checkpointing model at epoch 30 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:32:15] Model Checkpointing finished.
[2017-12-15 17:32:22] Epoch 0031 mean train/dev loss: 103.3621 / 110.7206
[2017-12-15 17:32:29] Epoch 0032 mean train/dev loss: 103.2944 / 107.6114
[2017-12-15 17:32:37] Epoch 0033 mean train/dev loss: 103.3869 / 107.4352
[2017-12-15 17:32:44] Epoch 0034 mean train/dev loss: 103.3582 / 106.2536
[2017-12-15 17:32:52] Epoch 0035 mean train/dev loss: 103.3931 / 106.6005
[2017-12-15 17:32:59] Epoch 0036 mean train/dev loss: 103.4692 / 107.3433
[2017-12-15 17:33:07] Epoch 0037 mean train/dev loss: 103.4643 / 106.7757
[2017-12-15 17:33:15] Epoch 0038 mean train/dev loss: 103.4835 / 108.0630
[2017-12-15 17:33:22] Epoch 0039 mean train/dev loss: 103.5771 / 104.5737
[2017-12-15 17:33:29] Epoch 0040 mean train/dev loss: 103.4968 / 106.7270
[2017-12-15 17:33:29] Checkpointing model at epoch 40 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:33:30] Model Checkpointing finished.
[2017-12-15 17:33:37] Epoch 0041 mean train/dev loss: 103.5994 / 107.7980
[2017-12-15 17:33:45] Epoch 0042 mean train/dev loss: 103.6430 / 105.7173
[2017-12-15 17:33:52] Epoch 0043 mean train/dev loss: 103.6363 / 105.3289
[2017-12-15 17:33:59] Epoch 0044 mean train/dev loss: 103.4946 / 107.6328
[2017-12-15 17:34:06] Epoch 0045 mean train/dev loss: 103.4257 / 104.9911
[2017-12-15 17:34:06] Learning rate decayed by 0.5000
[2017-12-15 17:34:14] Epoch 0046 mean train/dev loss: 103.2233 / 106.1322
[2017-12-15 17:34:21] Epoch 0047 mean train/dev loss: 103.1816 / 105.6197
[2017-12-15 17:34:28] Epoch 0048 mean train/dev loss: 103.1695 / 106.1263
[2017-12-15 17:34:36] Epoch 0049 mean train/dev loss: 103.1813 / 104.8409
[2017-12-15 17:34:43] Epoch 0050 mean train/dev loss: 103.1348 / 103.7050
[2017-12-15 17:34:43] Checkpointing model at epoch 50 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:34:43] Model Checkpointing finished.
[2017-12-15 17:34:50] Epoch 0051 mean train/dev loss: 103.1190 / 106.7222
[2017-12-15 17:34:58] Epoch 0052 mean train/dev loss: 103.1259 / 106.0719
[2017-12-15 17:35:05] Epoch 0053 mean train/dev loss: 103.1388 / 106.2405
[2017-12-15 17:35:12] Epoch 0054 mean train/dev loss: 103.1562 / 105.2886
[2017-12-15 17:35:19] Epoch 0055 mean train/dev loss: 103.1568 / 104.2275
[2017-12-15 17:35:26] Epoch 0056 mean train/dev loss: 103.1620 / 104.9133
[2017-12-15 17:35:34] Epoch 0057 mean train/dev loss: 103.0973 / 105.9234
[2017-12-15 17:35:42] Epoch 0058 mean train/dev loss: 102.9303 / 105.8792
[2017-12-15 17:35:49] Epoch 0059 mean train/dev loss: 102.9098 / 105.8941
[2017-12-15 17:35:57] Epoch 0060 mean train/dev loss: 102.8708 / 104.5129
[2017-12-15 17:35:57] Learning rate decayed by 0.5000
[2017-12-15 17:35:57] Checkpointing model at epoch 60 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:35:58] Model Checkpointing finished.
[2017-12-15 17:36:05] Epoch 0061 mean train/dev loss: 102.6776 / 104.9266
[2017-12-15 17:36:13] Epoch 0062 mean train/dev loss: 102.6958 / 105.2669
[2017-12-15 17:36:20] Epoch 0063 mean train/dev loss: 102.6645 / 104.9117
[2017-12-15 17:36:27] Epoch 0064 mean train/dev loss: 102.6936 / 105.4369
[2017-12-15 17:36:35] Epoch 0065 mean train/dev loss: 102.6685 / 105.8911
[2017-12-15 17:36:42] Epoch 0066 mean train/dev loss: 102.6876 / 105.0672
[2017-12-15 17:36:49] Epoch 0067 mean train/dev loss: 102.6789 / 103.2491
[2017-12-15 17:36:56] Epoch 0068 mean train/dev loss: 102.7130 / 104.7267
[2017-12-15 17:37:04] Epoch 0069 mean train/dev loss: 102.6936 / 104.5632
[2017-12-15 17:37:11] Epoch 0070 mean train/dev loss: 102.6817 / 104.4331
[2017-12-15 17:37:11] Checkpointing model at epoch 70 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:37:12] Model Checkpointing finished.
[2017-12-15 17:37:19] Epoch 0071 mean train/dev loss: 102.7248 / 104.5728
[2017-12-15 17:37:27] Epoch 0072 mean train/dev loss: 102.6858 / 106.7409
[2017-12-15 17:37:34] Epoch 0073 mean train/dev loss: 102.6961 / 104.9913
[2017-12-15 17:37:41] Epoch 0074 mean train/dev loss: 102.6999 / 105.3914
[2017-12-15 17:37:49] Epoch 0075 mean train/dev loss: 102.6933 / 104.1237
[2017-12-15 17:37:49] Learning rate decayed by 0.5000
[2017-12-15 17:37:55] Epoch 0076 mean train/dev loss: 102.6357 / 103.9914
[2017-12-15 17:38:03] Epoch 0077 mean train/dev loss: 102.6316 / 103.7531
[2017-12-15 17:38:10] Epoch 0078 mean train/dev loss: 102.6333 / 104.4135
[2017-12-15 17:38:18] Epoch 0079 mean train/dev loss: 102.6083 / 103.3192
[2017-12-15 17:38:25] Epoch 0080 mean train/dev loss: 102.6131 / 104.3294
[2017-12-15 17:38:25] Checkpointing model at epoch 80 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:38:26] Model Checkpointing finished.
[2017-12-15 17:38:33] Epoch 0081 mean train/dev loss: 102.6528 / 104.1641
[2017-12-15 17:38:41] Epoch 0082 mean train/dev loss: 102.6256 / 104.1546
[2017-12-15 17:38:48] Epoch 0083 mean train/dev loss: 102.6213 / 104.4102
[2017-12-15 17:38:55] Epoch 0084 mean train/dev loss: 102.6448 / 105.0560
[2017-12-15 17:39:01] Epoch 0085 mean train/dev loss: 102.6488 / 104.8471
[2017-12-15 17:39:07] Epoch 0086 mean train/dev loss: 102.6582 / 103.2652
[2017-12-15 17:39:13] Epoch 0087 mean train/dev loss: 102.6423 / 103.5768
[2017-12-15 17:39:18] Epoch 0088 mean train/dev loss: 102.6514 / 104.8282
[2017-12-15 17:39:23] Epoch 0089 mean train/dev loss: 102.6395 / 103.8817
[2017-12-15 17:39:27] Epoch 0090 mean train/dev loss: 102.6238 / 104.5245
[2017-12-15 17:39:27] Learning rate decayed by 0.5000
[2017-12-15 17:39:27] Checkpointing model at epoch 90 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:39:28] Model Checkpointing finished.
[2017-12-15 17:39:33] Epoch 0091 mean train/dev loss: 102.6032 / 103.8899
[2017-12-15 17:39:38] Epoch 0092 mean train/dev loss: 102.5944 / 103.9417
[2017-12-15 17:39:42] Epoch 0093 mean train/dev loss: 102.5903 / 104.5218
[2017-12-15 17:39:47] Epoch 0094 mean train/dev loss: 102.6069 / 104.4436
[2017-12-15 17:39:52] Epoch 0095 mean train/dev loss: 102.5945 / 104.2853
[2017-12-15 17:39:57] Epoch 0096 mean train/dev loss: 102.6015 / 104.8075
[2017-12-15 17:40:02] Epoch 0097 mean train/dev loss: 102.5984 / 103.7102
[2017-12-15 17:40:07] Epoch 0098 mean train/dev loss: 102.6160 / 104.6971
[2017-12-15 17:40:12] Epoch 0099 mean train/dev loss: 102.5838 / 104.4701
[2017-12-15 17:40:17] Epoch 0100 mean train/dev loss: 102.5846 / 103.8155
[2017-12-15 17:40:17] Checkpointing model at epoch 100 for ffn.hl_50.lr_0.01.wd_0.1
[2017-12-15 17:40:18] Model Checkpointing finished.
[2017-12-15 17:40:23] Epoch 0101 mean train/dev loss: 102.6127 / 104.5617
[2017-12-15 17:40:28] Epoch 0102 mean train/dev loss: 102.6071 / 104.2978
[2017-12-15 17:40:33] Epoch 0103 mean train/dev loss: 102.6121 / 104.3756
[2017-12-15 17:40:37] Epoch 0104 mean train/dev loss: 102.5928 / 104.5383
[2017-12-15 17:40:42] Epoch 0105 mean train/dev loss: 102.6034 / 104.1767
[2017-12-15 17:40:42] Learning rate decayed by 0.5000
[2017-12-15 17:40:47] Epoch 0106 mean train/dev loss: 102.5970 / 104.1073
[2017-12-15 17:40:52] Epoch 0107 mean train/dev loss: 102.6103 / 104.2193
[2017-12-15 17:40:57] Epoch 0108 mean train/dev loss: 102.5770 / 104.0633
[2017-12-15 17:40:57] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:40:57] 
                       *** Training finished *** 
[2017-12-15 17:40:58] Dev MSE: 104.0633
[2017-12-15 17:41:02] Training MSE: 102.5708
[2017-12-15 17:41:03] Experiment ffn.hl_50.lr_0.01.wd_0.1 logging ended.
