[2017-12-15 15:20:49] Experiment ffn.hl_50_50.lr_0.1.wd_0.01 logging started.
[2017-12-15 15:20:49] 
                       *** Starting Experiment ffn.hl_50_50.lr_0.1.wd_0.01 ***
                      
[2017-12-15 15:20:49] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 15:20:49] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 1)
                      )
[2017-12-15 15:20:49]  *** Training on GPU ***
[2017-12-15 15:20:58] Epoch 0001 mean train/dev loss: 4196.8751 / 154.7133
[2017-12-15 15:21:06] Epoch 0002 mean train/dev loss: 120.7024 / 1029.3881
[2017-12-15 15:21:14] Epoch 0003 mean train/dev loss: 125.5263 / 192.4045
[2017-12-15 15:21:23] Epoch 0004 mean train/dev loss: 145.0290 / 495.7160
[2017-12-15 15:21:30] Epoch 0005 mean train/dev loss: 173.2340 / 258.2914
[2017-12-15 15:21:38] Epoch 0006 mean train/dev loss: 146.0439 / 189.3830
[2017-12-15 15:21:46] Epoch 0007 mean train/dev loss: 148.6387 / 211.5632
[2017-12-15 15:21:55] Epoch 0008 mean train/dev loss: 248.8749 / 339.4914
[2017-12-15 15:22:03] Epoch 0009 mean train/dev loss: 158.8363 / 109.3992
[2017-12-15 15:22:11] Epoch 0010 mean train/dev loss: 88.7686 / 124.5802
[2017-12-15 15:22:11] Checkpointing model at epoch 10 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:22:11] Model Checkpointing finished.
[2017-12-15 15:22:19] Epoch 0011 mean train/dev loss: 93.8146 / 142.8994
[2017-12-15 15:22:27] Epoch 0012 mean train/dev loss: 103.7796 / 144.8088
[2017-12-15 15:22:35] Epoch 0013 mean train/dev loss: 107.4836 / 113.1960
[2017-12-15 15:22:43] Epoch 0014 mean train/dev loss: 100.4057 / 157.6764
[2017-12-15 15:22:51] Epoch 0015 mean train/dev loss: 120.5000 / 146.2014
[2017-12-15 15:22:51] Learning rate decayed by 0.5000
[2017-12-15 15:22:59] Epoch 0016 mean train/dev loss: 72.4979 / 125.0112
[2017-12-15 15:23:07] Epoch 0017 mean train/dev loss: 71.7079 / 94.4145
[2017-12-15 15:23:14] Epoch 0018 mean train/dev loss: 71.9599 / 106.6218
[2017-12-15 15:23:23] Epoch 0019 mean train/dev loss: 74.4489 / 94.9981
[2017-12-15 15:23:30] Epoch 0020 mean train/dev loss: 71.9271 / 96.3860
[2017-12-15 15:23:30] Checkpointing model at epoch 20 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:23:30] Model Checkpointing finished.
[2017-12-15 15:23:39] Epoch 0021 mean train/dev loss: 71.7670 / 92.9331
[2017-12-15 15:23:47] Epoch 0022 mean train/dev loss: 72.5848 / 112.0033
[2017-12-15 15:23:54] Epoch 0023 mean train/dev loss: 70.9919 / 132.3055
[2017-12-15 15:24:02] Epoch 0024 mean train/dev loss: 69.2217 / 96.6794
[2017-12-15 15:24:10] Epoch 0025 mean train/dev loss: 70.1967 / 124.6424
[2017-12-15 15:24:18] Epoch 0026 mean train/dev loss: 68.1532 / 76.9168
[2017-12-15 15:24:26] Epoch 0027 mean train/dev loss: 69.4225 / 107.2261
[2017-12-15 15:24:34] Epoch 0028 mean train/dev loss: 69.3740 / 83.0931
[2017-12-15 15:24:42] Epoch 0029 mean train/dev loss: 67.8758 / 127.3319
[2017-12-15 15:24:50] Epoch 0030 mean train/dev loss: 66.6871 / 163.9975
[2017-12-15 15:24:50] Learning rate decayed by 0.5000
[2017-12-15 15:24:50] Checkpointing model at epoch 30 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:24:50] Model Checkpointing finished.
[2017-12-15 15:24:59] Epoch 0031 mean train/dev loss: 58.2699 / 85.3176
[2017-12-15 15:25:07] Epoch 0032 mean train/dev loss: 58.7458 / 102.6637
[2017-12-15 15:25:15] Epoch 0033 mean train/dev loss: 58.9444 / 81.8070
[2017-12-15 15:25:22] Epoch 0034 mean train/dev loss: 59.6924 / 76.1676
[2017-12-15 15:25:30] Epoch 0035 mean train/dev loss: 59.2521 / 87.9620
[2017-12-15 15:25:38] Epoch 0036 mean train/dev loss: 58.4839 / 122.6296
[2017-12-15 15:25:46] Epoch 0037 mean train/dev loss: 57.5297 / 97.2376
[2017-12-15 15:25:54] Epoch 0038 mean train/dev loss: 56.4221 / 85.3705
[2017-12-15 15:26:02] Epoch 0039 mean train/dev loss: 55.7092 / 102.9671
[2017-12-15 15:26:10] Epoch 0040 mean train/dev loss: 57.2140 / 94.2815
[2017-12-15 15:26:10] Checkpointing model at epoch 40 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:26:10] Model Checkpointing finished.
[2017-12-15 15:26:18] Epoch 0041 mean train/dev loss: 55.9263 / 86.6010
[2017-12-15 15:26:26] Epoch 0042 mean train/dev loss: 55.4364 / 99.5967
[2017-12-15 15:26:34] Epoch 0043 mean train/dev loss: 55.6139 / 101.8698
[2017-12-15 15:26:42] Epoch 0044 mean train/dev loss: 55.6073 / 81.4707
[2017-12-15 15:26:49] Epoch 0045 mean train/dev loss: 55.4310 / 95.7803
[2017-12-15 15:26:49] Learning rate decayed by 0.5000
[2017-12-15 15:26:57] Epoch 0046 mean train/dev loss: 51.7755 / 92.8456
[2017-12-15 15:27:05] Epoch 0047 mean train/dev loss: 51.6974 / 83.3831
[2017-12-15 15:27:13] Epoch 0048 mean train/dev loss: 52.0271 / 90.6311
[2017-12-15 15:27:21] Epoch 0049 mean train/dev loss: 52.3337 / 83.9205
[2017-12-15 15:27:29] Epoch 0050 mean train/dev loss: 51.9220 / 92.6457
[2017-12-15 15:27:29] Checkpointing model at epoch 50 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:27:30] Model Checkpointing finished.
[2017-12-15 15:27:38] Epoch 0051 mean train/dev loss: 52.0926 / 90.7411
[2017-12-15 15:27:46] Epoch 0052 mean train/dev loss: 51.9284 / 89.6899
[2017-12-15 15:27:54] Epoch 0053 mean train/dev loss: 51.7492 / 96.8822
[2017-12-15 15:28:01] Epoch 0054 mean train/dev loss: 51.8037 / 85.7748
[2017-12-15 15:28:09] Epoch 0055 mean train/dev loss: 51.5641 / 88.2896
[2017-12-15 15:28:18] Epoch 0056 mean train/dev loss: 51.4453 / 93.7049
[2017-12-15 15:28:25] Epoch 0057 mean train/dev loss: 51.5871 / 84.5341
[2017-12-15 15:28:33] Epoch 0058 mean train/dev loss: 51.3506 / 86.9876
[2017-12-15 15:28:41] Epoch 0059 mean train/dev loss: 51.8443 / 82.9168
[2017-12-15 15:28:49] Epoch 0060 mean train/dev loss: 51.4905 / 82.1401
[2017-12-15 15:28:49] Learning rate decayed by 0.5000
[2017-12-15 15:28:49] Checkpointing model at epoch 60 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:28:50] Model Checkpointing finished.
[2017-12-15 15:28:58] Epoch 0061 mean train/dev loss: 49.7679 / 86.6658
[2017-12-15 15:29:05] Epoch 0062 mean train/dev loss: 49.8002 / 86.7388
[2017-12-15 15:29:13] Epoch 0063 mean train/dev loss: 49.7693 / 75.6637
[2017-12-15 15:29:21] Epoch 0064 mean train/dev loss: 49.8780 / 82.4072
[2017-12-15 15:29:29] Epoch 0065 mean train/dev loss: 49.7654 / 80.5454
[2017-12-15 15:29:36] Epoch 0066 mean train/dev loss: 49.8971 / 86.8121
[2017-12-15 15:29:44] Epoch 0067 mean train/dev loss: 49.7902 / 94.0944
[2017-12-15 15:29:52] Epoch 0068 mean train/dev loss: 49.8748 / 83.2058
[2017-12-15 15:30:00] Epoch 0069 mean train/dev loss: 49.6334 / 91.6617
[2017-12-15 15:30:08] Epoch 0070 mean train/dev loss: 49.6048 / 96.2415
[2017-12-15 15:30:08] Checkpointing model at epoch 70 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:30:08] Model Checkpointing finished.
[2017-12-15 15:30:16] Epoch 0071 mean train/dev loss: 49.7516 / 89.6017
[2017-12-15 15:30:24] Epoch 0072 mean train/dev loss: 49.7162 / 80.1216
[2017-12-15 15:30:32] Epoch 0073 mean train/dev loss: 49.6925 / 83.5172
[2017-12-15 15:30:40] Epoch 0074 mean train/dev loss: 49.4640 / 79.6778
[2017-12-15 15:30:48] Epoch 0075 mean train/dev loss: 49.6221 / 78.9422
[2017-12-15 15:30:48] Learning rate decayed by 0.5000
[2017-12-15 15:30:56] Epoch 0076 mean train/dev loss: 48.6664 / 83.0116
[2017-12-15 15:31:03] Epoch 0077 mean train/dev loss: 48.6975 / 86.0517
[2017-12-15 15:31:11] Epoch 0078 mean train/dev loss: 48.6397 / 85.4803
[2017-12-15 15:31:19] Epoch 0079 mean train/dev loss: 48.6796 / 82.4336
[2017-12-15 15:31:27] Epoch 0080 mean train/dev loss: 48.6427 / 94.7808
[2017-12-15 15:31:27] Checkpointing model at epoch 80 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:31:27] Model Checkpointing finished.
[2017-12-15 15:31:36] Epoch 0081 mean train/dev loss: 48.6346 / 87.1273
[2017-12-15 15:31:43] Epoch 0082 mean train/dev loss: 48.5960 / 82.3428
[2017-12-15 15:31:51] Epoch 0083 mean train/dev loss: 48.6044 / 84.0534
[2017-12-15 15:31:59] Epoch 0084 mean train/dev loss: 48.5528 / 86.9465
[2017-12-15 15:32:07] Epoch 0085 mean train/dev loss: 48.5544 / 80.6788
[2017-12-15 15:32:15] Epoch 0086 mean train/dev loss: 48.5339 / 78.4826
[2017-12-15 15:32:23] Epoch 0087 mean train/dev loss: 48.5045 / 78.9711
[2017-12-15 15:32:31] Epoch 0088 mean train/dev loss: 48.6492 / 77.9216
[2017-12-15 15:32:39] Epoch 0089 mean train/dev loss: 48.5062 / 85.8355
[2017-12-15 15:32:47] Epoch 0090 mean train/dev loss: 48.4849 / 82.4611
[2017-12-15 15:32:47] Learning rate decayed by 0.5000
[2017-12-15 15:32:47] Checkpointing model at epoch 90 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:32:47] Model Checkpointing finished.
[2017-12-15 15:32:55] Epoch 0091 mean train/dev loss: 48.0061 / 81.7119
[2017-12-15 15:33:03] Epoch 0092 mean train/dev loss: 48.0125 / 80.3062
[2017-12-15 15:33:11] Epoch 0093 mean train/dev loss: 48.0492 / 86.1430
[2017-12-15 15:33:19] Epoch 0094 mean train/dev loss: 48.0004 / 82.7728
[2017-12-15 15:33:26] Epoch 0095 mean train/dev loss: 48.0273 / 83.4787
[2017-12-15 15:33:32] Epoch 0096 mean train/dev loss: 48.0384 / 78.4550
[2017-12-15 15:33:40] Epoch 0097 mean train/dev loss: 47.9706 / 81.5287
[2017-12-15 15:33:47] Epoch 0098 mean train/dev loss: 47.9935 / 79.7168
[2017-12-15 15:33:54] Epoch 0099 mean train/dev loss: 48.0047 / 83.5168
[2017-12-15 15:34:01] Epoch 0100 mean train/dev loss: 48.0303 / 79.6881
[2017-12-15 15:34:01] Checkpointing model at epoch 100 for ffn.hl_50_50.lr_0.1.wd_0.01
[2017-12-15 15:34:01] Model Checkpointing finished.
[2017-12-15 15:34:08] Epoch 0101 mean train/dev loss: 47.9921 / 81.0412
[2017-12-15 15:34:15] Epoch 0102 mean train/dev loss: 47.9417 / 87.2779
[2017-12-15 15:34:22] Epoch 0103 mean train/dev loss: 47.9786 / 79.5023
[2017-12-15 15:34:29] Epoch 0104 mean train/dev loss: 47.9113 / 77.5178
[2017-12-15 15:34:29] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:34:29] 
                       *** Training finished *** 
[2017-12-15 15:34:30] Dev MSE: 77.5178
[2017-12-15 15:34:36] Training MSE: 47.7578
[2017-12-15 15:34:39] Experiment ffn.hl_50_50.lr_0.1.wd_0.01 logging ended.
