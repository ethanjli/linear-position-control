[2017-12-15 14:56:58] Experiment ffn.hl_20_20_20.lr_0.1.wd_1.0 logging started.
[2017-12-15 14:56:58] 
                       *** Starting Experiment ffn.hl_20_20_20.lr_0.1.wd_1.0 ***
                      
[2017-12-15 14:56:58] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 14:56:58] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 20)
                        (relu3): ReLU ()
                        (linear4): Linear (20 -> 1)
                      )
[2017-12-15 14:56:58]  *** Training on GPU ***
[2017-12-15 14:57:06] Epoch 0001 mean train/dev loss: 4785.3098 / 214.0322
[2017-12-15 14:57:13] Epoch 0002 mean train/dev loss: 296.1417 / 177.4749
[2017-12-15 14:57:20] Epoch 0003 mean train/dev loss: 139.3096 / 225.0910
[2017-12-15 14:57:27] Epoch 0004 mean train/dev loss: 500.1581 / 1342.7277
[2017-12-15 14:57:34] Epoch 0005 mean train/dev loss: 130.6574 / 122.5432
[2017-12-15 14:57:42] Epoch 0006 mean train/dev loss: 123.0036 / 155.0298
[2017-12-15 14:57:50] Epoch 0007 mean train/dev loss: 184.9742 / 338.2568
[2017-12-15 14:57:57] Epoch 0008 mean train/dev loss: 129.5619 / 127.2739
[2017-12-15 14:58:04] Epoch 0009 mean train/dev loss: 149.2999 / 175.0488
[2017-12-15 14:58:12] Epoch 0010 mean train/dev loss: 150.1583 / 174.4916
[2017-12-15 14:58:12] Checkpointing model at epoch 10 for ffn.hl_20_20_20.lr_0.1.wd_1.0
[2017-12-15 14:58:12] Model Checkpointing finished.
[2017-12-15 14:58:19] Epoch 0011 mean train/dev loss: 164.7328 / 207.6744
[2017-12-15 14:58:27] Epoch 0012 mean train/dev loss: 138.1574 / 138.3661
[2017-12-15 14:58:34] Epoch 0013 mean train/dev loss: 155.2791 / 122.8810
[2017-12-15 14:58:41] Epoch 0014 mean train/dev loss: 139.8029 / 159.0120
[2017-12-15 14:58:48] Epoch 0015 mean train/dev loss: 161.5181 / 136.8645
[2017-12-15 14:58:48] Learning rate decayed by 0.5000
[2017-12-15 14:58:55] Epoch 0016 mean train/dev loss: 114.3764 / 127.0215
[2017-12-15 14:59:02] Epoch 0017 mean train/dev loss: 115.0395 / 124.4886
[2017-12-15 14:59:09] Epoch 0018 mean train/dev loss: 119.4835 / 143.8291
[2017-12-15 14:59:16] Epoch 0019 mean train/dev loss: 119.8413 / 166.4708
[2017-12-15 14:59:24] Epoch 0020 mean train/dev loss: 123.1900 / 152.8867
[2017-12-15 14:59:24] Checkpointing model at epoch 20 for ffn.hl_20_20_20.lr_0.1.wd_1.0
[2017-12-15 14:59:24] Model Checkpointing finished.
[2017-12-15 14:59:31] Epoch 0021 mean train/dev loss: 121.7966 / 116.8646
[2017-12-15 14:59:38] Epoch 0022 mean train/dev loss: 124.6334 / 132.4932
[2017-12-15 14:59:45] Epoch 0023 mean train/dev loss: 115.1314 / 116.6822
[2017-12-15 14:59:53] Epoch 0024 mean train/dev loss: 117.9603 / 119.1680
[2017-12-15 15:00:00] Epoch 0025 mean train/dev loss: 118.3482 / 114.4731
[2017-12-15 15:00:07] Epoch 0026 mean train/dev loss: 117.4279 / 132.0560
[2017-12-15 15:00:15] Epoch 0027 mean train/dev loss: 119.0986 / 126.5788
[2017-12-15 15:00:21] Epoch 0028 mean train/dev loss: 116.0455 / 119.1500
[2017-12-15 15:00:29] Epoch 0029 mean train/dev loss: 116.9332 / 118.6477
[2017-12-15 15:00:36] Epoch 0030 mean train/dev loss: 116.4602 / 132.1237
[2017-12-15 15:00:36] Learning rate decayed by 0.5000
[2017-12-15 15:00:36] Checkpointing model at epoch 30 for ffn.hl_20_20_20.lr_0.1.wd_1.0
[2017-12-15 15:00:36] Model Checkpointing finished.
[2017-12-15 15:00:44] Epoch 0031 mean train/dev loss: 107.2573 / 110.2115
[2017-12-15 15:00:52] Epoch 0032 mean train/dev loss: 106.8718 / 109.7087
[2017-12-15 15:00:59] Epoch 0033 mean train/dev loss: 107.7861 / 120.8369
[2017-12-15 15:01:06] Epoch 0034 mean train/dev loss: 106.7986 / 111.4247
[2017-12-15 15:01:14] Epoch 0035 mean train/dev loss: 107.0683 / 105.3651
[2017-12-15 15:01:21] Epoch 0036 mean train/dev loss: 106.5973 / 116.2506
[2017-12-15 15:01:28] Epoch 0037 mean train/dev loss: 109.0880 / 134.0128
[2017-12-15 15:01:35] Epoch 0038 mean train/dev loss: 106.0816 / 130.4991
[2017-12-15 15:01:43] Epoch 0039 mean train/dev loss: 107.4490 / 127.2389
[2017-12-15 15:01:50] Epoch 0040 mean train/dev loss: 106.0539 / 125.4588
[2017-12-15 15:01:50] Checkpointing model at epoch 40 for ffn.hl_20_20_20.lr_0.1.wd_1.0
[2017-12-15 15:01:50] Model Checkpointing finished.
[2017-12-15 15:01:57] Epoch 0041 mean train/dev loss: 106.7403 / 127.8052
[2017-12-15 15:02:05] Epoch 0042 mean train/dev loss: 106.5094 / 123.9487
[2017-12-15 15:02:13] Epoch 0043 mean train/dev loss: 107.0661 / 149.7095
[2017-12-15 15:02:20] Epoch 0044 mean train/dev loss: 106.1088 / 134.8453
[2017-12-15 15:02:27] Epoch 0045 mean train/dev loss: 106.3220 / 118.1559
[2017-12-15 15:02:27] Learning rate decayed by 0.5000
[2017-12-15 15:02:34] Epoch 0046 mean train/dev loss: 100.9571 / 117.5653
[2017-12-15 15:02:41] Epoch 0047 mean train/dev loss: 101.1032 / 122.3751
[2017-12-15 15:02:49] Epoch 0048 mean train/dev loss: 101.7712 / 130.7524
[2017-12-15 15:02:56] Epoch 0049 mean train/dev loss: 101.3013 / 122.6884
[2017-12-15 15:03:04] Epoch 0050 mean train/dev loss: 101.4672 / 119.5850
[2017-12-15 15:03:04] Checkpointing model at epoch 50 for ffn.hl_20_20_20.lr_0.1.wd_1.0
[2017-12-15 15:03:04] Model Checkpointing finished.
[2017-12-15 15:03:12] Epoch 0051 mean train/dev loss: 100.7717 / 113.2421
[2017-12-15 15:03:18] Epoch 0052 mean train/dev loss: 101.4152 / 127.8144
[2017-12-15 15:03:25] Epoch 0053 mean train/dev loss: 100.7912 / 120.7286
[2017-12-15 15:03:33] Epoch 0054 mean train/dev loss: 100.8575 / 116.0715
[2017-12-15 15:03:40] Epoch 0055 mean train/dev loss: 100.7915 / 129.6858
[2017-12-15 15:03:46] Epoch 0056 mean train/dev loss: 100.9087 / 131.6886
[2017-12-15 15:03:54] Epoch 0057 mean train/dev loss: 101.2711 / 125.7073
[2017-12-15 15:04:01] Epoch 0058 mean train/dev loss: 100.7675 / 115.5971
[2017-12-15 15:04:08] Epoch 0059 mean train/dev loss: 100.8808 / 125.1426
[2017-12-15 15:04:15] Epoch 0060 mean train/dev loss: 100.8228 / 120.3109
[2017-12-15 15:04:15] Learning rate decayed by 0.5000
[2017-12-15 15:04:15] Checkpointing model at epoch 60 for ffn.hl_20_20_20.lr_0.1.wd_1.0
[2017-12-15 15:04:15] Model Checkpointing finished.
[2017-12-15 15:04:22] Epoch 0061 mean train/dev loss: 98.4713 / 118.2050
[2017-12-15 15:04:30] Epoch 0062 mean train/dev loss: 98.4870 / 118.3881
[2017-12-15 15:04:37] Epoch 0063 mean train/dev loss: 98.6281 / 118.8071
[2017-12-15 15:04:44] Epoch 0064 mean train/dev loss: 98.4686 / 120.6372
[2017-12-15 15:04:52] Epoch 0065 mean train/dev loss: 98.1811 / 118.3419
[2017-12-15 15:04:59] Epoch 0066 mean train/dev loss: 98.3945 / 127.2032
[2017-12-15 15:05:06] Epoch 0067 mean train/dev loss: 98.4045 / 126.6039
[2017-12-15 15:05:14] Epoch 0068 mean train/dev loss: 98.2362 / 114.7219
[2017-12-15 15:05:21] Epoch 0069 mean train/dev loss: 98.5429 / 121.6833
[2017-12-15 15:05:28] Epoch 0070 mean train/dev loss: 98.2355 / 120.8752
[2017-12-15 15:05:28] Checkpointing model at epoch 70 for ffn.hl_20_20_20.lr_0.1.wd_1.0
[2017-12-15 15:05:28] Model Checkpointing finished.
[2017-12-15 15:05:35] Epoch 0071 mean train/dev loss: 98.4751 / 114.8332
[2017-12-15 15:05:42] Epoch 0072 mean train/dev loss: 98.3377 / 126.8699
[2017-12-15 15:05:48] Epoch 0073 mean train/dev loss: 98.4224 / 118.8529
[2017-12-15 15:05:56] Epoch 0074 mean train/dev loss: 98.4491 / 115.4464
[2017-12-15 15:06:03] Epoch 0075 mean train/dev loss: 98.5646 / 114.2858
[2017-12-15 15:06:03] Learning rate decayed by 0.5000
[2017-12-15 15:06:10] Epoch 0076 mean train/dev loss: 97.1356 / 119.1889
[2017-12-15 15:06:10] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:06:10] 
                       *** Training finished *** 
[2017-12-15 15:06:12] Dev MSE: 119.1889
[2017-12-15 15:06:17] Training MSE: 96.7109
[2017-12-15 15:06:19] Experiment ffn.hl_20_20_20.lr_0.1.wd_1.0 logging ended.
