[2017-12-15 17:41:06] Experiment ffn.hl_50_50.lr_0.01.wd_1.0 logging started.
[2017-12-15 17:41:06] 
                       *** Starting Experiment ffn.hl_50_50.lr_0.01.wd_1.0 ***
                      
[2017-12-15 17:41:06] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 17:41:06] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 1)
                      )
[2017-12-15 17:41:06]  *** Training on GPU ***
[2017-12-15 17:41:14] Epoch 0001 mean train/dev loss: 20314.9156 / 320.7936
[2017-12-15 17:41:22] Epoch 0002 mean train/dev loss: 159.3502 / 184.3686
[2017-12-15 17:41:30] Epoch 0003 mean train/dev loss: 130.7220 / 151.9597
[2017-12-15 17:41:38] Epoch 0004 mean train/dev loss: 122.4525 / 153.0039
[2017-12-15 17:41:46] Epoch 0005 mean train/dev loss: 118.0055 / 147.7033
[2017-12-15 17:41:53] Epoch 0006 mean train/dev loss: 115.3418 / 120.6810
[2017-12-15 17:42:01] Epoch 0007 mean train/dev loss: 114.6056 / 125.5359
[2017-12-15 17:42:09] Epoch 0008 mean train/dev loss: 114.2732 / 119.2331
[2017-12-15 17:42:16] Epoch 0009 mean train/dev loss: 114.0150 / 116.3259
[2017-12-15 17:42:24] Epoch 0010 mean train/dev loss: 114.1877 / 143.0635
[2017-12-15 17:42:24] Checkpointing model at epoch 10 for ffn.hl_50_50.lr_0.01.wd_1.0
[2017-12-15 17:42:25] Model Checkpointing finished.
[2017-12-15 17:42:33] Epoch 0011 mean train/dev loss: 113.5660 / 111.5414
[2017-12-15 17:42:41] Epoch 0012 mean train/dev loss: 111.7855 / 122.6619
[2017-12-15 17:42:49] Epoch 0013 mean train/dev loss: 111.9147 / 110.6859
[2017-12-15 17:42:56] Epoch 0014 mean train/dev loss: 111.4630 / 116.5540
[2017-12-15 17:43:04] Epoch 0015 mean train/dev loss: 111.6151 / 126.7272
[2017-12-15 17:43:04] Learning rate decayed by 0.5000
[2017-12-15 17:43:12] Epoch 0016 mean train/dev loss: 108.9609 / 107.8304
[2017-12-15 17:43:20] Epoch 0017 mean train/dev loss: 108.8689 / 110.0405
[2017-12-15 17:43:28] Epoch 0018 mean train/dev loss: 109.2861 / 110.8969
[2017-12-15 17:43:36] Epoch 0019 mean train/dev loss: 109.0473 / 120.8070
[2017-12-15 17:43:44] Epoch 0020 mean train/dev loss: 108.9631 / 118.0278
[2017-12-15 17:43:44] Checkpointing model at epoch 20 for ffn.hl_50_50.lr_0.01.wd_1.0
[2017-12-15 17:43:44] Model Checkpointing finished.
[2017-12-15 17:43:53] Epoch 0021 mean train/dev loss: 109.4096 / 110.4769
[2017-12-15 17:44:01] Epoch 0022 mean train/dev loss: 109.2237 / 110.6308
[2017-12-15 17:44:09] Epoch 0023 mean train/dev loss: 109.1146 / 109.0299
[2017-12-15 17:44:17] Epoch 0024 mean train/dev loss: 109.0994 / 118.7285
[2017-12-15 17:44:25] Epoch 0025 mean train/dev loss: 109.2507 / 113.4262
[2017-12-15 17:44:33] Epoch 0026 mean train/dev loss: 109.0704 / 109.1011
[2017-12-15 17:44:41] Epoch 0027 mean train/dev loss: 108.9712 / 109.8061
[2017-12-15 17:44:49] Epoch 0028 mean train/dev loss: 109.2379 / 110.0714
[2017-12-15 17:44:57] Epoch 0029 mean train/dev loss: 108.9639 / 104.8816
[2017-12-15 17:45:05] Epoch 0030 mean train/dev loss: 109.1777 / 106.0903
[2017-12-15 17:45:05] Learning rate decayed by 0.5000
[2017-12-15 17:45:05] Checkpointing model at epoch 30 for ffn.hl_50_50.lr_0.01.wd_1.0
[2017-12-15 17:45:05] Model Checkpointing finished.
[2017-12-15 17:45:14] Epoch 0031 mean train/dev loss: 107.7281 / 107.3061
[2017-12-15 17:45:21] Epoch 0032 mean train/dev loss: 107.8625 / 109.2895
[2017-12-15 17:45:30] Epoch 0033 mean train/dev loss: 107.8792 / 107.7096
[2017-12-15 17:45:38] Epoch 0034 mean train/dev loss: 107.8935 / 109.3335
[2017-12-15 17:45:45] Epoch 0035 mean train/dev loss: 108.0373 / 110.0751
[2017-12-15 17:45:53] Epoch 0036 mean train/dev loss: 107.9405 / 108.5275
[2017-12-15 17:46:01] Epoch 0037 mean train/dev loss: 107.2483 / 110.5772
[2017-12-15 17:46:09] Epoch 0038 mean train/dev loss: 106.6479 / 109.3928
[2017-12-15 17:46:18] Epoch 0039 mean train/dev loss: 106.5985 / 109.1874
[2017-12-15 17:46:26] Epoch 0040 mean train/dev loss: 106.5880 / 112.0253
[2017-12-15 17:46:26] Checkpointing model at epoch 40 for ffn.hl_50_50.lr_0.01.wd_1.0
[2017-12-15 17:46:26] Model Checkpointing finished.
[2017-12-15 17:46:34] Epoch 0041 mean train/dev loss: 106.5143 / 111.7964
[2017-12-15 17:46:42] Epoch 0042 mean train/dev loss: 106.6892 / 113.1596
[2017-12-15 17:46:50] Epoch 0043 mean train/dev loss: 106.4430 / 112.1223
[2017-12-15 17:46:58] Epoch 0044 mean train/dev loss: 106.2797 / 109.7438
[2017-12-15 17:47:07] Epoch 0045 mean train/dev loss: 106.3617 / 113.1276
[2017-12-15 17:47:07] Learning rate decayed by 0.5000
[2017-12-15 17:47:14] Epoch 0046 mean train/dev loss: 105.6983 / 112.5849
[2017-12-15 17:47:22] Epoch 0047 mean train/dev loss: 105.6706 / 112.2589
[2017-12-15 17:47:30] Epoch 0048 mean train/dev loss: 105.7693 / 114.6293
[2017-12-15 17:47:39] Epoch 0049 mean train/dev loss: 105.7454 / 113.2228
[2017-12-15 17:47:46] Epoch 0050 mean train/dev loss: 105.7071 / 113.8088
[2017-12-15 17:47:46] Checkpointing model at epoch 50 for ffn.hl_50_50.lr_0.01.wd_1.0
[2017-12-15 17:47:47] Model Checkpointing finished.
[2017-12-15 17:47:55] Epoch 0051 mean train/dev loss: 105.7039 / 111.5910
[2017-12-15 17:48:03] Epoch 0052 mean train/dev loss: 105.7083 / 112.3569
[2017-12-15 17:48:11] Epoch 0053 mean train/dev loss: 105.7036 / 116.7490
[2017-12-15 17:48:19] Epoch 0054 mean train/dev loss: 105.7168 / 110.8909
[2017-12-15 17:48:27] Epoch 0055 mean train/dev loss: 105.6882 / 111.7875
[2017-12-15 17:48:35] Epoch 0056 mean train/dev loss: 105.7369 / 111.7822
[2017-12-15 17:48:44] Epoch 0057 mean train/dev loss: 105.6875 / 110.6208
[2017-12-15 17:48:52] Epoch 0058 mean train/dev loss: 105.6400 / 113.5363
[2017-12-15 17:49:00] Epoch 0059 mean train/dev loss: 105.6437 / 111.7755
[2017-12-15 17:49:08] Epoch 0060 mean train/dev loss: 105.6374 / 113.5383
[2017-12-15 17:49:08] Learning rate decayed by 0.5000
[2017-12-15 17:49:08] Checkpointing model at epoch 60 for ffn.hl_50_50.lr_0.01.wd_1.0
[2017-12-15 17:49:08] Model Checkpointing finished.
[2017-12-15 17:49:16] Epoch 0061 mean train/dev loss: 105.2570 / 111.2967
[2017-12-15 17:49:24] Epoch 0062 mean train/dev loss: 105.3263 / 112.0591
[2017-12-15 17:49:32] Epoch 0063 mean train/dev loss: 105.3290 / 114.6038
[2017-12-15 17:49:40] Epoch 0064 mean train/dev loss: 105.2682 / 113.6138
[2017-12-15 17:49:47] Epoch 0065 mean train/dev loss: 105.2918 / 112.5656
[2017-12-15 17:49:53] Epoch 0066 mean train/dev loss: 105.3111 / 112.2192
[2017-12-15 17:50:01] Epoch 0067 mean train/dev loss: 105.3248 / 113.3877
[2017-12-15 17:50:08] Epoch 0068 mean train/dev loss: 105.2777 / 113.2097
[2017-12-15 17:50:15] Epoch 0069 mean train/dev loss: 105.2982 / 112.9887
[2017-12-15 17:50:22] Epoch 0070 mean train/dev loss: 105.2892 / 113.6185
[2017-12-15 17:50:22] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:50:22] 
                       *** Training finished *** 
[2017-12-15 17:50:23] Dev MSE: 113.6185
[2017-12-15 17:50:29] Training MSE: 105.1657
[2017-12-15 17:50:30] Experiment ffn.hl_50_50.lr_0.01.wd_1.0 logging ended.
