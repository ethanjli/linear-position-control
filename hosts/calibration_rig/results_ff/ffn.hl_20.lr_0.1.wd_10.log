[2017-12-15 14:32:06] Experiment ffn.hl_20.lr_0.1.wd_10 logging started.
[2017-12-15 14:32:06] 
                       *** Starting Experiment ffn.hl_20.lr_0.1.wd_10 ***
                      
[2017-12-15 14:32:06] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 14:32:09] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 1)
                      )
[2017-12-15 14:32:09]  *** Training on GPU ***
[2017-12-15 14:32:15] Epoch 0001 mean train/dev loss: 15080.6793 / 478.7910
[2017-12-15 14:32:20] Epoch 0002 mean train/dev loss: 316.6045 / 283.8967
[2017-12-15 14:32:26] Epoch 0003 mean train/dev loss: 251.2837 / 237.1539
[2017-12-15 14:32:31] Epoch 0004 mean train/dev loss: 238.3570 / 248.0296
[2017-12-15 14:32:37] Epoch 0005 mean train/dev loss: 234.2429 / 240.5261
[2017-12-15 14:32:43] Epoch 0006 mean train/dev loss: 234.5978 / 215.9140
[2017-12-15 14:32:49] Epoch 0007 mean train/dev loss: 232.7778 / 215.9091
[2017-12-15 14:32:55] Epoch 0008 mean train/dev loss: 234.8439 / 244.3018
[2017-12-15 14:33:02] Epoch 0009 mean train/dev loss: 242.2102 / 222.2239
[2017-12-15 14:33:07] Epoch 0010 mean train/dev loss: 233.9584 / 236.3449
[2017-12-15 14:33:07] Checkpointing model at epoch 10 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:33:08] Model Checkpointing finished.
[2017-12-15 14:33:14] Epoch 0011 mean train/dev loss: 240.0921 / 251.6868
[2017-12-15 14:33:19] Epoch 0012 mean train/dev loss: 234.2377 / 216.4155
[2017-12-15 14:33:25] Epoch 0013 mean train/dev loss: 234.3885 / 226.4487
[2017-12-15 14:33:31] Epoch 0014 mean train/dev loss: 234.5884 / 237.9653
[2017-12-15 14:33:37] Epoch 0015 mean train/dev loss: 239.2789 / 222.2867
[2017-12-15 14:33:37] Learning rate decayed by 0.5000
[2017-12-15 14:33:43] Epoch 0016 mean train/dev loss: 226.8643 / 225.4329
[2017-12-15 14:33:49] Epoch 0017 mean train/dev loss: 228.4591 / 230.2518
[2017-12-15 14:33:55] Epoch 0018 mean train/dev loss: 228.5388 / 198.4446
[2017-12-15 14:34:01] Epoch 0019 mean train/dev loss: 229.0821 / 221.1598
[2017-12-15 14:34:07] Epoch 0020 mean train/dev loss: 228.4899 / 221.1685
[2017-12-15 14:34:07] Checkpointing model at epoch 20 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:34:08] Model Checkpointing finished.
[2017-12-15 14:34:14] Epoch 0021 mean train/dev loss: 228.7822 / 212.4488
[2017-12-15 14:34:20] Epoch 0022 mean train/dev loss: 229.1155 / 243.9817
[2017-12-15 14:34:26] Epoch 0023 mean train/dev loss: 228.6729 / 217.3989
[2017-12-15 14:34:32] Epoch 0024 mean train/dev loss: 229.7032 / 210.8737
[2017-12-15 14:34:38] Epoch 0025 mean train/dev loss: 228.3044 / 207.3143
[2017-12-15 14:34:43] Epoch 0026 mean train/dev loss: 228.9245 / 228.8466
[2017-12-15 14:34:49] Epoch 0027 mean train/dev loss: 228.1260 / 227.5728
[2017-12-15 14:34:56] Epoch 0028 mean train/dev loss: 228.2931 / 202.2783
[2017-12-15 14:35:01] Epoch 0029 mean train/dev loss: 228.0008 / 220.8714
[2017-12-15 14:35:07] Epoch 0030 mean train/dev loss: 230.0570 / 234.0151
[2017-12-15 14:35:07] Learning rate decayed by 0.5000
[2017-12-15 14:35:07] Checkpointing model at epoch 30 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:35:07] Model Checkpointing finished.
[2017-12-15 14:35:13] Epoch 0031 mean train/dev loss: 225.3583 / 211.1611
[2017-12-15 14:35:19] Epoch 0032 mean train/dev loss: 224.9612 / 232.9948
[2017-12-15 14:35:25] Epoch 0033 mean train/dev loss: 225.6046 / 233.6019
[2017-12-15 14:35:31] Epoch 0034 mean train/dev loss: 225.4939 / 212.2648
[2017-12-15 14:35:37] Epoch 0035 mean train/dev loss: 225.8451 / 206.6867
[2017-12-15 14:35:42] Epoch 0036 mean train/dev loss: 225.4799 / 222.8789
[2017-12-15 14:35:47] Epoch 0037 mean train/dev loss: 225.9789 / 204.9596
[2017-12-15 14:35:54] Epoch 0038 mean train/dev loss: 225.7585 / 226.3528
[2017-12-15 14:36:00] Epoch 0039 mean train/dev loss: 225.8129 / 205.2882
[2017-12-15 14:36:06] Epoch 0040 mean train/dev loss: 225.6088 / 215.1741
[2017-12-15 14:36:06] Checkpointing model at epoch 40 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:36:06] Model Checkpointing finished.
[2017-12-15 14:36:12] Epoch 0041 mean train/dev loss: 226.0649 / 209.8732
[2017-12-15 14:36:18] Epoch 0042 mean train/dev loss: 225.8653 / 208.2105
[2017-12-15 14:36:24] Epoch 0043 mean train/dev loss: 225.7854 / 202.7209
[2017-12-15 14:36:31] Epoch 0044 mean train/dev loss: 225.2632 / 235.8329
[2017-12-15 14:36:36] Epoch 0045 mean train/dev loss: 226.1856 / 222.3953
[2017-12-15 14:36:36] Learning rate decayed by 0.5000
[2017-12-15 14:36:42] Epoch 0046 mean train/dev loss: 223.6552 / 212.8469
[2017-12-15 14:36:48] Epoch 0047 mean train/dev loss: 223.7522 / 214.1158
[2017-12-15 14:36:54] Epoch 0048 mean train/dev loss: 223.9819 / 215.0880
[2017-12-15 14:37:00] Epoch 0049 mean train/dev loss: 224.1322 / 210.8693
[2017-12-15 14:37:06] Epoch 0050 mean train/dev loss: 224.1945 / 203.3471
[2017-12-15 14:37:06] Checkpointing model at epoch 50 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:37:07] Model Checkpointing finished.
[2017-12-15 14:37:13] Epoch 0051 mean train/dev loss: 224.3201 / 204.7442
[2017-12-15 14:37:19] Epoch 0052 mean train/dev loss: 224.0461 / 208.8369
[2017-12-15 14:37:25] Epoch 0053 mean train/dev loss: 224.0456 / 215.8158
[2017-12-15 14:37:31] Epoch 0054 mean train/dev loss: 224.4016 / 214.4613
[2017-12-15 14:37:37] Epoch 0055 mean train/dev loss: 224.1287 / 212.0089
[2017-12-15 14:37:43] Epoch 0056 mean train/dev loss: 224.0383 / 209.9553
[2017-12-15 14:37:49] Epoch 0057 mean train/dev loss: 224.1269 / 229.0000
[2017-12-15 14:37:56] Epoch 0058 mean train/dev loss: 224.3640 / 195.9571
[2017-12-15 14:38:02] Epoch 0059 mean train/dev loss: 223.8893 / 208.1774
[2017-12-15 14:38:07] Epoch 0060 mean train/dev loss: 224.1157 / 217.3270
[2017-12-15 14:38:07] Learning rate decayed by 0.5000
[2017-12-15 14:38:07] Checkpointing model at epoch 60 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:38:07] Model Checkpointing finished.
[2017-12-15 14:38:14] Epoch 0061 mean train/dev loss: 222.9903 / 217.1886
[2017-12-15 14:38:20] Epoch 0062 mean train/dev loss: 223.0792 / 217.1284
[2017-12-15 14:38:26] Epoch 0063 mean train/dev loss: 223.2188 / 214.9625
[2017-12-15 14:38:31] Epoch 0064 mean train/dev loss: 223.1479 / 205.4357
[2017-12-15 14:38:37] Epoch 0065 mean train/dev loss: 223.1580 / 204.8825
[2017-12-15 14:38:43] Epoch 0066 mean train/dev loss: 223.1336 / 212.5564
[2017-12-15 14:38:49] Epoch 0067 mean train/dev loss: 223.0451 / 212.8300
[2017-12-15 14:38:55] Epoch 0068 mean train/dev loss: 223.2503 / 213.5020
[2017-12-15 14:39:01] Epoch 0069 mean train/dev loss: 223.1178 / 206.6968
[2017-12-15 14:39:07] Epoch 0070 mean train/dev loss: 223.0911 / 224.5659
[2017-12-15 14:39:07] Checkpointing model at epoch 70 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:39:07] Model Checkpointing finished.
[2017-12-15 14:39:13] Epoch 0071 mean train/dev loss: 223.2227 / 219.2827
[2017-12-15 14:39:20] Epoch 0072 mean train/dev loss: 223.2305 / 214.4955
[2017-12-15 14:39:26] Epoch 0073 mean train/dev loss: 223.4648 / 205.4680
[2017-12-15 14:39:32] Epoch 0074 mean train/dev loss: 223.0732 / 213.6553
[2017-12-15 14:39:38] Epoch 0075 mean train/dev loss: 223.1701 / 210.1907
[2017-12-15 14:39:38] Learning rate decayed by 0.5000
[2017-12-15 14:39:44] Epoch 0076 mean train/dev loss: 222.5912 / 212.9047
[2017-12-15 14:39:50] Epoch 0077 mean train/dev loss: 222.7081 / 213.0627
[2017-12-15 14:39:56] Epoch 0078 mean train/dev loss: 222.6690 / 212.9052
[2017-12-15 14:40:02] Epoch 0079 mean train/dev loss: 222.7351 / 214.7684
[2017-12-15 14:40:09] Epoch 0080 mean train/dev loss: 222.6184 / 211.8164
[2017-12-15 14:40:09] Checkpointing model at epoch 80 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:40:09] Model Checkpointing finished.
[2017-12-15 14:40:16] Epoch 0081 mean train/dev loss: 222.7200 / 212.3796
[2017-12-15 14:40:22] Epoch 0082 mean train/dev loss: 222.7488 / 210.6491
[2017-12-15 14:40:28] Epoch 0083 mean train/dev loss: 222.7572 / 207.5825
[2017-12-15 14:40:34] Epoch 0084 mean train/dev loss: 222.7256 / 209.0251
[2017-12-15 14:40:42] Epoch 0085 mean train/dev loss: 222.7781 / 212.4813
[2017-12-15 14:40:47] Epoch 0086 mean train/dev loss: 222.4881 / 213.4477
[2017-12-15 14:40:52] Epoch 0087 mean train/dev loss: 222.5734 / 213.6265
[2017-12-15 14:40:56] Epoch 0088 mean train/dev loss: 222.8440 / 214.4577
[2017-12-15 14:41:01] Epoch 0089 mean train/dev loss: 222.7241 / 213.4785
[2017-12-15 14:41:06] Epoch 0090 mean train/dev loss: 222.6367 / 210.4260
[2017-12-15 14:41:06] Learning rate decayed by 0.5000
[2017-12-15 14:41:06] Checkpointing model at epoch 90 for ffn.hl_20.lr_0.1.wd_10
[2017-12-15 14:41:06] Model Checkpointing finished.
[2017-12-15 14:41:11] Epoch 0091 mean train/dev loss: 222.4794 / 210.4048
[2017-12-15 14:41:15] Epoch 0092 mean train/dev loss: 222.3590 / 208.5806
[2017-12-15 14:41:20] Epoch 0093 mean train/dev loss: 222.3542 / 207.7509
[2017-12-15 14:41:25] Epoch 0094 mean train/dev loss: 222.4133 / 211.3717
[2017-12-15 14:41:29] Epoch 0095 mean train/dev loss: 222.2859 / 211.7008
[2017-12-15 14:41:34] Epoch 0096 mean train/dev loss: 222.5654 / 211.7624
[2017-12-15 14:41:39] Epoch 0097 mean train/dev loss: 222.4542 / 209.9663
[2017-12-15 14:41:44] Epoch 0098 mean train/dev loss: 222.3218 / 209.6791
[2017-12-15 14:41:49] Epoch 0099 mean train/dev loss: 222.3592 / 210.9520
[2017-12-15 14:41:49] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:41:49] 
                       *** Training finished *** 
[2017-12-15 14:41:50] Dev MSE: 210.9520
[2017-12-15 14:41:54] Training MSE: 223.9081
[2017-12-15 14:41:55] Experiment ffn.hl_20.lr_0.1.wd_10 logging ended.
