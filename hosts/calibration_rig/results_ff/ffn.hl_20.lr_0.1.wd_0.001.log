[2017-12-15 14:04:00] Experiment ffn.hl_20.lr_0.1.wd_0.001 logging started.
[2017-12-15 14:04:00] 
                       *** Starting Experiment ffn.hl_20.lr_0.1.wd_0.001 ***
                      
[2017-12-15 14:04:00] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 14:04:02] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 1)
                      )
[2017-12-15 14:04:02]  *** Training on GPU ***
[2017-12-15 14:04:09] Epoch 0001 mean train/dev loss: 15475.6282 / 406.1018
[2017-12-15 14:04:15] Epoch 0002 mean train/dev loss: 245.7245 / 261.4427
[2017-12-15 14:04:23] Epoch 0003 mean train/dev loss: 176.6042 / 206.6922
[2017-12-15 14:04:30] Epoch 0004 mean train/dev loss: 146.4202 / 163.4507
[2017-12-15 14:04:36] Epoch 0005 mean train/dev loss: 129.4405 / 151.5803
[2017-12-15 14:04:43] Epoch 0006 mean train/dev loss: 120.3064 / 135.6901
[2017-12-15 14:04:50] Epoch 0007 mean train/dev loss: 117.6891 / 129.1814
[2017-12-15 14:04:57] Epoch 0008 mean train/dev loss: 116.3953 / 135.2501
[2017-12-15 14:05:04] Epoch 0009 mean train/dev loss: 115.8428 / 120.3793
[2017-12-15 14:05:11] Epoch 0010 mean train/dev loss: 116.6649 / 125.4814
[2017-12-15 14:05:11] Checkpointing model at epoch 10 for ffn.hl_20.lr_0.1.wd_0.001
[2017-12-15 14:05:12] Model Checkpointing finished.
[2017-12-15 14:05:19] Epoch 0011 mean train/dev loss: 116.2316 / 187.8135
[2017-12-15 14:05:26] Epoch 0012 mean train/dev loss: 115.4210 / 159.2401
[2017-12-15 14:05:33] Epoch 0013 mean train/dev loss: 114.7566 / 120.2773
[2017-12-15 14:05:40] Epoch 0014 mean train/dev loss: 113.0636 / 124.2781
[2017-12-15 14:05:47] Epoch 0015 mean train/dev loss: 114.1609 / 136.9038
[2017-12-15 14:05:47] Learning rate decayed by 0.5000
[2017-12-15 14:05:55] Epoch 0016 mean train/dev loss: 109.7592 / 128.6366
[2017-12-15 14:06:02] Epoch 0017 mean train/dev loss: 110.7346 / 168.2417
[2017-12-15 14:06:09] Epoch 0018 mean train/dev loss: 109.9314 / 146.1447
[2017-12-15 14:06:16] Epoch 0019 mean train/dev loss: 110.2975 / 163.4190
[2017-12-15 14:06:23] Epoch 0020 mean train/dev loss: 110.2990 / 129.9468
[2017-12-15 14:06:23] Checkpointing model at epoch 20 for ffn.hl_20.lr_0.1.wd_0.001
[2017-12-15 14:06:23] Model Checkpointing finished.
[2017-12-15 14:06:30] Epoch 0021 mean train/dev loss: 110.0931 / 151.6607
[2017-12-15 14:06:37] Epoch 0022 mean train/dev loss: 110.2890 / 179.1119
[2017-12-15 14:06:44] Epoch 0023 mean train/dev loss: 110.2882 / 122.2081
[2017-12-15 14:06:51] Epoch 0024 mean train/dev loss: 109.7570 / 139.0349
[2017-12-15 14:06:58] Epoch 0025 mean train/dev loss: 110.0218 / 124.4595
[2017-12-15 14:07:06] Epoch 0026 mean train/dev loss: 109.7683 / 133.9598
[2017-12-15 14:07:13] Epoch 0027 mean train/dev loss: 109.8928 / 161.8474
[2017-12-15 14:07:19] Epoch 0028 mean train/dev loss: 109.7608 / 142.5553
[2017-12-15 14:07:27] Epoch 0029 mean train/dev loss: 108.5606 / 148.8775
[2017-12-15 14:07:34] Epoch 0030 mean train/dev loss: 106.3239 / 124.8351
[2017-12-15 14:07:34] Learning rate decayed by 0.5000
[2017-12-15 14:07:34] Checkpointing model at epoch 30 for ffn.hl_20.lr_0.1.wd_0.001
[2017-12-15 14:07:34] Model Checkpointing finished.
[2017-12-15 14:07:41] Epoch 0031 mean train/dev loss: 104.4050 / 173.6788
[2017-12-15 14:07:48] Epoch 0032 mean train/dev loss: 104.6324 / 172.6329
[2017-12-15 14:07:55] Epoch 0033 mean train/dev loss: 104.5651 / 173.1969
[2017-12-15 14:08:03] Epoch 0034 mean train/dev loss: 104.4671 / 151.2880
[2017-12-15 14:08:10] Epoch 0035 mean train/dev loss: 104.4004 / 160.9338
[2017-12-15 14:08:17] Epoch 0036 mean train/dev loss: 104.6727 / 180.0023
[2017-12-15 14:08:24] Epoch 0037 mean train/dev loss: 104.5029 / 167.0912
[2017-12-15 14:08:31] Epoch 0038 mean train/dev loss: 104.4437 / 194.6834
[2017-12-15 14:08:38] Epoch 0039 mean train/dev loss: 104.6302 / 164.3836
[2017-12-15 14:08:45] Epoch 0040 mean train/dev loss: 104.3391 / 155.7355
[2017-12-15 14:08:45] Checkpointing model at epoch 40 for ffn.hl_20.lr_0.1.wd_0.001
[2017-12-15 14:08:45] Model Checkpointing finished.
[2017-12-15 14:08:52] Epoch 0041 mean train/dev loss: 104.3990 / 161.3948
[2017-12-15 14:08:59] Epoch 0042 mean train/dev loss: 104.2929 / 178.6733
[2017-12-15 14:09:06] Epoch 0043 mean train/dev loss: 104.1420 / 168.8681
[2017-12-15 14:09:14] Epoch 0044 mean train/dev loss: 104.2335 / 172.3914
[2017-12-15 14:09:20] Epoch 0045 mean train/dev loss: 104.3372 / 164.8024
[2017-12-15 14:09:20] Learning rate decayed by 0.5000
[2017-12-15 14:09:28] Epoch 0046 mean train/dev loss: 103.2911 / 175.3721
[2017-12-15 14:09:35] Epoch 0047 mean train/dev loss: 103.3701 / 153.7251
[2017-12-15 14:09:42] Epoch 0048 mean train/dev loss: 103.3450 / 186.8680
[2017-12-15 14:09:49] Epoch 0049 mean train/dev loss: 103.3200 / 175.9691
[2017-12-15 14:09:56] Epoch 0050 mean train/dev loss: 103.3830 / 196.0330
[2017-12-15 14:09:56] Checkpointing model at epoch 50 for ffn.hl_20.lr_0.1.wd_0.001
[2017-12-15 14:09:56] Model Checkpointing finished.
[2017-12-15 14:10:03] Epoch 0051 mean train/dev loss: 103.2817 / 171.6448
[2017-12-15 14:10:10] Epoch 0052 mean train/dev loss: 103.3824 / 179.4731
[2017-12-15 14:10:18] Epoch 0053 mean train/dev loss: 103.3401 / 184.5459
[2017-12-15 14:10:25] Epoch 0054 mean train/dev loss: 103.2591 / 174.7788
[2017-12-15 14:10:25] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:10:25] 
                       *** Training finished *** 
[2017-12-15 14:10:27] Dev MSE: 174.7788
[2017-12-15 14:10:33] Training MSE: 102.7476
[2017-12-15 14:10:35] Experiment ffn.hl_20.lr_0.1.wd_0.001 logging ended.
