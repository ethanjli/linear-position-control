[2017-12-15 16:30:20] Experiment ffn.hl_linear.lr_0.01.wd_0.1 logging started.
[2017-12-15 16:30:20] 
                       *** Starting Experiment ffn.hl_linear.lr_0.01.wd_0.1 ***
                      
[2017-12-15 16:30:20] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] []  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 16:30:20] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 1)
                      )
[2017-12-15 16:30:20]  *** Training on GPU ***
[2017-12-15 16:30:26] Epoch 0001 mean train/dev loss: 329476.2736 / 326454.8125
[2017-12-15 16:30:34] Epoch 0002 mean train/dev loss: 318636.5918 / 315482.7188
[2017-12-15 16:30:41] Epoch 0003 mean train/dev loss: 308321.5496 / 305038.0625
[2017-12-15 16:30:48] Epoch 0004 mean train/dev loss: 298429.8503 / 295061.4062
[2017-12-15 16:30:54] Epoch 0005 mean train/dev loss: 289081.0534 / 285520.7812
[2017-12-15 16:31:01] Epoch 0006 mean train/dev loss: 280082.1783 / 276381.9062
[2017-12-15 16:31:08] Epoch 0007 mean train/dev loss: 271449.0746 / 267628.1250
[2017-12-15 16:31:15] Epoch 0008 mean train/dev loss: 263201.8264 / 259232.4688
[2017-12-15 16:31:22] Epoch 0009 mean train/dev loss: 255296.7784 / 251194.3594
[2017-12-15 16:31:29] Epoch 0010 mean train/dev loss: 247714.1313 / 243481.7656
[2017-12-15 16:31:29] Checkpointing model at epoch 10 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:31:29] Model Checkpointing finished.
[2017-12-15 16:31:35] Epoch 0011 mean train/dev loss: 240417.3612 / 236093.0938
[2017-12-15 16:31:42] Epoch 0012 mean train/dev loss: 233416.2521 / 229005.4688
[2017-12-15 16:31:50] Epoch 0013 mean train/dev loss: 226672.0129 / 222199.6719
[2017-12-15 16:31:56] Epoch 0014 mean train/dev loss: 220212.0282 / 215656.1562
[2017-12-15 16:32:03] Epoch 0015 mean train/dev loss: 213933.8074 / 209341.5938
[2017-12-15 16:32:03] Learning rate decayed by 0.5000
[2017-12-15 16:32:10] Epoch 0016 mean train/dev loss: 209320.4712 / 206244.1094
[2017-12-15 16:32:17] Epoch 0017 mean train/dev loss: 206332.3449 / 203173.4062
[2017-12-15 16:32:23] Epoch 0018 mean train/dev loss: 203273.3584 / 200135.9844
[2017-12-15 16:32:31] Epoch 0019 mean train/dev loss: 200333.1381 / 197142.7031
[2017-12-15 16:32:37] Epoch 0020 mean train/dev loss: 197354.7703 / 194187.5000
[2017-12-15 16:32:37] Checkpointing model at epoch 20 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:32:37] Model Checkpointing finished.
[2017-12-15 16:32:44] Epoch 0021 mean train/dev loss: 194475.4653 / 191266.4844
[2017-12-15 16:32:51] Epoch 0022 mean train/dev loss: 191590.6164 / 188382.1562
[2017-12-15 16:32:58] Epoch 0023 mean train/dev loss: 188727.9295 / 185530.3594
[2017-12-15 16:33:05] Epoch 0024 mean train/dev loss: 185908.3513 / 182712.8750
[2017-12-15 16:33:12] Epoch 0025 mean train/dev loss: 183105.0809 / 179926.8906
[2017-12-15 16:33:20] Epoch 0026 mean train/dev loss: 180333.3861 / 177172.0625
[2017-12-15 16:33:27] Epoch 0027 mean train/dev loss: 177591.0203 / 174443.1406
[2017-12-15 16:33:34] Epoch 0028 mean train/dev loss: 174869.0024 / 171749.6562
[2017-12-15 16:33:41] Epoch 0029 mean train/dev loss: 172204.0064 / 169083.1094
[2017-12-15 16:33:48] Epoch 0030 mean train/dev loss: 169514.4476 / 166445.2812
[2017-12-15 16:33:48] Learning rate decayed by 0.5000
[2017-12-15 16:33:48] Checkpointing model at epoch 30 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:33:49] Model Checkpointing finished.
[2017-12-15 16:33:56] Epoch 0031 mean train/dev loss: 167532.0062 / 165132.8594
[2017-12-15 16:34:03] Epoch 0032 mean train/dev loss: 166203.6938 / 163825.0000
[2017-12-15 16:34:11] Epoch 0033 mean train/dev loss: 164903.7668 / 162519.0156
[2017-12-15 16:34:18] Epoch 0034 mean train/dev loss: 163604.7255 / 161219.8750
[2017-12-15 16:34:25] Epoch 0035 mean train/dev loss: 162291.3970 / 159926.8125
[2017-12-15 16:34:32] Epoch 0036 mean train/dev loss: 160986.4814 / 158638.9219
[2017-12-15 16:34:40] Epoch 0037 mean train/dev loss: 159705.3237 / 157355.5625
[2017-12-15 16:34:47] Epoch 0038 mean train/dev loss: 158398.7344 / 156082.5156
[2017-12-15 16:34:54] Epoch 0039 mean train/dev loss: 157133.6824 / 154814.7500
[2017-12-15 16:35:01] Epoch 0040 mean train/dev loss: 155858.8545 / 153554.0000
[2017-12-15 16:35:01] Checkpointing model at epoch 40 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:35:02] Model Checkpointing finished.
[2017-12-15 16:35:08] Epoch 0041 mean train/dev loss: 154566.0835 / 152298.8750
[2017-12-15 16:35:15] Epoch 0042 mean train/dev loss: 153313.0366 / 151050.8125
[2017-12-15 16:35:21] Epoch 0043 mean train/dev loss: 152048.4048 / 149807.4062
[2017-12-15 16:35:28] Epoch 0044 mean train/dev loss: 150800.7997 / 148569.9688
[2017-12-15 16:35:35] Epoch 0045 mean train/dev loss: 149571.4222 / 147337.8906
[2017-12-15 16:35:35] Learning rate decayed by 0.5000
[2017-12-15 16:35:43] Epoch 0046 mean train/dev loss: 148625.5860 / 146724.5156
[2017-12-15 16:35:50] Epoch 0047 mean train/dev loss: 148024.8517 / 146111.7031
[2017-12-15 16:35:58] Epoch 0048 mean train/dev loss: 147392.8700 / 145497.9844
[2017-12-15 16:36:05] Epoch 0049 mean train/dev loss: 146778.7557 / 144887.2656
[2017-12-15 16:36:12] Epoch 0050 mean train/dev loss: 146164.7629 / 144276.5000
[2017-12-15 16:36:12] Checkpointing model at epoch 50 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:36:12] Model Checkpointing finished.
[2017-12-15 16:36:19] Epoch 0051 mean train/dev loss: 145547.3459 / 143667.4375
[2017-12-15 16:36:25] Epoch 0052 mean train/dev loss: 144925.7596 / 143060.6406
[2017-12-15 16:36:32] Epoch 0053 mean train/dev loss: 144325.1167 / 142454.7656
[2017-12-15 16:36:39] Epoch 0054 mean train/dev loss: 143702.4577 / 141849.7344
[2017-12-15 16:36:46] Epoch 0055 mean train/dev loss: 143089.6893 / 141246.0781
[2017-12-15 16:36:52] Epoch 0056 mean train/dev loss: 142484.5544 / 140644.0156
[2017-12-15 16:37:00] Epoch 0057 mean train/dev loss: 141847.5453 / 140044.0781
[2017-12-15 16:37:06] Epoch 0058 mean train/dev loss: 141271.4923 / 139444.7812
[2017-12-15 16:37:13] Epoch 0059 mean train/dev loss: 140661.7558 / 138847.0312
[2017-12-15 16:37:17] Epoch 0060 mean train/dev loss: 140058.0418 / 138249.6250
[2017-12-15 16:37:17] Learning rate decayed by 0.5000
[2017-12-15 16:37:17] Checkpointing model at epoch 60 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:37:18] Model Checkpointing finished.
[2017-12-15 16:37:25] Epoch 0061 mean train/dev loss: 139608.5353 / 137950.6875
[2017-12-15 16:37:31] Epoch 0062 mean train/dev loss: 139325.1551 / 137652.1094
[2017-12-15 16:37:38] Epoch 0063 mean train/dev loss: 139001.8479 / 137353.8906
[2017-12-15 16:37:45] Epoch 0064 mean train/dev loss: 138713.4560 / 137055.9844
[2017-12-15 16:37:53] Epoch 0065 mean train/dev loss: 138403.3239 / 136758.0156
[2017-12-15 16:38:00] Epoch 0066 mean train/dev loss: 138089.5810 / 136460.6719
[2017-12-15 16:38:08] Epoch 0067 mean train/dev loss: 137809.0464 / 136163.7344
[2017-12-15 16:38:15] Epoch 0068 mean train/dev loss: 137501.5304 / 135866.9688
[2017-12-15 16:38:22] Epoch 0069 mean train/dev loss: 137214.6053 / 135570.6406
[2017-12-15 16:38:29] Epoch 0070 mean train/dev loss: 136913.6956 / 135274.9375
[2017-12-15 16:38:29] Checkpointing model at epoch 70 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:38:29] Model Checkpointing finished.
[2017-12-15 16:38:36] Epoch 0071 mean train/dev loss: 136614.2241 / 134979.0312
[2017-12-15 16:38:43] Epoch 0072 mean train/dev loss: 136304.3927 / 134683.8906
[2017-12-15 16:38:51] Epoch 0073 mean train/dev loss: 136013.2783 / 134388.8594
[2017-12-15 16:38:59] Epoch 0074 mean train/dev loss: 135723.0655 / 134094.3281
[2017-12-15 16:39:06] Epoch 0075 mean train/dev loss: 135417.7457 / 133799.9062
[2017-12-15 16:39:06] Learning rate decayed by 0.5000
[2017-12-15 16:39:12] Epoch 0076 mean train/dev loss: 135198.6985 / 133654.9062
[2017-12-15 16:39:18] Epoch 0077 mean train/dev loss: 135048.2469 / 133509.7031
[2017-12-15 16:39:24] Epoch 0078 mean train/dev loss: 134887.5142 / 133364.4844
[2017-12-15 16:39:30] Epoch 0079 mean train/dev loss: 134755.9459 / 133219.5781
[2017-12-15 16:39:35] Epoch 0080 mean train/dev loss: 134618.1776 / 133074.7656
[2017-12-15 16:39:35] Checkpointing model at epoch 80 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:39:35] Model Checkpointing finished.
[2017-12-15 16:39:42] Epoch 0081 mean train/dev loss: 134463.6825 / 132929.8125
[2017-12-15 16:39:49] Epoch 0082 mean train/dev loss: 134319.4286 / 132785.1562
[2017-12-15 16:39:55] Epoch 0083 mean train/dev loss: 134172.7164 / 132640.1875
[2017-12-15 16:40:00] Epoch 0084 mean train/dev loss: 134015.6354 / 132495.3750
[2017-12-15 16:40:07] Epoch 0085 mean train/dev loss: 133895.0220 / 132350.6406
[2017-12-15 16:40:14] Epoch 0086 mean train/dev loss: 133726.4533 / 132206.3906
[2017-12-15 16:40:21] Epoch 0087 mean train/dev loss: 133587.4168 / 132061.9531
[2017-12-15 16:40:26] Epoch 0088 mean train/dev loss: 133432.9371 / 131917.4062
[2017-12-15 16:40:34] Epoch 0089 mean train/dev loss: 133288.2599 / 131773.0156
[2017-12-15 16:40:41] Epoch 0090 mean train/dev loss: 133157.8131 / 131628.9062
[2017-12-15 16:40:41] Learning rate decayed by 0.5000
[2017-12-15 16:40:41] Checkpointing model at epoch 90 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:40:41] Model Checkpointing finished.
[2017-12-15 16:40:47] Epoch 0091 mean train/dev loss: 133045.7338 / 131557.5938
[2017-12-15 16:40:51] Epoch 0092 mean train/dev loss: 132968.3652 / 131486.2812
[2017-12-15 16:40:56] Epoch 0093 mean train/dev loss: 132893.3697 / 131414.9375
[2017-12-15 16:41:01] Epoch 0094 mean train/dev loss: 132816.4195 / 131343.6562
[2017-12-15 16:41:08] Epoch 0095 mean train/dev loss: 132759.9367 / 131272.4375
[2017-12-15 16:41:14] Epoch 0096 mean train/dev loss: 132667.9987 / 131201.1094
[2017-12-15 16:41:20] Epoch 0097 mean train/dev loss: 132623.8345 / 131129.7969
[2017-12-15 16:41:25] Epoch 0098 mean train/dev loss: 132511.4231 / 131058.4297
[2017-12-15 16:41:30] Epoch 0099 mean train/dev loss: 132442.2362 / 130987.2031
[2017-12-15 16:41:37] Epoch 0100 mean train/dev loss: 132401.7057 / 130916.0391
[2017-12-15 16:41:37] Checkpointing model at epoch 100 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:41:37] Model Checkpointing finished.
[2017-12-15 16:41:43] Epoch 0101 mean train/dev loss: 132314.8408 / 130844.8750
[2017-12-15 16:41:48] Epoch 0102 mean train/dev loss: 132235.8341 / 130773.7344
[2017-12-15 16:41:53] Epoch 0103 mean train/dev loss: 132177.0813 / 130702.6016
[2017-12-15 16:42:01] Epoch 0104 mean train/dev loss: 132109.2352 / 130631.5547
[2017-12-15 16:42:07] Epoch 0105 mean train/dev loss: 132015.3552 / 130560.5312
[2017-12-15 16:42:07] Learning rate decayed by 0.5000
[2017-12-15 16:42:12] Epoch 0106 mean train/dev loss: 131988.5935 / 130524.9766
[2017-12-15 16:42:17] Epoch 0107 mean train/dev loss: 131928.8442 / 130489.4766
[2017-12-15 16:42:21] Epoch 0108 mean train/dev loss: 131908.0420 / 130453.9375
[2017-12-15 16:42:27] Epoch 0109 mean train/dev loss: 131863.1422 / 130418.4844
[2017-12-15 16:42:34] Epoch 0110 mean train/dev loss: 131840.4492 / 130382.9375
[2017-12-15 16:42:34] Checkpointing model at epoch 110 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:42:34] Model Checkpointing finished.
[2017-12-15 16:42:40] Epoch 0111 mean train/dev loss: 131793.7628 / 130347.5547
[2017-12-15 16:42:48] Epoch 0112 mean train/dev loss: 131758.5848 / 130312.0078
[2017-12-15 16:42:53] Epoch 0113 mean train/dev loss: 131716.1738 / 130276.5391
[2017-12-15 16:43:00] Epoch 0114 mean train/dev loss: 131682.3159 / 130241.0312
[2017-12-15 16:43:08] Epoch 0115 mean train/dev loss: 131653.0260 / 130205.4844
[2017-12-15 16:43:12] Epoch 0116 mean train/dev loss: 131622.7598 / 130170.0312
[2017-12-15 16:43:19] Epoch 0117 mean train/dev loss: 131583.4854 / 130134.5391
[2017-12-15 16:43:27] Epoch 0118 mean train/dev loss: 131538.7227 / 130099.0078
[2017-12-15 16:43:33] Epoch 0119 mean train/dev loss: 131512.6889 / 130063.6094
[2017-12-15 16:43:41] Epoch 0120 mean train/dev loss: 131467.5811 / 130028.1016
[2017-12-15 16:43:41] Learning rate decayed by 0.5000
[2017-12-15 16:43:41] Checkpointing model at epoch 120 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:43:41] Model Checkpointing finished.
[2017-12-15 16:43:48] Epoch 0121 mean train/dev loss: 131440.8435 / 130007.3984
[2017-12-15 16:43:53] Epoch 0122 mean train/dev loss: 131431.3605 / 129986.7578
[2017-12-15 16:43:57] Epoch 0123 mean train/dev loss: 131396.6406 / 129966.0312
[2017-12-15 16:44:02] Epoch 0124 mean train/dev loss: 131379.9681 / 129945.3750
[2017-12-15 16:44:07] Epoch 0125 mean train/dev loss: 131358.4705 / 129924.6719
[2017-12-15 16:44:12] Epoch 0126 mean train/dev loss: 131339.0695 / 129904.0312
[2017-12-15 16:44:17] Epoch 0127 mean train/dev loss: 131309.1717 / 129883.3984
[2017-12-15 16:44:22] Epoch 0128 mean train/dev loss: 131293.5254 / 129862.7109
[2017-12-15 16:44:28] Epoch 0129 mean train/dev loss: 131277.0726 / 129842.0234
[2017-12-15 16:44:33] Epoch 0130 mean train/dev loss: 131245.8561 / 129821.3125
[2017-12-15 16:44:33] Checkpointing model at epoch 130 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:44:33] Model Checkpointing finished.
[2017-12-15 16:44:38] Epoch 0131 mean train/dev loss: 131245.1429 / 129800.6562
[2017-12-15 16:44:44] Epoch 0132 mean train/dev loss: 131228.7906 / 129780.0391
[2017-12-15 16:44:50] Epoch 0133 mean train/dev loss: 131194.4624 / 129759.3984
[2017-12-15 16:44:55] Epoch 0134 mean train/dev loss: 131168.5561 / 129738.6719
[2017-12-15 16:44:59] Epoch 0135 mean train/dev loss: 131142.3087 / 129718.0234
[2017-12-15 16:44:59] Learning rate decayed by 0.5000
[2017-12-15 16:45:04] Epoch 0136 mean train/dev loss: 131142.7922 / 129710.8125
[2017-12-15 16:45:09] Epoch 0137 mean train/dev loss: 131134.9745 / 129703.6016
[2017-12-15 16:45:13] Epoch 0138 mean train/dev loss: 131122.9834 / 129696.4141
[2017-12-15 16:45:18] Epoch 0139 mean train/dev loss: 131112.0549 / 129689.1719
[2017-12-15 16:45:24] Epoch 0140 mean train/dev loss: 131109.9563 / 129681.9844
[2017-12-15 16:45:24] Checkpointing model at epoch 140 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:45:24] Model Checkpointing finished.
[2017-12-15 16:45:30] Epoch 0141 mean train/dev loss: 131092.3101 / 129674.7578
[2017-12-15 16:45:37] Epoch 0142 mean train/dev loss: 131096.1943 / 129667.5312
[2017-12-15 16:45:44] Epoch 0143 mean train/dev loss: 131095.1828 / 129660.3203
[2017-12-15 16:45:48] Epoch 0144 mean train/dev loss: 131072.6527 / 129653.1172
[2017-12-15 16:45:53] Epoch 0145 mean train/dev loss: 131080.4877 / 129645.9219
[2017-12-15 16:45:59] Epoch 0146 mean train/dev loss: 131060.7594 / 129638.7031
[2017-12-15 16:46:03] Epoch 0147 mean train/dev loss: 131053.3690 / 129631.5234
[2017-12-15 16:46:08] Epoch 0148 mean train/dev loss: 131052.4599 / 129624.2969
[2017-12-15 16:46:14] Epoch 0149 mean train/dev loss: 131042.2414 / 129617.1250
[2017-12-15 16:46:21] Epoch 0150 mean train/dev loss: 131046.2612 / 129609.8828
[2017-12-15 16:46:21] Learning rate decayed by 0.5000
[2017-12-15 16:46:21] Checkpointing model at epoch 150 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:46:21] Model Checkpointing finished.
[2017-12-15 16:46:29] Epoch 0151 mean train/dev loss: 131033.2687 / 129603.0469
[2017-12-15 16:46:34] Epoch 0152 mean train/dev loss: 131010.2362 / 129596.2031
[2017-12-15 16:46:41] Epoch 0153 mean train/dev loss: 131026.0363 / 129589.3594
[2017-12-15 16:46:46] Epoch 0154 mean train/dev loss: 131002.4741 / 129582.5391
[2017-12-15 16:46:50] Epoch 0155 mean train/dev loss: 130994.6221 / 129575.7109
[2017-12-15 16:46:56] Epoch 0156 mean train/dev loss: 131003.3726 / 129568.8672
[2017-12-15 16:47:03] Epoch 0157 mean train/dev loss: 130989.2900 / 129562.0234
[2017-12-15 16:47:11] Epoch 0158 mean train/dev loss: 130993.3094 / 129555.1875
[2017-12-15 16:47:17] Epoch 0159 mean train/dev loss: 130978.0064 / 129548.3281
[2017-12-15 16:47:25] Epoch 0160 mean train/dev loss: 130978.2204 / 129541.4844
[2017-12-15 16:47:25] Checkpointing model at epoch 160 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:47:25] Model Checkpointing finished.
[2017-12-15 16:47:32] Epoch 0161 mean train/dev loss: 130954.9489 / 129534.6094
[2017-12-15 16:47:39] Epoch 0162 mean train/dev loss: 130952.6392 / 129527.8047
[2017-12-15 16:47:44] Epoch 0163 mean train/dev loss: 130933.5308 / 129520.9609
[2017-12-15 16:47:49] Epoch 0164 mean train/dev loss: 130939.7267 / 129514.1016
[2017-12-15 16:47:55] Epoch 0165 mean train/dev loss: 130918.1170 / 129507.2734
[2017-12-15 16:47:55] Learning rate decayed by 0.5000
[2017-12-15 16:48:02] Epoch 0166 mean train/dev loss: 130926.6813 / 129507.2578
[2017-12-15 16:48:07] Epoch 0167 mean train/dev loss: 130934.8578 / 129507.2578
[2017-12-15 16:48:12] Epoch 0168 mean train/dev loss: 130923.2473 / 129507.2578
[2017-12-15 16:48:17] Epoch 0169 mean train/dev loss: 130928.7916 / 129507.2578
[2017-12-15 16:48:25] Epoch 0170 mean train/dev loss: 130930.9946 / 129507.2578
[2017-12-15 16:48:25] Checkpointing model at epoch 170 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:48:25] Model Checkpointing finished.
[2017-12-15 16:48:32] Epoch 0171 mean train/dev loss: 130926.0197 / 129507.2344
[2017-12-15 16:48:39] Epoch 0172 mean train/dev loss: 130936.0962 / 129507.2422
[2017-12-15 16:48:46] Epoch 0173 mean train/dev loss: 130937.7830 / 129507.2422
[2017-12-15 16:48:53] Epoch 0174 mean train/dev loss: 130932.9521 / 129507.2422
[2017-12-15 16:48:58] Epoch 0175 mean train/dev loss: 130922.5259 / 129507.2344
[2017-12-15 16:49:05] Epoch 0176 mean train/dev loss: 130945.1209 / 129507.2422
[2017-12-15 16:49:10] Epoch 0177 mean train/dev loss: 130933.8074 / 129507.2188
[2017-12-15 16:49:17] Epoch 0178 mean train/dev loss: 130920.5571 / 129507.2344
[2017-12-15 16:49:24] Epoch 0179 mean train/dev loss: 130920.2680 / 129507.2188
[2017-12-15 16:49:29] Epoch 0180 mean train/dev loss: 130931.9145 / 129507.2188
[2017-12-15 16:49:29] Learning rate decayed by 0.5000
[2017-12-15 16:49:29] Checkpointing model at epoch 180 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:49:29] Model Checkpointing finished.
[2017-12-15 16:49:34] Epoch 0181 mean train/dev loss: 130931.8160 / 129507.2344
[2017-12-15 16:49:41] Epoch 0182 mean train/dev loss: 130922.8658 / 129507.2344
[2017-12-15 16:49:48] Epoch 0183 mean train/dev loss: 130924.6925 / 129507.2344
[2017-12-15 16:49:55] Epoch 0184 mean train/dev loss: 130912.9825 / 129507.2031
[2017-12-15 16:50:02] Epoch 0185 mean train/dev loss: 130945.0236 / 129507.2031
[2017-12-15 16:50:09] Epoch 0186 mean train/dev loss: 130935.7213 / 129507.2031
[2017-12-15 16:50:14] Epoch 0187 mean train/dev loss: 130915.2068 / 129507.2344
[2017-12-15 16:50:18] Epoch 0188 mean train/dev loss: 130936.0632 / 129507.2344
[2017-12-15 16:50:24] Epoch 0189 mean train/dev loss: 130921.2545 / 129507.2344
[2017-12-15 16:50:30] Epoch 0190 mean train/dev loss: 130922.9761 / 129507.2344
[2017-12-15 16:50:30] Checkpointing model at epoch 190 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:50:30] Model Checkpointing finished.
[2017-12-15 16:50:36] Epoch 0191 mean train/dev loss: 130919.7339 / 129507.2188
[2017-12-15 16:50:42] Epoch 0192 mean train/dev loss: 130932.7577 / 129507.2031
[2017-12-15 16:50:48] Epoch 0193 mean train/dev loss: 130915.7967 / 129507.2344
[2017-12-15 16:50:53] Epoch 0194 mean train/dev loss: 130924.1142 / 129507.2031
[2017-12-15 16:50:57] Epoch 0195 mean train/dev loss: 130909.1797 / 129507.2031
[2017-12-15 16:50:57] Learning rate decayed by 0.5000
[2017-12-15 16:51:02] Epoch 0196 mean train/dev loss: 130933.2601 / 129507.2031
[2017-12-15 16:51:06] Epoch 0197 mean train/dev loss: 130938.7219 / 129507.2031
[2017-12-15 16:51:10] Epoch 0198 mean train/dev loss: 130931.1442 / 129507.2031
[2017-12-15 16:51:15] Epoch 0199 mean train/dev loss: 130923.5137 / 129507.2031
[2017-12-15 16:51:20] Epoch 0200 mean train/dev loss: 130921.8334 / 129507.2031
[2017-12-15 16:51:20] Checkpointing model at epoch 200 for ffn.hl_linear.lr_0.01.wd_0.1
[2017-12-15 16:51:20] Model Checkpointing finished.
[2017-12-15 16:51:20] 
                       *** Training finished *** 
[2017-12-15 16:51:21] Dev MSE: 129507.2031
[2017-12-15 16:51:25] Training MSE: 130927.7031
[2017-12-15 16:51:25] Experiment ffn.hl_linear.lr_0.01.wd_0.1 logging ended.
