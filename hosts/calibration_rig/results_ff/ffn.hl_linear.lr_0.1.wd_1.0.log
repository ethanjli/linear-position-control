[2017-12-15 14:20:16] Experiment ffn.hl_linear.lr_0.1.wd_1.0 logging started.
[2017-12-15 14:20:16] 
                       *** Starting Experiment ffn.hl_linear.lr_0.1.wd_1.0 ***
                      
[2017-12-15 14:20:16] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] []  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 14:20:19] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 1)
                      )
[2017-12-15 14:20:19]  *** Training on GPU ***
[2017-12-15 14:20:24] Epoch 0001 mean train/dev loss: 287856.8220 / 249127.0938
[2017-12-15 14:20:29] Epoch 0002 mean train/dev loss: 220919.8677 / 195639.7500
[2017-12-15 14:20:33] Epoch 0003 mean train/dev loss: 175774.0480 / 157059.4375
[2017-12-15 14:20:39] Epoch 0004 mean train/dev loss: 141901.8745 / 127924.8906
[2017-12-15 14:20:43] Epoch 0005 mean train/dev loss: 116166.8379 / 105645.2578
[2017-12-15 14:20:49] Epoch 0006 mean train/dev loss: 96643.0003 / 88946.4453
[2017-12-15 14:20:54] Epoch 0007 mean train/dev loss: 82261.4376 / 76955.8359
[2017-12-15 14:20:58] Epoch 0008 mean train/dev loss: 72213.6889 / 68997.5391
[2017-12-15 14:21:03] Epoch 0009 mean train/dev loss: 65857.6110 / 64238.7148
[2017-12-15 14:21:10] Epoch 0010 mean train/dev loss: 62307.5096 / 61873.1836
[2017-12-15 14:21:10] Checkpointing model at epoch 10 for ffn.hl_linear.lr_0.1.wd_1.0
[2017-12-15 14:21:10] Model Checkpointing finished.
[2017-12-15 14:21:15] Epoch 0011 mean train/dev loss: 60758.1974 / 61020.7266
[2017-12-15 14:21:20] Epoch 0012 mean train/dev loss: 60280.1198 / 60752.9492
[2017-12-15 14:21:24] Epoch 0013 mean train/dev loss: 60127.2928 / 60795.3633
[2017-12-15 14:21:29] Epoch 0014 mean train/dev loss: 60126.1833 / 60910.8516
[2017-12-15 14:21:34] Epoch 0015 mean train/dev loss: 60164.3588 / 60808.3633
[2017-12-15 14:21:34] Learning rate decayed by 0.5000
[2017-12-15 14:21:38] Epoch 0016 mean train/dev loss: 60160.2987 / 60754.2031
[2017-12-15 14:21:43] Epoch 0017 mean train/dev loss: 60145.5544 / 60652.5312
[2017-12-15 14:21:50] Epoch 0018 mean train/dev loss: 60129.3067 / 60735.0938
[2017-12-15 14:21:56] Epoch 0019 mean train/dev loss: 60134.4593 / 60633.9531
[2017-12-15 14:22:01] Epoch 0020 mean train/dev loss: 60132.6144 / 60730.4375
[2017-12-15 14:22:01] Checkpointing model at epoch 20 for ffn.hl_linear.lr_0.1.wd_1.0
[2017-12-15 14:22:01] Model Checkpointing finished.
[2017-12-15 14:22:08] Epoch 0021 mean train/dev loss: 60139.8647 / 60640.1094
[2017-12-15 14:22:14] Epoch 0022 mean train/dev loss: 60115.8704 / 60731.1992
[2017-12-15 14:22:21] Epoch 0023 mean train/dev loss: 60130.4163 / 60877.2578
[2017-12-15 14:22:26] Epoch 0024 mean train/dev loss: 60179.5444 / 60605.2344
[2017-12-15 14:22:31] Epoch 0025 mean train/dev loss: 60144.6875 / 60672.5078
[2017-12-15 14:22:35] Epoch 0026 mean train/dev loss: 60118.7997 / 60700.1328
[2017-12-15 14:22:41] Epoch 0027 mean train/dev loss: 60127.4142 / 60753.1406
[2017-12-15 14:22:46] Epoch 0028 mean train/dev loss: 60148.2708 / 60733.3945
[2017-12-15 14:22:53] Epoch 0029 mean train/dev loss: 60153.4460 / 60648.3633
[2017-12-15 14:22:59] Epoch 0030 mean train/dev loss: 60122.5298 / 60788.6016
[2017-12-15 14:22:59] Learning rate decayed by 0.5000
[2017-12-15 14:22:59] Checkpointing model at epoch 30 for ffn.hl_linear.lr_0.1.wd_1.0
[2017-12-15 14:22:59] Model Checkpointing finished.
[2017-12-15 14:23:06] Epoch 0031 mean train/dev loss: 60143.7034 / 60729.0117
[2017-12-15 14:23:11] Epoch 0032 mean train/dev loss: 60133.8163 / 60773.4844
[2017-12-15 14:23:16] Epoch 0033 mean train/dev loss: 60140.4544 / 60717.1641
[2017-12-15 14:23:23] Epoch 0034 mean train/dev loss: 60115.7688 / 60815.3125
[2017-12-15 14:23:30] Epoch 0035 mean train/dev loss: 60173.3860 / 60673.1328
[2017-12-15 14:23:37] Epoch 0036 mean train/dev loss: 60131.0689 / 60649.1953
[2017-12-15 14:23:42] Epoch 0037 mean train/dev loss: 60134.6837 / 60716.3047
[2017-12-15 14:23:48] Epoch 0038 mean train/dev loss: 60129.7177 / 60735.3047
[2017-12-15 14:23:53] Epoch 0039 mean train/dev loss: 60127.0508 / 60737.0508
[2017-12-15 14:23:59] Epoch 0040 mean train/dev loss: 60153.7117 / 60676.4141
[2017-12-15 14:23:59] Checkpointing model at epoch 40 for ffn.hl_linear.lr_0.1.wd_1.0
[2017-12-15 14:23:59] Model Checkpointing finished.
[2017-12-15 14:24:05] Epoch 0041 mean train/dev loss: 60153.2260 / 60548.6680
[2017-12-15 14:24:10] Epoch 0042 mean train/dev loss: 60096.5500 / 60802.3633
[2017-12-15 14:24:17] Epoch 0043 mean train/dev loss: 60144.0514 / 60728.3633
[2017-12-15 14:24:23] Epoch 0044 mean train/dev loss: 60154.4573 / 60690.6133
[2017-12-15 14:24:29] Epoch 0045 mean train/dev loss: 60138.6337 / 60700.5352
[2017-12-15 14:24:29] Learning rate decayed by 0.5000
[2017-12-15 14:24:33] Epoch 0046 mean train/dev loss: 60104.3979 / 60780.9180
[2017-12-15 14:24:39] Epoch 0047 mean train/dev loss: 60116.4144 / 60769.4219
[2017-12-15 14:24:45] Epoch 0048 mean train/dev loss: 60182.3790 / 60692.3242
[2017-12-15 14:24:50] Epoch 0049 mean train/dev loss: 60122.3732 / 60728.0586
[2017-12-15 14:24:55] Epoch 0050 mean train/dev loss: 60134.5061 / 60720.8984
[2017-12-15 14:24:55] Checkpointing model at epoch 50 for ffn.hl_linear.lr_0.1.wd_1.0
[2017-12-15 14:24:56] Model Checkpointing finished.
[2017-12-15 14:25:00] Epoch 0051 mean train/dev loss: 60123.5643 / 60751.4961
[2017-12-15 14:25:05] Epoch 0052 mean train/dev loss: 60193.1361 / 60648.2266
[2017-12-15 14:25:10] Epoch 0053 mean train/dev loss: 60081.1800 / 60738.2109
[2017-12-15 14:25:17] Epoch 0054 mean train/dev loss: 60145.8512 / 60740.8477
[2017-12-15 14:25:24] Epoch 0055 mean train/dev loss: 60151.9658 / 60689.3672
[2017-12-15 14:25:31] Epoch 0056 mean train/dev loss: 60114.4593 / 60738.5352
[2017-12-15 14:25:38] Epoch 0057 mean train/dev loss: 60178.7520 / 60646.4492
[2017-12-15 14:25:46] Epoch 0058 mean train/dev loss: 60099.0293 / 60730.3828
[2017-12-15 14:25:52] Epoch 0059 mean train/dev loss: 60135.7043 / 60725.9102
[2017-12-15 14:25:58] Epoch 0060 mean train/dev loss: 60145.0461 / 60737.8281
[2017-12-15 14:25:58] Learning rate decayed by 0.5000
[2017-12-15 14:25:58] Checkpointing model at epoch 60 for ffn.hl_linear.lr_0.1.wd_1.0
[2017-12-15 14:25:59] Model Checkpointing finished.
[2017-12-15 14:26:03] Epoch 0061 mean train/dev loss: 60108.5694 / 60749.8047
[2017-12-15 14:26:08] Epoch 0062 mean train/dev loss: 60165.4147 / 60725.7812
[2017-12-15 14:26:12] Epoch 0063 mean train/dev loss: 60128.2682 / 60724.8516
[2017-12-15 14:26:17] Epoch 0064 mean train/dev loss: 60122.6416 / 60740.7812
[2017-12-15 14:26:21] Epoch 0065 mean train/dev loss: 60133.9038 / 60747.1875
[2017-12-15 14:26:25] Epoch 0066 mean train/dev loss: 60158.9265 / 60736.0000
[2017-12-15 14:26:30] Epoch 0067 mean train/dev loss: 60129.2734 / 60732.2852
[2017-12-15 14:26:34] Epoch 0068 mean train/dev loss: 60139.0231 / 60713.7461
[2017-12-15 14:26:39] Epoch 0069 mean train/dev loss: 60145.4681 / 60724.9219
[2017-12-15 14:26:43] Epoch 0070 mean train/dev loss: 60129.1267 / 60743.3828
[2017-12-15 14:26:43] Checkpointing model at epoch 70 for ffn.hl_linear.lr_0.1.wd_1.0
[2017-12-15 14:26:43] Model Checkpointing finished.
[2017-12-15 14:26:47] Epoch 0071 mean train/dev loss: 60127.0763 / 60718.3477
[2017-12-15 14:26:52] Epoch 0072 mean train/dev loss: 60135.3387 / 60722.3516
[2017-12-15 14:26:56] Epoch 0073 mean train/dev loss: 60149.5209 / 60712.2617
[2017-12-15 14:27:01] Epoch 0074 mean train/dev loss: 60143.1232 / 60711.4141
[2017-12-15 14:27:05] Epoch 0075 mean train/dev loss: 60140.3454 / 60705.6484
[2017-12-15 14:27:05] Learning rate decayed by 0.5000
[2017-12-15 14:27:09] Epoch 0076 mean train/dev loss: 60106.3952 / 60717.1836
[2017-12-15 14:27:14] Epoch 0077 mean train/dev loss: 60123.8868 / 60724.9297
[2017-12-15 14:27:18] Epoch 0078 mean train/dev loss: 60138.5026 / 60725.4531
[2017-12-15 14:27:23] Epoch 0079 mean train/dev loss: 60141.4810 / 60724.3008
[2017-12-15 14:27:27] Epoch 0080 mean train/dev loss: 60138.1648 / 60721.6680
[2017-12-15 14:27:27] Checkpointing model at epoch 80 for ffn.hl_linear.lr_0.1.wd_1.0
[2017-12-15 14:27:27] Model Checkpointing finished.
[2017-12-15 14:27:31] Epoch 0081 mean train/dev loss: 60133.2588 / 60722.9727
[2017-12-15 14:27:36] Epoch 0082 mean train/dev loss: 60130.9718 / 60731.2539
[2017-12-15 14:27:36] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:27:36] 
                       *** Training finished *** 
[2017-12-15 14:27:37] Dev MSE: 60731.2539
[2017-12-15 14:27:40] Training MSE: 60139.9258
[2017-12-15 14:27:40] Experiment ffn.hl_linear.lr_0.1.wd_1.0 logging ended.
