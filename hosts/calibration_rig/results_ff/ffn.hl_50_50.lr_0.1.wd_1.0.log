[2017-12-15 15:20:50] Experiment ffn.hl_50_50.lr_0.1.wd_1.0 logging started.
[2017-12-15 15:20:50] 
                       *** Starting Experiment ffn.hl_50_50.lr_0.1.wd_1.0 ***
                      
[2017-12-15 15:20:50] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 1.0  
[2017-12-15 15:20:50] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 1)
                      )
[2017-12-15 15:20:50]  *** Training on GPU ***
[2017-12-15 15:20:57] Epoch 0001 mean train/dev loss: 4325.5942 / 319.4646
[2017-12-15 15:21:05] Epoch 0002 mean train/dev loss: 167.2380 / 315.7643
[2017-12-15 15:21:13] Epoch 0003 mean train/dev loss: 176.8551 / 167.7776
[2017-12-15 15:21:21] Epoch 0004 mean train/dev loss: 149.8885 / 271.4025
[2017-12-15 15:21:29] Epoch 0005 mean train/dev loss: 160.6674 / 204.8241
[2017-12-15 15:21:37] Epoch 0006 mean train/dev loss: 136.7952 / 122.2622
[2017-12-15 15:21:45] Epoch 0007 mean train/dev loss: 128.8747 / 124.5195
[2017-12-15 15:21:53] Epoch 0008 mean train/dev loss: 148.5742 / 151.2946
[2017-12-15 15:22:01] Epoch 0009 mean train/dev loss: 123.8379 / 141.9783
[2017-12-15 15:22:09] Epoch 0010 mean train/dev loss: 125.9145 / 146.8426
[2017-12-15 15:22:09] Checkpointing model at epoch 10 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:22:09] Model Checkpointing finished.
[2017-12-15 15:22:17] Epoch 0011 mean train/dev loss: 128.5783 / 138.8171
[2017-12-15 15:22:25] Epoch 0012 mean train/dev loss: 128.5832 / 121.0838
[2017-12-15 15:22:33] Epoch 0013 mean train/dev loss: 124.1217 / 126.1869
[2017-12-15 15:22:41] Epoch 0014 mean train/dev loss: 123.6682 / 111.1476
[2017-12-15 15:22:49] Epoch 0015 mean train/dev loss: 132.6564 / 148.2086
[2017-12-15 15:22:49] Learning rate decayed by 0.5000
[2017-12-15 15:22:57] Epoch 0016 mean train/dev loss: 113.8548 / 120.0434
[2017-12-15 15:23:06] Epoch 0017 mean train/dev loss: 113.3851 / 118.4245
[2017-12-15 15:23:14] Epoch 0018 mean train/dev loss: 114.3657 / 123.0252
[2017-12-15 15:23:21] Epoch 0019 mean train/dev loss: 115.1874 / 132.4229
[2017-12-15 15:23:30] Epoch 0020 mean train/dev loss: 115.0407 / 118.7570
[2017-12-15 15:23:30] Checkpointing model at epoch 20 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:23:30] Model Checkpointing finished.
[2017-12-15 15:23:38] Epoch 0021 mean train/dev loss: 115.1584 / 117.2101
[2017-12-15 15:23:46] Epoch 0022 mean train/dev loss: 114.9508 / 133.9170
[2017-12-15 15:23:54] Epoch 0023 mean train/dev loss: 115.0976 / 115.2538
[2017-12-15 15:24:02] Epoch 0024 mean train/dev loss: 115.0951 / 124.2635
[2017-12-15 15:24:09] Epoch 0025 mean train/dev loss: 115.3529 / 113.2651
[2017-12-15 15:24:17] Epoch 0026 mean train/dev loss: 114.9184 / 111.4174
[2017-12-15 15:24:25] Epoch 0027 mean train/dev loss: 114.8034 / 116.3758
[2017-12-15 15:24:33] Epoch 0028 mean train/dev loss: 115.0039 / 124.7136
[2017-12-15 15:24:41] Epoch 0029 mean train/dev loss: 114.3612 / 116.8787
[2017-12-15 15:24:49] Epoch 0030 mean train/dev loss: 114.7680 / 112.1989
[2017-12-15 15:24:49] Learning rate decayed by 0.5000
[2017-12-15 15:24:49] Checkpointing model at epoch 30 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:24:49] Model Checkpointing finished.
[2017-12-15 15:24:57] Epoch 0031 mean train/dev loss: 110.8817 / 108.8879
[2017-12-15 15:25:05] Epoch 0032 mean train/dev loss: 111.1174 / 119.1989
[2017-12-15 15:25:13] Epoch 0033 mean train/dev loss: 111.4528 / 109.8718
[2017-12-15 15:25:21] Epoch 0034 mean train/dev loss: 111.3151 / 106.0259
[2017-12-15 15:25:29] Epoch 0035 mean train/dev loss: 111.2223 / 119.9184
[2017-12-15 15:25:37] Epoch 0036 mean train/dev loss: 111.4442 / 108.8054
[2017-12-15 15:25:45] Epoch 0037 mean train/dev loss: 111.3932 / 111.2311
[2017-12-15 15:25:52] Epoch 0038 mean train/dev loss: 111.2436 / 120.5160
[2017-12-15 15:26:00] Epoch 0039 mean train/dev loss: 110.9040 / 123.4355
[2017-12-15 15:26:08] Epoch 0040 mean train/dev loss: 111.3822 / 108.9559
[2017-12-15 15:26:08] Checkpointing model at epoch 40 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:26:08] Model Checkpointing finished.
[2017-12-15 15:26:17] Epoch 0041 mean train/dev loss: 111.4030 / 108.1416
[2017-12-15 15:26:24] Epoch 0042 mean train/dev loss: 111.2708 / 108.2749
[2017-12-15 15:26:32] Epoch 0043 mean train/dev loss: 111.0024 / 108.6654
[2017-12-15 15:26:40] Epoch 0044 mean train/dev loss: 110.9199 / 108.9069
[2017-12-15 15:26:48] Epoch 0045 mean train/dev loss: 110.8475 / 107.4573
[2017-12-15 15:26:48] Learning rate decayed by 0.5000
[2017-12-15 15:26:56] Epoch 0046 mean train/dev loss: 109.4138 / 105.9041
[2017-12-15 15:27:04] Epoch 0047 mean train/dev loss: 109.4741 / 113.6775
[2017-12-15 15:27:13] Epoch 0048 mean train/dev loss: 109.5534 / 107.7908
[2017-12-15 15:27:20] Epoch 0049 mean train/dev loss: 109.3374 / 105.5501
[2017-12-15 15:27:28] Epoch 0050 mean train/dev loss: 109.5315 / 106.7565
[2017-12-15 15:27:28] Checkpointing model at epoch 50 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:27:29] Model Checkpointing finished.
[2017-12-15 15:27:37] Epoch 0051 mean train/dev loss: 109.3928 / 104.6464
[2017-12-15 15:27:45] Epoch 0052 mean train/dev loss: 109.6083 / 106.4450
[2017-12-15 15:27:53] Epoch 0053 mean train/dev loss: 109.4598 / 112.8860
[2017-12-15 15:28:00] Epoch 0054 mean train/dev loss: 109.4972 / 111.2817
[2017-12-15 15:28:08] Epoch 0055 mean train/dev loss: 109.2825 / 107.6268
[2017-12-15 15:28:16] Epoch 0056 mean train/dev loss: 109.5494 / 108.1595
[2017-12-15 15:28:24] Epoch 0057 mean train/dev loss: 109.2685 / 109.5327
[2017-12-15 15:28:32] Epoch 0058 mean train/dev loss: 109.3873 / 111.3659
[2017-12-15 15:28:40] Epoch 0059 mean train/dev loss: 109.6413 / 107.7285
[2017-12-15 15:28:48] Epoch 0060 mean train/dev loss: 109.4279 / 108.8825
[2017-12-15 15:28:48] Learning rate decayed by 0.5000
[2017-12-15 15:28:48] Checkpointing model at epoch 60 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:28:48] Model Checkpointing finished.
[2017-12-15 15:28:56] Epoch 0061 mean train/dev loss: 108.4582 / 108.2574
[2017-12-15 15:29:04] Epoch 0062 mean train/dev loss: 108.4487 / 105.7379
[2017-12-15 15:29:13] Epoch 0063 mean train/dev loss: 108.6544 / 105.8804
[2017-12-15 15:29:21] Epoch 0064 mean train/dev loss: 108.7188 / 110.8608
[2017-12-15 15:29:29] Epoch 0065 mean train/dev loss: 108.6482 / 109.3954
[2017-12-15 15:29:37] Epoch 0066 mean train/dev loss: 108.6173 / 107.2239
[2017-12-15 15:29:45] Epoch 0067 mean train/dev loss: 108.4690 / 104.8717
[2017-12-15 15:29:53] Epoch 0068 mean train/dev loss: 108.5740 / 104.7736
[2017-12-15 15:30:02] Epoch 0069 mean train/dev loss: 108.5214 / 106.4298
[2017-12-15 15:30:10] Epoch 0070 mean train/dev loss: 108.5994 / 104.8264
[2017-12-15 15:30:10] Checkpointing model at epoch 70 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:30:10] Model Checkpointing finished.
[2017-12-15 15:30:18] Epoch 0071 mean train/dev loss: 108.6125 / 106.2472
[2017-12-15 15:30:26] Epoch 0072 mean train/dev loss: 108.5711 / 109.2074
[2017-12-15 15:30:34] Epoch 0073 mean train/dev loss: 108.6115 / 108.9773
[2017-12-15 15:30:42] Epoch 0074 mean train/dev loss: 108.6483 / 106.5933
[2017-12-15 15:30:50] Epoch 0075 mean train/dev loss: 108.5444 / 105.7724
[2017-12-15 15:30:50] Learning rate decayed by 0.5000
[2017-12-15 15:30:58] Epoch 0076 mean train/dev loss: 108.0331 / 106.2295
[2017-12-15 15:31:06] Epoch 0077 mean train/dev loss: 107.9583 / 106.8859
[2017-12-15 15:31:14] Epoch 0078 mean train/dev loss: 108.0610 / 105.4967
[2017-12-15 15:31:22] Epoch 0079 mean train/dev loss: 108.0562 / 107.6394
[2017-12-15 15:31:30] Epoch 0080 mean train/dev loss: 108.1556 / 106.2402
[2017-12-15 15:31:30] Checkpointing model at epoch 80 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:31:30] Model Checkpointing finished.
[2017-12-15 15:31:38] Epoch 0081 mean train/dev loss: 108.0789 / 105.8571
[2017-12-15 15:31:46] Epoch 0082 mean train/dev loss: 108.0474 / 106.2038
[2017-12-15 15:31:53] Epoch 0083 mean train/dev loss: 107.9661 / 106.1089
[2017-12-15 15:32:01] Epoch 0084 mean train/dev loss: 108.0185 / 105.8437
[2017-12-15 15:32:09] Epoch 0085 mean train/dev loss: 108.1325 / 105.7585
[2017-12-15 15:32:18] Epoch 0086 mean train/dev loss: 108.0874 / 105.9282
[2017-12-15 15:32:26] Epoch 0087 mean train/dev loss: 108.0655 / 106.2347
[2017-12-15 15:32:34] Epoch 0088 mean train/dev loss: 107.9602 / 105.9253
[2017-12-15 15:32:42] Epoch 0089 mean train/dev loss: 108.0516 / 105.0456
[2017-12-15 15:32:50] Epoch 0090 mean train/dev loss: 107.9790 / 106.2383
[2017-12-15 15:32:50] Learning rate decayed by 0.5000
[2017-12-15 15:32:50] Checkpointing model at epoch 90 for ffn.hl_50_50.lr_0.1.wd_1.0
[2017-12-15 15:32:50] Model Checkpointing finished.
[2017-12-15 15:32:58] Epoch 0091 mean train/dev loss: 107.6655 / 105.6126
[2017-12-15 15:33:06] Epoch 0092 mean train/dev loss: 107.6998 / 104.9651
[2017-12-15 15:33:06] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:33:06] 
                       *** Training finished *** 
[2017-12-15 15:33:07] Dev MSE: 104.9651
[2017-12-15 15:33:14] Training MSE: 107.9742
[2017-12-15 15:33:16] Experiment ffn.hl_50_50.lr_0.1.wd_1.0 logging ended.
