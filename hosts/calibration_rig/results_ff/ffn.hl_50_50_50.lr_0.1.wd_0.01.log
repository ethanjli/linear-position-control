[2017-12-15 15:38:11] Experiment ffn.hl_50_50_50.lr_0.1.wd_0.01 logging started.
[2017-12-15 15:38:11] 
                       *** Starting Experiment ffn.hl_50_50_50.lr_0.1.wd_0.01 ***
                      
[2017-12-15 15:38:11] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 15:38:11] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 50)
                        (relu3): ReLU ()
                        (linear4): Linear (50 -> 1)
                      )
[2017-12-15 15:38:11]  *** Training on GPU ***
[2017-12-15 15:38:20] Epoch 0001 mean train/dev loss: 5358.1958 / 206.0451
[2017-12-15 15:38:29] Epoch 0002 mean train/dev loss: 121.3327 / 228.4346
[2017-12-15 15:38:37] Epoch 0003 mean train/dev loss: 332.6684 / 146.9379
[2017-12-15 15:38:45] Epoch 0004 mean train/dev loss: 1179.8583 / 377.3929
[2017-12-15 15:38:54] Epoch 0005 mean train/dev loss: 121.0865 / 182.4454
[2017-12-15 15:39:03] Epoch 0006 mean train/dev loss: 104.5589 / 153.9908
[2017-12-15 15:39:12] Epoch 0007 mean train/dev loss: 143.3104 / 253.4279
[2017-12-15 15:39:20] Epoch 0008 mean train/dev loss: 158.1480 / 170.0937
[2017-12-15 15:39:29] Epoch 0009 mean train/dev loss: 121.6730 / 193.2216
[2017-12-15 15:39:38] Epoch 0010 mean train/dev loss: 222.1050 / 161.6982
[2017-12-15 15:39:38] Checkpointing model at epoch 10 for ffn.hl_50_50_50.lr_0.1.wd_0.01
[2017-12-15 15:39:38] Model Checkpointing finished.
[2017-12-15 15:39:47] Epoch 0011 mean train/dev loss: 98.9121 / 153.3811
[2017-12-15 15:39:55] Epoch 0012 mean train/dev loss: 377.4530 / 310.5276
[2017-12-15 15:40:04] Epoch 0013 mean train/dev loss: 97.6007 / 181.7332
[2017-12-15 15:40:12] Epoch 0014 mean train/dev loss: 114.2272 / 146.9530
[2017-12-15 15:40:21] Epoch 0015 mean train/dev loss: 315.7689 / 455.6255
[2017-12-15 15:40:21] Learning rate decayed by 0.5000
[2017-12-15 15:40:29] Epoch 0016 mean train/dev loss: 90.3424 / 138.4908
[2017-12-15 15:40:38] Epoch 0017 mean train/dev loss: 85.0254 / 136.0509
[2017-12-15 15:40:46] Epoch 0018 mean train/dev loss: 85.1151 / 122.3301
[2017-12-15 15:40:55] Epoch 0019 mean train/dev loss: 82.8069 / 179.6267
[2017-12-15 15:41:03] Epoch 0020 mean train/dev loss: 82.3923 / 139.7628
[2017-12-15 15:41:03] Checkpointing model at epoch 20 for ffn.hl_50_50_50.lr_0.1.wd_0.01
[2017-12-15 15:41:03] Model Checkpointing finished.
[2017-12-15 15:41:12] Epoch 0021 mean train/dev loss: 82.7547 / 172.9644
[2017-12-15 15:41:20] Epoch 0022 mean train/dev loss: 188.1737 / 159.8654
[2017-12-15 15:41:29] Epoch 0023 mean train/dev loss: 87.5447 / 136.8778
[2017-12-15 15:41:37] Epoch 0024 mean train/dev loss: 83.5762 / 117.6424
[2017-12-15 15:41:46] Epoch 0025 mean train/dev loss: 82.5029 / 144.2109
[2017-12-15 15:41:54] Epoch 0026 mean train/dev loss: 79.8793 / 133.8086
[2017-12-15 15:42:02] Epoch 0027 mean train/dev loss: 85.9487 / 115.1114
[2017-12-15 15:42:11] Epoch 0028 mean train/dev loss: 81.4029 / 113.2706
[2017-12-15 15:42:19] Epoch 0029 mean train/dev loss: 75.0780 / 138.2042
[2017-12-15 15:42:27] Epoch 0030 mean train/dev loss: 98.3041 / 217.3782
[2017-12-15 15:42:27] Learning rate decayed by 0.5000
[2017-12-15 15:42:27] Checkpointing model at epoch 30 for ffn.hl_50_50_50.lr_0.1.wd_0.01
[2017-12-15 15:42:28] Model Checkpointing finished.
[2017-12-15 15:42:36] Epoch 0031 mean train/dev loss: 67.7648 / 118.1306
[2017-12-15 15:42:45] Epoch 0032 mean train/dev loss: 64.9030 / 123.0108
[2017-12-15 15:42:53] Epoch 0033 mean train/dev loss: 64.1646 / 118.8235
[2017-12-15 15:43:02] Epoch 0034 mean train/dev loss: 63.7694 / 137.6008
[2017-12-15 15:43:10] Epoch 0035 mean train/dev loss: 65.5731 / 132.8289
[2017-12-15 15:43:19] Epoch 0036 mean train/dev loss: 64.5401 / 138.4474
[2017-12-15 15:43:27] Epoch 0037 mean train/dev loss: 64.2223 / 151.8279
[2017-12-15 15:43:36] Epoch 0038 mean train/dev loss: 63.8349 / 142.7629
[2017-12-15 15:43:44] Epoch 0039 mean train/dev loss: 63.8179 / 126.2660
[2017-12-15 15:43:53] Epoch 0040 mean train/dev loss: 63.2409 / 141.5040
[2017-12-15 15:43:53] Checkpointing model at epoch 40 for ffn.hl_50_50_50.lr_0.1.wd_0.01
[2017-12-15 15:43:53] Model Checkpointing finished.
[2017-12-15 15:44:01] Epoch 0041 mean train/dev loss: 61.8299 / 164.0964
[2017-12-15 15:44:10] Epoch 0042 mean train/dev loss: 62.5977 / 154.4575
[2017-12-15 15:44:19] Epoch 0043 mean train/dev loss: 61.9246 / 156.3723
[2017-12-15 15:44:27] Epoch 0044 mean train/dev loss: 62.0190 / 174.0825
[2017-12-15 15:44:35] Epoch 0045 mean train/dev loss: 62.9953 / 152.2899
[2017-12-15 15:44:35] Learning rate decayed by 0.5000
[2017-12-15 15:44:44] Epoch 0046 mean train/dev loss: 57.2616 / 156.9786
[2017-12-15 15:44:52] Epoch 0047 mean train/dev loss: 57.3293 / 154.2845
[2017-12-15 15:45:00] Epoch 0048 mean train/dev loss: 57.2399 / 146.0196
[2017-12-15 15:45:09] Epoch 0049 mean train/dev loss: 58.0599 / 151.9464
[2017-12-15 15:45:17] Epoch 0050 mean train/dev loss: 57.6365 / 163.8828
[2017-12-15 15:45:17] Checkpointing model at epoch 50 for ffn.hl_50_50_50.lr_0.1.wd_0.01
[2017-12-15 15:45:18] Model Checkpointing finished.
[2017-12-15 15:45:26] Epoch 0051 mean train/dev loss: 57.5438 / 137.6533
[2017-12-15 15:45:34] Epoch 0052 mean train/dev loss: 57.3271 / 140.3974
[2017-12-15 15:45:43] Epoch 0053 mean train/dev loss: 57.2421 / 138.7251
[2017-12-15 15:45:51] Epoch 0054 mean train/dev loss: 57.1502 / 149.1770
[2017-12-15 15:45:59] Epoch 0055 mean train/dev loss: 57.0247 / 135.8759
[2017-12-15 15:46:08] Epoch 0056 mean train/dev loss: 57.3819 / 142.2377
[2017-12-15 15:46:17] Epoch 0057 mean train/dev loss: 56.7948 / 125.5345
[2017-12-15 15:46:25] Epoch 0058 mean train/dev loss: 57.2183 / 151.0109
[2017-12-15 15:46:33] Epoch 0059 mean train/dev loss: 57.5415 / 138.1826
[2017-12-15 15:46:41] Epoch 0060 mean train/dev loss: 56.0798 / 134.4356
[2017-12-15 15:46:41] Learning rate decayed by 0.5000
[2017-12-15 15:46:41] Checkpointing model at epoch 60 for ffn.hl_50_50_50.lr_0.1.wd_0.01
[2017-12-15 15:46:42] Model Checkpointing finished.
[2017-12-15 15:46:50] Epoch 0061 mean train/dev loss: 54.2379 / 127.5693
[2017-12-15 15:46:59] Epoch 0062 mean train/dev loss: 54.4283 / 134.5505
[2017-12-15 15:47:07] Epoch 0063 mean train/dev loss: 54.3991 / 124.4982
[2017-12-15 15:47:15] Epoch 0064 mean train/dev loss: 54.3786 / 132.5357
[2017-12-15 15:47:24] Epoch 0065 mean train/dev loss: 54.1953 / 134.5544
[2017-12-15 15:47:32] Epoch 0066 mean train/dev loss: 53.9949 / 123.3989
[2017-12-15 15:47:40] Epoch 0067 mean train/dev loss: 53.6980 / 140.3943
[2017-12-15 15:47:49] Epoch 0068 mean train/dev loss: 53.6586 / 127.1368
[2017-12-15 15:47:57] Epoch 0069 mean train/dev loss: 53.7372 / 126.7034
[2017-12-15 15:47:57] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:47:57] 
                       *** Training finished *** 
[2017-12-15 15:47:58] Dev MSE: 126.7034
[2017-12-15 15:48:06] Training MSE: 52.5533
[2017-12-15 15:48:09] Experiment ffn.hl_50_50_50.lr_0.1.wd_0.01 logging ended.
