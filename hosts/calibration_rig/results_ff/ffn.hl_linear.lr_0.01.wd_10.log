[2017-12-15 16:30:20] Experiment ffn.hl_linear.lr_0.01.wd_10 logging started.
[2017-12-15 16:30:20] 
                       *** Starting Experiment ffn.hl_linear.lr_0.01.wd_10 ***
                      
[2017-12-15 16:30:20] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] []  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 16:30:20] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 1)
                      )
[2017-12-15 16:30:20]  *** Training on GPU ***
[2017-12-15 16:30:27] Epoch 0001 mean train/dev loss: 329553.6077 / 326929.5625
[2017-12-15 16:30:35] Epoch 0002 mean train/dev loss: 319824.2643 / 317709.9375
[2017-12-15 16:30:42] Epoch 0003 mean train/dev loss: 311589.5351 / 309946.6875
[2017-12-15 16:30:48] Epoch 0004 mean train/dev loss: 304614.8439 / 303403.7812
[2017-12-15 16:30:54] Epoch 0005 mean train/dev loss: 298636.3295 / 297685.6875
[2017-12-15 16:31:01] Epoch 0006 mean train/dev loss: 293267.8583 / 292558.5000
[2017-12-15 16:31:08] Epoch 0007 mean train/dev loss: 288329.2565 / 287753.6562
[2017-12-15 16:31:15] Epoch 0008 mean train/dev loss: 283629.3599 / 283124.2500
[2017-12-15 16:31:22] Epoch 0009 mean train/dev loss: 279026.0239 / 278695.0312
[2017-12-15 16:31:29] Epoch 0010 mean train/dev loss: 274654.1419 / 274417.0625
[2017-12-15 16:31:29] Checkpointing model at epoch 10 for ffn.hl_linear.lr_0.01.wd_10
[2017-12-15 16:31:29] Model Checkpointing finished.
[2017-12-15 16:31:36] Epoch 0011 mean train/dev loss: 270472.5601 / 270222.0312
[2017-12-15 16:31:44] Epoch 0012 mean train/dev loss: 266464.1526 / 266249.3125
[2017-12-15 16:31:51] Epoch 0013 mean train/dev loss: 262510.8057 / 262548.4375
[2017-12-15 16:31:58] Epoch 0014 mean train/dev loss: 258941.2613 / 259056.9062
[2017-12-15 16:32:04] Epoch 0015 mean train/dev loss: 255547.6478 / 255865.6719
[2017-12-15 16:32:04] Learning rate decayed by 0.5000
[2017-12-15 16:32:10] Epoch 0016 mean train/dev loss: 253229.8835 / 254298.9688
[2017-12-15 16:32:18] Epoch 0017 mean train/dev loss: 251659.3089 / 252741.9531
[2017-12-15 16:32:25] Epoch 0018 mean train/dev loss: 250122.4353 / 251261.2812
[2017-12-15 16:32:30] Epoch 0019 mean train/dev loss: 248693.5559 / 249866.2344
[2017-12-15 16:32:37] Epoch 0020 mean train/dev loss: 247349.6411 / 248628.1719
[2017-12-15 16:32:37] Checkpointing model at epoch 20 for ffn.hl_linear.lr_0.01.wd_10
[2017-12-15 16:32:38] Model Checkpointing finished.
[2017-12-15 16:32:45] Epoch 0021 mean train/dev loss: 246184.6400 / 247560.0625
[2017-12-15 16:32:51] Epoch 0022 mean train/dev loss: 245259.3224 / 246726.5156
[2017-12-15 16:32:57] Epoch 0023 mean train/dev loss: 244586.0902 / 246136.5312
[2017-12-15 16:33:04] Epoch 0024 mean train/dev loss: 244100.5451 / 245791.5781
[2017-12-15 16:33:11] Epoch 0025 mean train/dev loss: 243858.1086 / 245681.6719
[2017-12-15 16:33:19] Epoch 0026 mean train/dev loss: 243780.7098 / 245615.7031
[2017-12-15 16:33:25] Epoch 0027 mean train/dev loss: 243771.7304 / 245603.9531
[2017-12-15 16:33:33] Epoch 0028 mean train/dev loss: 243697.8747 / 245626.5312
[2017-12-15 16:33:39] Epoch 0029 mean train/dev loss: 243751.3555 / 245596.6406
[2017-12-15 16:33:44] Epoch 0030 mean train/dev loss: 243727.5343 / 245604.5000
[2017-12-15 16:33:44] Learning rate decayed by 0.5000
[2017-12-15 16:33:44] Checkpointing model at epoch 30 for ffn.hl_linear.lr_0.01.wd_10
[2017-12-15 16:33:45] Model Checkpointing finished.
[2017-12-15 16:33:52] Epoch 0031 mean train/dev loss: 243748.1659 / 245611.9219
[2017-12-15 16:33:58] Epoch 0032 mean train/dev loss: 243745.3800 / 245588.9844
[2017-12-15 16:34:05] Epoch 0033 mean train/dev loss: 243719.6901 / 245596.3750
[2017-12-15 16:34:11] Epoch 0034 mean train/dev loss: 243709.2606 / 245601.6562
[2017-12-15 16:34:18] Epoch 0035 mean train/dev loss: 243759.8659 / 245602.3125
[2017-12-15 16:34:26] Epoch 0036 mean train/dev loss: 243729.0525 / 245608.5625
[2017-12-15 16:34:33] Epoch 0037 mean train/dev loss: 243766.7712 / 245586.7188
[2017-12-15 16:34:40] Epoch 0038 mean train/dev loss: 243714.7213 / 245586.8281
[2017-12-15 16:34:46] Epoch 0039 mean train/dev loss: 243780.1745 / 245568.0938
[2017-12-15 16:34:52] Epoch 0040 mean train/dev loss: 243685.5374 / 245604.8750
[2017-12-15 16:34:52] Checkpointing model at epoch 40 for ffn.hl_linear.lr_0.01.wd_10
[2017-12-15 16:34:53] Model Checkpointing finished.
[2017-12-15 16:35:00] Epoch 0041 mean train/dev loss: 243750.6247 / 245594.4062
[2017-12-15 16:35:06] Epoch 0042 mean train/dev loss: 243735.3653 / 245599.6406
[2017-12-15 16:35:13] Epoch 0043 mean train/dev loss: 243743.6199 / 245596.2031
[2017-12-15 16:35:18] Epoch 0044 mean train/dev loss: 243744.2990 / 245588.4375
[2017-12-15 16:35:25] Epoch 0045 mean train/dev loss: 243766.0631 / 245584.9219
[2017-12-15 16:35:25] Learning rate decayed by 0.5000
[2017-12-15 16:35:31] Epoch 0046 mean train/dev loss: 243740.2586 / 245590.0938
[2017-12-15 16:35:38] Epoch 0047 mean train/dev loss: 243736.6458 / 245598.3438
[2017-12-15 16:35:45] Epoch 0048 mean train/dev loss: 243746.8698 / 245598.3594
[2017-12-15 16:35:53] Epoch 0049 mean train/dev loss: 243714.8939 / 245605.0469
[2017-12-15 16:35:59] Epoch 0050 mean train/dev loss: 243695.0701 / 245605.1562
[2017-12-15 16:35:59] Checkpointing model at epoch 50 for ffn.hl_linear.lr_0.01.wd_10
[2017-12-15 16:35:59] Model Checkpointing finished.
[2017-12-15 16:36:06] Epoch 0051 mean train/dev loss: 243727.5577 / 245607.6875
[2017-12-15 16:36:12] Epoch 0052 mean train/dev loss: 243740.5460 / 245603.7656
[2017-12-15 16:36:20] Epoch 0053 mean train/dev loss: 243705.0501 / 245601.6406
[2017-12-15 16:36:27] Epoch 0054 mean train/dev loss: 243786.8387 / 245596.0938
[2017-12-15 16:36:34] Epoch 0055 mean train/dev loss: 243727.4709 / 245594.2188
[2017-12-15 16:36:41] Epoch 0056 mean train/dev loss: 243738.7358 / 245603.2188
[2017-12-15 16:36:49] Epoch 0057 mean train/dev loss: 243739.7889 / 245602.8438
[2017-12-15 16:36:55] Epoch 0058 mean train/dev loss: 243686.2770 / 245604.8281
[2017-12-15 16:37:01] Epoch 0059 mean train/dev loss: 243713.5904 / 245610.1250
[2017-12-15 16:37:08] Epoch 0060 mean train/dev loss: 243714.4664 / 245607.5000
[2017-12-15 16:37:08] Learning rate decayed by 0.5000
[2017-12-15 16:37:08] Checkpointing model at epoch 60 for ffn.hl_linear.lr_0.01.wd_10
[2017-12-15 16:37:08] Model Checkpointing finished.
[2017-12-15 16:37:15] Epoch 0061 mean train/dev loss: 243726.8046 / 245609.1406
[2017-12-15 16:37:21] Epoch 0062 mean train/dev loss: 243711.5951 / 245606.0000
[2017-12-15 16:37:27] Epoch 0063 mean train/dev loss: 243762.7296 / 245602.5156
[2017-12-15 16:37:35] Epoch 0064 mean train/dev loss: 243713.6218 / 245604.2500
[2017-12-15 16:37:42] Epoch 0065 mean train/dev loss: 243758.0046 / 245603.1094
[2017-12-15 16:37:47] Epoch 0066 mean train/dev loss: 243698.4473 / 245596.7344
[2017-12-15 16:37:52] Epoch 0067 mean train/dev loss: 243709.3414 / 245603.2188
[2017-12-15 16:37:58] Epoch 0068 mean train/dev loss: 243743.1523 / 245601.6562
[2017-12-15 16:38:04] Epoch 0069 mean train/dev loss: 243737.9976 / 245601.2812
[2017-12-15 16:38:09] Epoch 0070 mean train/dev loss: 243726.1419 / 245599.5625
[2017-12-15 16:38:09] Checkpointing model at epoch 70 for ffn.hl_linear.lr_0.01.wd_10
[2017-12-15 16:38:10] Model Checkpointing finished.
[2017-12-15 16:38:17] Epoch 0071 mean train/dev loss: 243736.3218 / 245598.5000
[2017-12-15 16:38:25] Epoch 0072 mean train/dev loss: 243698.8258 / 245598.7188
[2017-12-15 16:38:32] Epoch 0073 mean train/dev loss: 243762.5505 / 245602.3438
[2017-12-15 16:38:38] Epoch 0074 mean train/dev loss: 243734.3153 / 245598.2812
[2017-12-15 16:38:45] Epoch 0075 mean train/dev loss: 243757.8399 / 245596.2656
[2017-12-15 16:38:45] Learning rate decayed by 0.5000
[2017-12-15 16:38:51] Epoch 0076 mean train/dev loss: 243706.5795 / 245596.9219
[2017-12-15 16:38:57] Epoch 0077 mean train/dev loss: 243768.1913 / 245597.8125
[2017-12-15 16:39:03] Epoch 0078 mean train/dev loss: 243735.2951 / 245598.0625
[2017-12-15 16:39:10] Epoch 0079 mean train/dev loss: 243723.2560 / 245599.9219
[2017-12-15 16:39:17] Epoch 0080 mean train/dev loss: 243714.5545 / 245599.1875
[2017-12-15 16:39:17] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 16:39:17] 
                       *** Training finished *** 
[2017-12-15 16:39:18] Dev MSE: 245599.1875
[2017-12-15 16:39:23] Training MSE: 243730.1094
[2017-12-15 16:39:24] Experiment ffn.hl_linear.lr_0.01.wd_10 logging ended.
