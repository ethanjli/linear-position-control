[2017-12-15 15:09:35] Experiment ffn.hl_50.lr_0.1.wd_0.01 logging started.
[2017-12-15 15:09:35] 
                       *** Starting Experiment ffn.hl_50.lr_0.1.wd_0.01 ***
                      
[2017-12-15 15:09:35] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 15:09:35] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 1)
                      )
[2017-12-15 15:09:35]  *** Training on GPU ***
[2017-12-15 15:09:41] Epoch 0001 mean train/dev loss: 10599.8145 / 242.2114
[2017-12-15 15:09:47] Epoch 0002 mean train/dev loss: 136.1148 / 224.4974
[2017-12-15 15:09:53] Epoch 0003 mean train/dev loss: 119.5694 / 156.4853
[2017-12-15 15:09:59] Epoch 0004 mean train/dev loss: 118.4244 / 183.7187
[2017-12-15 15:10:06] Epoch 0005 mean train/dev loss: 115.8083 / 353.3576
[2017-12-15 15:10:12] Epoch 0006 mean train/dev loss: 118.2081 / 219.7227
[2017-12-15 15:10:19] Epoch 0007 mean train/dev loss: 114.5852 / 172.5334
[2017-12-15 15:10:26] Epoch 0008 mean train/dev loss: 117.9673 / 281.0841
[2017-12-15 15:10:32] Epoch 0009 mean train/dev loss: 125.4267 / 148.5690
[2017-12-15 15:10:38] Epoch 0010 mean train/dev loss: 111.8724 / 209.3241
[2017-12-15 15:10:38] Checkpointing model at epoch 10 for ffn.hl_50.lr_0.1.wd_0.01
[2017-12-15 15:10:39] Model Checkpointing finished.
[2017-12-15 15:10:45] Epoch 0011 mean train/dev loss: 115.3303 / 127.6831
[2017-12-15 15:10:51] Epoch 0012 mean train/dev loss: 111.8782 / 417.0074
[2017-12-15 15:10:58] Epoch 0013 mean train/dev loss: 111.7267 / 175.9849
[2017-12-15 15:11:04] Epoch 0014 mean train/dev loss: 118.8743 / 200.7896
[2017-12-15 15:11:10] Epoch 0015 mean train/dev loss: 113.0780 / 130.5588
[2017-12-15 15:11:10] Learning rate decayed by 0.5000
[2017-12-15 15:11:16] Epoch 0016 mean train/dev loss: 100.3744 / 121.7031
[2017-12-15 15:11:22] Epoch 0017 mean train/dev loss: 101.2067 / 136.6430
[2017-12-15 15:11:29] Epoch 0018 mean train/dev loss: 100.8569 / 128.3310
[2017-12-15 15:11:35] Epoch 0019 mean train/dev loss: 99.3218 / 141.7834
[2017-12-15 15:11:41] Epoch 0020 mean train/dev loss: 100.5412 / 126.6829
[2017-12-15 15:11:41] Checkpointing model at epoch 20 for ffn.hl_50.lr_0.1.wd_0.01
[2017-12-15 15:11:42] Model Checkpointing finished.
[2017-12-15 15:11:48] Epoch 0021 mean train/dev loss: 99.9623 / 135.5958
[2017-12-15 15:11:54] Epoch 0022 mean train/dev loss: 95.4286 / 137.9991
[2017-12-15 15:12:00] Epoch 0023 mean train/dev loss: 94.8181 / 129.9755
[2017-12-15 15:12:07] Epoch 0024 mean train/dev loss: 91.1048 / 149.7733
[2017-12-15 15:12:12] Epoch 0025 mean train/dev loss: 92.9245 / 150.0688
[2017-12-15 15:12:18] Epoch 0026 mean train/dev loss: 90.3574 / 148.0724
[2017-12-15 15:12:25] Epoch 0027 mean train/dev loss: 91.7864 / 142.1831
[2017-12-15 15:12:31] Epoch 0028 mean train/dev loss: 88.9727 / 148.1697
[2017-12-15 15:12:37] Epoch 0029 mean train/dev loss: 89.1567 / 135.4464
[2017-12-15 15:12:43] Epoch 0030 mean train/dev loss: 90.4847 / 129.3727
[2017-12-15 15:12:43] Learning rate decayed by 0.5000
[2017-12-15 15:12:43] Checkpointing model at epoch 30 for ffn.hl_50.lr_0.1.wd_0.01
[2017-12-15 15:12:43] Model Checkpointing finished.
[2017-12-15 15:12:50] Epoch 0031 mean train/dev loss: 84.0673 / 117.5639
[2017-12-15 15:12:56] Epoch 0032 mean train/dev loss: 83.8056 / 127.0245
[2017-12-15 15:13:02] Epoch 0033 mean train/dev loss: 84.2387 / 115.3586
[2017-12-15 15:13:09] Epoch 0034 mean train/dev loss: 83.1151 / 126.9838
[2017-12-15 15:13:15] Epoch 0035 mean train/dev loss: 82.6180 / 124.9883
[2017-12-15 15:13:22] Epoch 0036 mean train/dev loss: 82.6903 / 118.7621
[2017-12-15 15:13:28] Epoch 0037 mean train/dev loss: 82.2795 / 117.2802
[2017-12-15 15:13:34] Epoch 0038 mean train/dev loss: 81.1677 / 112.5655
[2017-12-15 15:13:40] Epoch 0039 mean train/dev loss: 80.9928 / 122.5750
[2017-12-15 15:13:46] Epoch 0040 mean train/dev loss: 81.1207 / 125.1975
[2017-12-15 15:13:46] Checkpointing model at epoch 40 for ffn.hl_50.lr_0.1.wd_0.01
[2017-12-15 15:13:46] Model Checkpointing finished.
[2017-12-15 15:13:53] Epoch 0041 mean train/dev loss: 80.3108 / 118.1623
[2017-12-15 15:13:59] Epoch 0042 mean train/dev loss: 79.7472 / 121.7352
[2017-12-15 15:14:05] Epoch 0043 mean train/dev loss: 79.9413 / 133.4021
[2017-12-15 15:14:11] Epoch 0044 mean train/dev loss: 80.0552 / 147.4812
[2017-12-15 15:14:17] Epoch 0045 mean train/dev loss: 79.9267 / 132.6739
[2017-12-15 15:14:17] Learning rate decayed by 0.5000
[2017-12-15 15:14:23] Epoch 0046 mean train/dev loss: 77.7459 / 120.5359
[2017-12-15 15:14:30] Epoch 0047 mean train/dev loss: 77.6790 / 131.0682
[2017-12-15 15:14:35] Epoch 0048 mean train/dev loss: 77.9168 / 128.6151
[2017-12-15 15:14:41] Epoch 0049 mean train/dev loss: 77.8742 / 156.8239
[2017-12-15 15:14:48] Epoch 0050 mean train/dev loss: 77.7444 / 134.4222
[2017-12-15 15:14:48] Checkpointing model at epoch 50 for ffn.hl_50.lr_0.1.wd_0.01
[2017-12-15 15:14:48] Model Checkpointing finished.
[2017-12-15 15:14:54] Epoch 0051 mean train/dev loss: 77.6492 / 130.0795
[2017-12-15 15:15:01] Epoch 0052 mean train/dev loss: 77.8824 / 122.3867
[2017-12-15 15:15:07] Epoch 0053 mean train/dev loss: 77.7591 / 140.0704
[2017-12-15 15:15:13] Epoch 0054 mean train/dev loss: 77.5202 / 139.9309
[2017-12-15 15:15:20] Epoch 0055 mean train/dev loss: 77.4327 / 139.1399
[2017-12-15 15:15:26] Epoch 0056 mean train/dev loss: 77.3958 / 126.9595
[2017-12-15 15:15:32] Epoch 0057 mean train/dev loss: 77.5452 / 131.2246
[2017-12-15 15:15:38] Epoch 0058 mean train/dev loss: 77.5840 / 121.2991
[2017-12-15 15:15:45] Epoch 0059 mean train/dev loss: 77.2994 / 134.1550
[2017-12-15 15:15:51] Epoch 0060 mean train/dev loss: 77.3467 / 138.8954
[2017-12-15 15:15:51] Learning rate decayed by 0.5000
[2017-12-15 15:15:51] Checkpointing model at epoch 60 for ffn.hl_50.lr_0.1.wd_0.01
[2017-12-15 15:15:51] Model Checkpointing finished.
[2017-12-15 15:15:57] Epoch 0061 mean train/dev loss: 76.3040 / 134.4392
[2017-12-15 15:16:03] Epoch 0062 mean train/dev loss: 76.2875 / 129.6836
[2017-12-15 15:16:10] Epoch 0063 mean train/dev loss: 76.3145 / 126.0004
[2017-12-15 15:16:16] Epoch 0064 mean train/dev loss: 76.4123 / 136.7024
[2017-12-15 15:16:23] Epoch 0065 mean train/dev loss: 76.3277 / 129.4091
[2017-12-15 15:16:29] Epoch 0066 mean train/dev loss: 76.4515 / 136.8938
[2017-12-15 15:16:35] Epoch 0067 mean train/dev loss: 76.4041 / 131.2584
[2017-12-15 15:16:42] Epoch 0068 mean train/dev loss: 76.2497 / 127.8509
[2017-12-15 15:16:48] Epoch 0069 mean train/dev loss: 76.3855 / 130.4767
[2017-12-15 15:16:54] Epoch 0070 mean train/dev loss: 76.1963 / 122.1838
[2017-12-15 15:16:54] Checkpointing model at epoch 70 for ffn.hl_50.lr_0.1.wd_0.01
[2017-12-15 15:16:55] Model Checkpointing finished.
[2017-12-15 15:17:01] Epoch 0071 mean train/dev loss: 76.2666 / 131.8607
[2017-12-15 15:17:08] Epoch 0072 mean train/dev loss: 76.2381 / 128.1492
[2017-12-15 15:17:15] Epoch 0073 mean train/dev loss: 76.2277 / 144.8274
[2017-12-15 15:17:21] Epoch 0074 mean train/dev loss: 76.2613 / 134.5524
[2017-12-15 15:17:27] Epoch 0075 mean train/dev loss: 76.1114 / 140.0009
[2017-12-15 15:17:27] Learning rate decayed by 0.5000
[2017-12-15 15:17:32] Epoch 0076 mean train/dev loss: 75.6284 / 133.3280
[2017-12-15 15:17:37] Epoch 0077 mean train/dev loss: 75.6686 / 134.7417
[2017-12-15 15:17:42] Epoch 0078 mean train/dev loss: 75.6695 / 129.3113
[2017-12-15 15:17:47] Epoch 0079 mean train/dev loss: 75.6208 / 139.1111
[2017-12-15 15:17:47] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 15:17:47] 
                       *** Training finished *** 
[2017-12-15 15:17:48] Dev MSE: 139.1111
[2017-12-15 15:17:52] Training MSE: 75.7933
[2017-12-15 15:17:53] Experiment ffn.hl_50.lr_0.1.wd_0.01 logging ended.
