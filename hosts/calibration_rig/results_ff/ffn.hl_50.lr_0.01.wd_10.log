[2017-12-15 17:28:31] Experiment ffn.hl_50.lr_0.01.wd_10 logging started.
[2017-12-15 17:28:31] 
                       *** Starting Experiment ffn.hl_50.lr_0.01.wd_10 ***
                      
[2017-12-15 17:28:31] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 17:28:31] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 1)
                      )
[2017-12-15 17:28:31]  *** Training on GPU ***
[2017-12-15 17:28:39] Epoch 0001 mean train/dev loss: 69671.0922 / 2102.6299
[2017-12-15 17:28:47] Epoch 0002 mean train/dev loss: 1032.7244 / 941.1785
[2017-12-15 17:28:55] Epoch 0003 mean train/dev loss: 635.4546 / 707.2117
[2017-12-15 17:29:03] Epoch 0004 mean train/dev loss: 485.1619 / 498.8847
[2017-12-15 17:29:10] Epoch 0005 mean train/dev loss: 384.1833 / 423.3093
[2017-12-15 17:29:18] Epoch 0006 mean train/dev loss: 326.8644 / 345.7299
[2017-12-15 17:29:25] Epoch 0007 mean train/dev loss: 287.1537 / 288.0529
[2017-12-15 17:29:32] Epoch 0008 mean train/dev loss: 259.1457 / 260.1345
[2017-12-15 17:29:39] Epoch 0009 mean train/dev loss: 245.2954 / 240.8139
[2017-12-15 17:29:47] Epoch 0010 mean train/dev loss: 238.3780 / 221.0662
[2017-12-15 17:29:47] Checkpointing model at epoch 10 for ffn.hl_50.lr_0.01.wd_10
[2017-12-15 17:29:47] Model Checkpointing finished.
[2017-12-15 17:29:54] Epoch 0011 mean train/dev loss: 232.9795 / 219.0135
[2017-12-15 17:30:01] Epoch 0012 mean train/dev loss: 229.2956 / 210.2015
[2017-12-15 17:30:09] Epoch 0013 mean train/dev loss: 227.3800 / 225.0096
[2017-12-15 17:30:16] Epoch 0014 mean train/dev loss: 226.1872 / 202.0247
[2017-12-15 17:30:24] Epoch 0015 mean train/dev loss: 226.0503 / 227.9216
[2017-12-15 17:30:24] Learning rate decayed by 0.5000
[2017-12-15 17:30:31] Epoch 0016 mean train/dev loss: 224.5765 / 204.4647
[2017-12-15 17:30:38] Epoch 0017 mean train/dev loss: 224.2482 / 212.7940
[2017-12-15 17:30:45] Epoch 0018 mean train/dev loss: 224.4036 / 215.3858
[2017-12-15 17:30:53] Epoch 0019 mean train/dev loss: 224.5547 / 207.6775
[2017-12-15 17:31:00] Epoch 0020 mean train/dev loss: 224.8661 / 211.5433
[2017-12-15 17:31:00] Checkpointing model at epoch 20 for ffn.hl_50.lr_0.01.wd_10
[2017-12-15 17:31:00] Model Checkpointing finished.
[2017-12-15 17:31:07] Epoch 0021 mean train/dev loss: 224.3794 / 217.8664
[2017-12-15 17:31:15] Epoch 0022 mean train/dev loss: 224.3283 / 213.4969
[2017-12-15 17:31:22] Epoch 0023 mean train/dev loss: 224.3175 / 212.9580
[2017-12-15 17:31:29] Epoch 0024 mean train/dev loss: 224.2629 / 213.4158
[2017-12-15 17:31:37] Epoch 0025 mean train/dev loss: 224.2614 / 211.2158
[2017-12-15 17:31:44] Epoch 0026 mean train/dev loss: 224.3335 / 207.9686
[2017-12-15 17:31:51] Epoch 0027 mean train/dev loss: 224.0597 / 215.0227
[2017-12-15 17:31:59] Epoch 0028 mean train/dev loss: 224.2213 / 215.8976
[2017-12-15 17:32:06] Epoch 0029 mean train/dev loss: 224.0696 / 211.8609
[2017-12-15 17:32:13] Epoch 0030 mean train/dev loss: 224.3085 / 206.2936
[2017-12-15 17:32:13] Learning rate decayed by 0.5000
[2017-12-15 17:32:13] Checkpointing model at epoch 30 for ffn.hl_50.lr_0.01.wd_10
[2017-12-15 17:32:13] Model Checkpointing finished.
[2017-12-15 17:32:21] Epoch 0031 mean train/dev loss: 223.0731 / 207.2043
[2017-12-15 17:32:28] Epoch 0032 mean train/dev loss: 223.3040 / 210.6087
[2017-12-15 17:32:36] Epoch 0033 mean train/dev loss: 223.1375 / 213.1014
[2017-12-15 17:32:43] Epoch 0034 mean train/dev loss: 223.2626 / 214.2784
[2017-12-15 17:32:50] Epoch 0035 mean train/dev loss: 223.3457 / 210.1282
[2017-12-15 17:32:58] Epoch 0036 mean train/dev loss: 223.0569 / 206.7030
[2017-12-15 17:33:05] Epoch 0037 mean train/dev loss: 223.2456 / 205.2010
[2017-12-15 17:33:13] Epoch 0038 mean train/dev loss: 223.1216 / 216.2435
[2017-12-15 17:33:20] Epoch 0039 mean train/dev loss: 223.2207 / 215.5715
[2017-12-15 17:33:28] Epoch 0040 mean train/dev loss: 223.0872 / 216.9274
[2017-12-15 17:33:28] Checkpointing model at epoch 40 for ffn.hl_50.lr_0.01.wd_10
[2017-12-15 17:33:28] Model Checkpointing finished.
[2017-12-15 17:33:36] Epoch 0041 mean train/dev loss: 223.2324 / 210.2446
[2017-12-15 17:33:43] Epoch 0042 mean train/dev loss: 223.2116 / 206.6949
[2017-12-15 17:33:50] Epoch 0043 mean train/dev loss: 223.1101 / 206.8644
[2017-12-15 17:33:58] Epoch 0044 mean train/dev loss: 223.2240 / 210.7599
[2017-12-15 17:34:05] Epoch 0045 mean train/dev loss: 223.2231 / 201.7946
[2017-12-15 17:34:05] Learning rate decayed by 0.5000
[2017-12-15 17:34:12] Epoch 0046 mean train/dev loss: 222.6803 / 211.8378
[2017-12-15 17:34:19] Epoch 0047 mean train/dev loss: 222.6336 / 214.6520
[2017-12-15 17:34:27] Epoch 0048 mean train/dev loss: 222.9363 / 206.0941
[2017-12-15 17:34:34] Epoch 0049 mean train/dev loss: 222.5799 / 209.2931
[2017-12-15 17:34:41] Epoch 0050 mean train/dev loss: 222.7533 / 211.8002
[2017-12-15 17:34:41] Checkpointing model at epoch 50 for ffn.hl_50.lr_0.01.wd_10
[2017-12-15 17:34:42] Model Checkpointing finished.
[2017-12-15 17:34:49] Epoch 0051 mean train/dev loss: 222.7322 / 212.1409
[2017-12-15 17:34:57] Epoch 0052 mean train/dev loss: 222.8472 / 208.4919
[2017-12-15 17:35:04] Epoch 0053 mean train/dev loss: 222.6479 / 207.4095
[2017-12-15 17:35:12] Epoch 0054 mean train/dev loss: 222.6120 / 211.6546
[2017-12-15 17:35:19] Epoch 0055 mean train/dev loss: 222.7811 / 209.2441
[2017-12-15 17:35:26] Epoch 0056 mean train/dev loss: 222.6365 / 213.7355
[2017-12-15 17:35:34] Epoch 0057 mean train/dev loss: 222.8652 / 210.4729
[2017-12-15 17:35:41] Epoch 0058 mean train/dev loss: 222.8712 / 212.8462
[2017-12-15 17:35:48] Epoch 0059 mean train/dev loss: 222.7088 / 205.0601
[2017-12-15 17:35:55] Epoch 0060 mean train/dev loss: 222.6946 / 205.4986
[2017-12-15 17:35:55] Learning rate decayed by 0.5000
[2017-12-15 17:35:55] Checkpointing model at epoch 60 for ffn.hl_50.lr_0.01.wd_10
[2017-12-15 17:35:56] Model Checkpointing finished.
[2017-12-15 17:36:03] Epoch 0061 mean train/dev loss: 222.3823 / 209.1294
[2017-12-15 17:36:10] Epoch 0062 mean train/dev loss: 222.3212 / 212.4187
[2017-12-15 17:36:18] Epoch 0063 mean train/dev loss: 222.6436 / 210.1902
[2017-12-15 17:36:25] Epoch 0064 mean train/dev loss: 222.3198 / 210.0111
[2017-12-15 17:36:32] Epoch 0065 mean train/dev loss: 222.6597 / 210.5216
[2017-12-15 17:36:39] Epoch 0066 mean train/dev loss: 222.4728 / 207.3734
[2017-12-15 17:36:47] Epoch 0067 mean train/dev loss: 222.3464 / 213.9185
[2017-12-15 17:36:54] Epoch 0068 mean train/dev loss: 222.6324 / 209.2731
[2017-12-15 17:37:01] Epoch 0069 mean train/dev loss: 222.3771 / 213.5477
[2017-12-15 17:37:08] Epoch 0070 mean train/dev loss: 222.4371 / 212.7486
[2017-12-15 17:37:08] Checkpointing model at epoch 70 for ffn.hl_50.lr_0.01.wd_10
[2017-12-15 17:37:09] Model Checkpointing finished.
[2017-12-15 17:37:15] Epoch 0071 mean train/dev loss: 222.5069 / 214.1552
[2017-12-15 17:37:22] Epoch 0072 mean train/dev loss: 222.3614 / 210.1960
[2017-12-15 17:37:29] Epoch 0073 mean train/dev loss: 222.5375 / 211.3696
[2017-12-15 17:37:37] Epoch 0074 mean train/dev loss: 222.5729 / 209.9968
[2017-12-15 17:37:44] Epoch 0075 mean train/dev loss: 222.2848 / 210.6674
[2017-12-15 17:37:44] Learning rate decayed by 0.5000
[2017-12-15 17:37:51] Epoch 0076 mean train/dev loss: 222.5226 / 208.6611
[2017-12-15 17:37:59] Epoch 0077 mean train/dev loss: 222.5212 / 209.4470
[2017-12-15 17:38:06] Epoch 0078 mean train/dev loss: 222.2401 / 208.4700
[2017-12-15 17:38:13] Epoch 0079 mean train/dev loss: 222.2589 / 210.6417
[2017-12-15 17:38:20] Epoch 0080 mean train/dev loss: 222.3610 / 210.4111
[2017-12-15 17:38:20] Checkpointing model at epoch 80 for ffn.hl_50.lr_0.01.wd_10
[2017-12-15 17:38:21] Model Checkpointing finished.
[2017-12-15 17:38:28] Epoch 0081 mean train/dev loss: 222.3598 / 209.7109
[2017-12-15 17:38:35] Epoch 0082 mean train/dev loss: 222.4297 / 209.5263
[2017-12-15 17:38:43] Epoch 0083 mean train/dev loss: 222.4516 / 210.6894
[2017-12-15 17:38:51] Epoch 0084 mean train/dev loss: 222.1892 / 212.4945
[2017-12-15 17:38:57] Epoch 0085 mean train/dev loss: 222.2427 / 210.6076
[2017-12-15 17:39:03] Epoch 0086 mean train/dev loss: 222.3896 / 209.9865
[2017-12-15 17:39:03] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:39:03] 
                       *** Training finished *** 
[2017-12-15 17:39:04] Dev MSE: 209.9865
[2017-12-15 17:39:09] Training MSE: 222.7996
[2017-12-15 17:39:11] Experiment ffn.hl_50.lr_0.01.wd_10 logging ended.
