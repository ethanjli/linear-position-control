[2017-12-15 17:51:33] Experiment ffn.hl_50_50_50.lr_0.01.wd_0.01 logging started.
[2017-12-15 17:51:33] 
                       *** Starting Experiment ffn.hl_50_50_50.lr_0.01.wd_0.01 ***
                      
[2017-12-15 17:51:33] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 17:51:33] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 50)
                        (relu3): ReLU ()
                        (linear4): Linear (50 -> 1)
                      )
[2017-12-15 17:51:33]  *** Training on GPU ***
[2017-12-15 17:51:41] Epoch 0001 mean train/dev loss: 11986.8805 / 214.3812
[2017-12-15 17:51:50] Epoch 0002 mean train/dev loss: 118.4059 / 128.7510
[2017-12-15 17:51:59] Epoch 0003 mean train/dev loss: 108.3748 / 132.9732
[2017-12-15 17:52:08] Epoch 0004 mean train/dev loss: 103.9595 / 118.1665
[2017-12-15 17:52:17] Epoch 0005 mean train/dev loss: 101.3844 / 140.5267
[2017-12-15 17:52:26] Epoch 0006 mean train/dev loss: 100.0055 / 110.8316
[2017-12-15 17:52:34] Epoch 0007 mean train/dev loss: 100.2050 / 126.7520
[2017-12-15 17:52:42] Epoch 0008 mean train/dev loss: 97.8869 / 170.1853
[2017-12-15 17:52:51] Epoch 0009 mean train/dev loss: 96.3016 / 158.8997
[2017-12-15 17:53:00] Epoch 0010 mean train/dev loss: 97.3961 / 184.1122
[2017-12-15 17:53:00] Checkpointing model at epoch 10 for ffn.hl_50_50_50.lr_0.01.wd_0.01
[2017-12-15 17:53:00] Model Checkpointing finished.
[2017-12-15 17:53:09] Epoch 0011 mean train/dev loss: 92.8493 / 143.4379
[2017-12-15 17:53:18] Epoch 0012 mean train/dev loss: 95.1977 / 131.5651
[2017-12-15 17:53:27] Epoch 0013 mean train/dev loss: 94.0531 / 118.4285
[2017-12-15 17:53:35] Epoch 0014 mean train/dev loss: 92.0547 / 141.2032
[2017-12-15 17:53:43] Epoch 0015 mean train/dev loss: 91.3134 / 150.7840
[2017-12-15 17:53:43] Learning rate decayed by 0.5000
[2017-12-15 17:53:52] Epoch 0016 mean train/dev loss: 79.5949 / 150.9890
[2017-12-15 17:54:00] Epoch 0017 mean train/dev loss: 80.1794 / 167.3654
[2017-12-15 17:54:09] Epoch 0018 mean train/dev loss: 80.0774 / 135.4162
[2017-12-15 17:54:18] Epoch 0019 mean train/dev loss: 79.3071 / 133.9127
[2017-12-15 17:54:26] Epoch 0020 mean train/dev loss: 79.6776 / 132.6194
[2017-12-15 17:54:26] Checkpointing model at epoch 20 for ffn.hl_50_50_50.lr_0.01.wd_0.01
[2017-12-15 17:54:27] Model Checkpointing finished.
[2017-12-15 17:54:35] Epoch 0021 mean train/dev loss: 78.5429 / 125.4863
[2017-12-15 17:54:44] Epoch 0022 mean train/dev loss: 77.5641 / 149.0870
[2017-12-15 17:54:52] Epoch 0023 mean train/dev loss: 77.0116 / 161.2625
[2017-12-15 17:55:01] Epoch 0024 mean train/dev loss: 77.1472 / 125.4044
[2017-12-15 17:55:09] Epoch 0025 mean train/dev loss: 77.2154 / 114.8872
[2017-12-15 17:55:18] Epoch 0026 mean train/dev loss: 76.0930 / 147.5759
[2017-12-15 17:55:27] Epoch 0027 mean train/dev loss: 75.7038 / 125.4722
[2017-12-15 17:55:35] Epoch 0028 mean train/dev loss: 75.5436 / 112.8734
[2017-12-15 17:55:44] Epoch 0029 mean train/dev loss: 74.9071 / 134.9414
[2017-12-15 17:55:52] Epoch 0030 mean train/dev loss: 75.0403 / 140.4367
[2017-12-15 17:55:52] Learning rate decayed by 0.5000
[2017-12-15 17:55:52] Checkpointing model at epoch 30 for ffn.hl_50_50_50.lr_0.01.wd_0.01
[2017-12-15 17:55:53] Model Checkpointing finished.
[2017-12-15 17:56:01] Epoch 0031 mean train/dev loss: 70.5464 / 133.2692
[2017-12-15 17:56:10] Epoch 0032 mean train/dev loss: 70.7776 / 132.7260
[2017-12-15 17:56:18] Epoch 0033 mean train/dev loss: 70.8910 / 127.6289
[2017-12-15 17:56:27] Epoch 0034 mean train/dev loss: 70.6126 / 117.7858
[2017-12-15 17:56:36] Epoch 0035 mean train/dev loss: 70.8852 / 127.4842
[2017-12-15 17:56:45] Epoch 0036 mean train/dev loss: 70.5394 / 136.8384
[2017-12-15 17:56:53] Epoch 0037 mean train/dev loss: 70.3951 / 110.2065
[2017-12-15 17:57:02] Epoch 0038 mean train/dev loss: 69.8588 / 130.9039
[2017-12-15 17:57:10] Epoch 0039 mean train/dev loss: 69.9052 / 151.9600
[2017-12-15 17:57:19] Epoch 0040 mean train/dev loss: 69.7707 / 106.2295
[2017-12-15 17:57:19] Checkpointing model at epoch 40 for ffn.hl_50_50_50.lr_0.01.wd_0.01
[2017-12-15 17:57:20] Model Checkpointing finished.
[2017-12-15 17:57:28] Epoch 0041 mean train/dev loss: 69.7305 / 135.6450
[2017-12-15 17:57:36] Epoch 0042 mean train/dev loss: 69.4079 / 113.4817
[2017-12-15 17:57:45] Epoch 0043 mean train/dev loss: 69.0404 / 121.0869
[2017-12-15 17:57:54] Epoch 0044 mean train/dev loss: 69.4415 / 131.8234
[2017-12-15 17:58:02] Epoch 0045 mean train/dev loss: 68.8391 / 144.5615
[2017-12-15 17:58:02] Learning rate decayed by 0.5000
[2017-12-15 17:58:10] Epoch 0046 mean train/dev loss: 67.1543 / 129.9550
[2017-12-15 17:58:19] Epoch 0047 mean train/dev loss: 67.4734 / 123.5097
[2017-12-15 17:58:27] Epoch 0048 mean train/dev loss: 67.2471 / 122.7657
[2017-12-15 17:58:36] Epoch 0049 mean train/dev loss: 67.3598 / 132.3804
[2017-12-15 17:58:44] Epoch 0050 mean train/dev loss: 67.1386 / 124.8049
[2017-12-15 17:58:44] Checkpointing model at epoch 50 for ffn.hl_50_50_50.lr_0.01.wd_0.01
[2017-12-15 17:58:45] Model Checkpointing finished.
[2017-12-15 17:58:54] Epoch 0051 mean train/dev loss: 67.2654 / 130.7258
[2017-12-15 17:59:02] Epoch 0052 mean train/dev loss: 66.9860 / 123.7808
[2017-12-15 17:59:10] Epoch 0053 mean train/dev loss: 66.9870 / 121.4181
[2017-12-15 17:59:19] Epoch 0054 mean train/dev loss: 66.9161 / 132.1620
[2017-12-15 17:59:28] Epoch 0055 mean train/dev loss: 66.6851 / 134.0353
[2017-12-15 17:59:35] Epoch 0056 mean train/dev loss: 66.6561 / 134.0517
[2017-12-15 17:59:43] Epoch 0057 mean train/dev loss: 66.6639 / 121.0864
[2017-12-15 17:59:50] Epoch 0058 mean train/dev loss: 66.5368 / 139.8977
[2017-12-15 17:59:57] Epoch 0059 mean train/dev loss: 66.4171 / 120.1877
[2017-12-15 18:00:05] Epoch 0060 mean train/dev loss: 66.3110 / 130.8189
[2017-12-15 18:00:05] Learning rate decayed by 0.5000
[2017-12-15 18:00:05] Checkpointing model at epoch 60 for ffn.hl_50_50_50.lr_0.01.wd_0.01
[2017-12-15 18:00:05] Model Checkpointing finished.
[2017-12-15 18:00:12] Epoch 0061 mean train/dev loss: 65.3882 / 132.8763
[2017-12-15 18:00:20] Epoch 0062 mean train/dev loss: 65.4070 / 136.5357
[2017-12-15 18:00:26] Epoch 0063 mean train/dev loss: 65.3768 / 127.9829
[2017-12-15 18:00:32] Epoch 0064 mean train/dev loss: 65.3077 / 132.0560
[2017-12-15 18:00:38] Epoch 0065 mean train/dev loss: 65.2446 / 140.2836
[2017-12-15 18:00:44] Epoch 0066 mean train/dev loss: 65.2535 / 127.6489
[2017-12-15 18:00:49] Epoch 0067 mean train/dev loss: 65.0819 / 132.5629
[2017-12-15 18:00:55] Epoch 0068 mean train/dev loss: 65.0454 / 129.7052
[2017-12-15 18:01:01] Epoch 0069 mean train/dev loss: 64.9819 / 122.7022
[2017-12-15 18:01:07] Epoch 0070 mean train/dev loss: 64.9643 / 128.9843
[2017-12-15 18:01:07] Checkpointing model at epoch 70 for ffn.hl_50_50_50.lr_0.01.wd_0.01
[2017-12-15 18:01:07] Model Checkpointing finished.
[2017-12-15 18:01:13] Epoch 0071 mean train/dev loss: 64.8772 / 134.8180
[2017-12-15 18:01:18] Epoch 0072 mean train/dev loss: 64.7690 / 132.8540
[2017-12-15 18:01:24] Epoch 0073 mean train/dev loss: 64.7839 / 127.4571
[2017-12-15 18:01:30] Epoch 0074 mean train/dev loss: 64.6361 / 133.2665
[2017-12-15 18:01:36] Epoch 0075 mean train/dev loss: 64.6075 / 129.0059
[2017-12-15 18:01:36] Learning rate decayed by 0.5000
[2017-12-15 18:01:42] Epoch 0076 mean train/dev loss: 64.0755 / 132.9143
[2017-12-15 18:01:47] Epoch 0077 mean train/dev loss: 63.9899 / 129.7612
[2017-12-15 18:01:53] Epoch 0078 mean train/dev loss: 64.1216 / 129.3619
[2017-12-15 18:01:59] Epoch 0079 mean train/dev loss: 63.9940 / 134.4574
[2017-12-15 18:02:05] Epoch 0080 mean train/dev loss: 63.9239 / 131.4781
[2017-12-15 18:02:05] Checkpointing model at epoch 80 for ffn.hl_50_50_50.lr_0.01.wd_0.01
[2017-12-15 18:02:05] Model Checkpointing finished.
[2017-12-15 18:02:11] Epoch 0081 mean train/dev loss: 63.9351 / 144.9955
[2017-12-15 18:02:11] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:02:11] 
                       *** Training finished *** 
[2017-12-15 18:02:12] Dev MSE: 144.9955
[2017-12-15 18:02:17] Training MSE: 64.9980
[2017-12-15 18:02:19] Experiment ffn.hl_50_50_50.lr_0.01.wd_0.01 logging ended.
