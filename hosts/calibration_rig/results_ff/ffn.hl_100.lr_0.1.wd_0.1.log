[2017-12-15 16:01:48] Experiment ffn.hl_100.lr_0.1.wd_0.1 logging started.
[2017-12-15 16:01:48] 
                       *** Starting Experiment ffn.hl_100.lr_0.1.wd_0.1 ***
                      
[2017-12-15 16:01:48] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 16:01:48] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 1)
                      )
[2017-12-15 16:01:48]  *** Training on GPU ***
[2017-12-15 16:01:55] Epoch 0001 mean train/dev loss: 8317.0899 / 310.3960
[2017-12-15 16:02:03] Epoch 0002 mean train/dev loss: 160.0924 / 240.0117
[2017-12-15 16:02:10] Epoch 0003 mean train/dev loss: 141.5962 / 311.6307
[2017-12-15 16:02:17] Epoch 0004 mean train/dev loss: 131.7810 / 176.9593
[2017-12-15 16:02:25] Epoch 0005 mean train/dev loss: 134.5097 / 201.4931
[2017-12-15 16:02:32] Epoch 0006 mean train/dev loss: 125.1886 / 207.9477
[2017-12-15 16:02:40] Epoch 0007 mean train/dev loss: 130.2255 / 143.9669
[2017-12-15 16:02:47] Epoch 0008 mean train/dev loss: 124.8476 / 178.3734
[2017-12-15 16:02:54] Epoch 0009 mean train/dev loss: 124.4891 / 141.5672
[2017-12-15 16:03:02] Epoch 0010 mean train/dev loss: 120.7059 / 180.3114
[2017-12-15 16:03:02] Checkpointing model at epoch 10 for ffn.hl_100.lr_0.1.wd_0.1
[2017-12-15 16:03:02] Model Checkpointing finished.
[2017-12-15 16:03:09] Epoch 0011 mean train/dev loss: 120.4152 / 118.3693
[2017-12-15 16:03:17] Epoch 0012 mean train/dev loss: 118.7712 / 124.9893
[2017-12-15 16:03:24] Epoch 0013 mean train/dev loss: 123.1513 / 150.9170
[2017-12-15 16:03:32] Epoch 0014 mean train/dev loss: 118.4825 / 135.0209
[2017-12-15 16:03:39] Epoch 0015 mean train/dev loss: 117.1627 / 118.8870
[2017-12-15 16:03:39] Learning rate decayed by 0.5000
[2017-12-15 16:03:46] Epoch 0016 mean train/dev loss: 111.0484 / 125.9019
[2017-12-15 16:03:53] Epoch 0017 mean train/dev loss: 111.6402 / 112.3517
[2017-12-15 16:04:00] Epoch 0018 mean train/dev loss: 110.8807 / 111.9851
[2017-12-15 16:04:07] Epoch 0019 mean train/dev loss: 110.6163 / 116.9626
[2017-12-15 16:04:15] Epoch 0020 mean train/dev loss: 111.2006 / 111.4768
[2017-12-15 16:04:15] Checkpointing model at epoch 20 for ffn.hl_100.lr_0.1.wd_0.1
[2017-12-15 16:04:15] Model Checkpointing finished.
[2017-12-15 16:04:23] Epoch 0021 mean train/dev loss: 110.9075 / 122.1202
[2017-12-15 16:04:30] Epoch 0022 mean train/dev loss: 110.8028 / 112.5322
[2017-12-15 16:04:37] Epoch 0023 mean train/dev loss: 110.7978 / 113.6569
[2017-12-15 16:04:45] Epoch 0024 mean train/dev loss: 110.8781 / 111.3673
[2017-12-15 16:04:52] Epoch 0025 mean train/dev loss: 110.3869 / 109.4030
[2017-12-15 16:05:00] Epoch 0026 mean train/dev loss: 110.1665 / 116.8055
[2017-12-15 16:05:07] Epoch 0027 mean train/dev loss: 110.2365 / 115.1967
[2017-12-15 16:05:15] Epoch 0028 mean train/dev loss: 110.3897 / 106.9877
[2017-12-15 16:05:21] Epoch 0029 mean train/dev loss: 110.2227 / 112.5265
[2017-12-15 16:05:29] Epoch 0030 mean train/dev loss: 110.4532 / 121.9011
[2017-12-15 16:05:29] Learning rate decayed by 0.5000
[2017-12-15 16:05:29] Checkpointing model at epoch 30 for ffn.hl_100.lr_0.1.wd_0.1
[2017-12-15 16:05:29] Model Checkpointing finished.
[2017-12-15 16:05:37] Epoch 0031 mean train/dev loss: 107.6220 / 113.6369
[2017-12-15 16:05:44] Epoch 0032 mean train/dev loss: 107.5939 / 107.9150
[2017-12-15 16:05:52] Epoch 0033 mean train/dev loss: 107.3435 / 107.6752
[2017-12-15 16:05:59] Epoch 0034 mean train/dev loss: 107.8681 / 118.3932
[2017-12-15 16:06:06] Epoch 0035 mean train/dev loss: 107.4659 / 108.6988
[2017-12-15 16:06:13] Epoch 0036 mean train/dev loss: 107.5226 / 120.2760
[2017-12-15 16:06:21] Epoch 0037 mean train/dev loss: 107.6202 / 108.8564
[2017-12-15 16:06:28] Epoch 0038 mean train/dev loss: 107.6724 / 117.7780
[2017-12-15 16:06:36] Epoch 0039 mean train/dev loss: 107.5633 / 114.0772
[2017-12-15 16:06:42] Epoch 0040 mean train/dev loss: 107.6344 / 115.6341
[2017-12-15 16:06:42] Checkpointing model at epoch 40 for ffn.hl_100.lr_0.1.wd_0.1
[2017-12-15 16:06:43] Model Checkpointing finished.
[2017-12-15 16:06:50] Epoch 0041 mean train/dev loss: 107.7330 / 111.6207
[2017-12-15 16:06:57] Epoch 0042 mean train/dev loss: 107.5068 / 108.0000
[2017-12-15 16:07:04] Epoch 0043 mean train/dev loss: 107.8251 / 107.7451
[2017-12-15 16:07:12] Epoch 0044 mean train/dev loss: 107.5000 / 113.5530
[2017-12-15 16:07:19] Epoch 0045 mean train/dev loss: 107.7471 / 106.7202
[2017-12-15 16:07:19] Learning rate decayed by 0.5000
[2017-12-15 16:07:26] Epoch 0046 mean train/dev loss: 106.1905 / 107.1016
[2017-12-15 16:07:34] Epoch 0047 mean train/dev loss: 106.3377 / 108.9404
[2017-12-15 16:07:41] Epoch 0048 mean train/dev loss: 106.3872 / 107.8314
[2017-12-15 16:07:48] Epoch 0049 mean train/dev loss: 106.4576 / 107.6800
[2017-12-15 16:07:56] Epoch 0050 mean train/dev loss: 106.4741 / 107.6341
[2017-12-15 16:07:56] Checkpointing model at epoch 50 for ffn.hl_100.lr_0.1.wd_0.1
[2017-12-15 16:07:56] Model Checkpointing finished.
[2017-12-15 16:08:03] Epoch 0051 mean train/dev loss: 106.2369 / 114.1761
[2017-12-15 16:08:10] Epoch 0052 mean train/dev loss: 106.5058 / 111.9349
[2017-12-15 16:08:18] Epoch 0053 mean train/dev loss: 106.2460 / 106.7446
[2017-12-15 16:08:24] Epoch 0054 mean train/dev loss: 106.2889 / 108.8286
[2017-12-15 16:08:32] Epoch 0055 mean train/dev loss: 106.3571 / 117.6167
[2017-12-15 16:08:39] Epoch 0056 mean train/dev loss: 106.3958 / 109.4171
[2017-12-15 16:08:46] Epoch 0057 mean train/dev loss: 106.4123 / 110.5084
[2017-12-15 16:08:53] Epoch 0058 mean train/dev loss: 106.4198 / 113.4973
[2017-12-15 16:08:59] Epoch 0059 mean train/dev loss: 106.3920 / 111.6040
[2017-12-15 16:09:05] Epoch 0060 mean train/dev loss: 106.4181 / 111.3249
[2017-12-15 16:09:05] Learning rate decayed by 0.5000
[2017-12-15 16:09:05] Checkpointing model at epoch 60 for ffn.hl_100.lr_0.1.wd_0.1
[2017-12-15 16:09:05] Model Checkpointing finished.
[2017-12-15 16:09:12] Epoch 0061 mean train/dev loss: 105.7147 / 110.5368
[2017-12-15 16:09:18] Epoch 0062 mean train/dev loss: 105.7044 / 111.4163
[2017-12-15 16:09:23] Epoch 0063 mean train/dev loss: 105.7780 / 107.7642
[2017-12-15 16:09:28] Epoch 0064 mean train/dev loss: 105.7105 / 110.7488
[2017-12-15 16:09:33] Epoch 0065 mean train/dev loss: 105.7631 / 110.5199
[2017-12-15 16:09:37] Epoch 0066 mean train/dev loss: 105.8753 / 111.7762
[2017-12-15 16:09:42] Epoch 0067 mean train/dev loss: 105.8147 / 108.1402
[2017-12-15 16:09:47] Epoch 0068 mean train/dev loss: 105.7236 / 108.7222
[2017-12-15 16:09:52] Epoch 0069 mean train/dev loss: 105.7954 / 112.3509
[2017-12-15 16:09:57] Epoch 0070 mean train/dev loss: 105.6805 / 108.4958
[2017-12-15 16:09:57] Checkpointing model at epoch 70 for ffn.hl_100.lr_0.1.wd_0.1
[2017-12-15 16:09:57] Model Checkpointing finished.
[2017-12-15 16:10:02] Epoch 0071 mean train/dev loss: 105.7887 / 107.3892
[2017-12-15 16:10:07] Epoch 0072 mean train/dev loss: 105.7041 / 108.8702
[2017-12-15 16:10:12] Epoch 0073 mean train/dev loss: 105.7206 / 111.3687
[2017-12-15 16:10:17] Epoch 0074 mean train/dev loss: 105.7978 / 109.9468
[2017-12-15 16:10:22] Epoch 0075 mean train/dev loss: 105.6600 / 107.8473
[2017-12-15 16:10:22] Learning rate decayed by 0.5000
[2017-12-15 16:10:27] Epoch 0076 mean train/dev loss: 105.3531 / 107.9155
[2017-12-15 16:10:31] Epoch 0077 mean train/dev loss: 105.3460 / 110.0705
[2017-12-15 16:10:36] Epoch 0078 mean train/dev loss: 105.4225 / 111.4424
[2017-12-15 16:10:41] Epoch 0079 mean train/dev loss: 105.4184 / 108.9944
[2017-12-15 16:10:46] Epoch 0080 mean train/dev loss: 105.4026 / 109.8369
[2017-12-15 16:10:46] Checkpointing model at epoch 80 for ffn.hl_100.lr_0.1.wd_0.1
[2017-12-15 16:10:46] Model Checkpointing finished.
[2017-12-15 16:10:51] Epoch 0081 mean train/dev loss: 105.3931 / 109.8479
[2017-12-15 16:10:56] Epoch 0082 mean train/dev loss: 105.4153 / 108.9716
[2017-12-15 16:11:01] Epoch 0083 mean train/dev loss: 105.3845 / 109.2854
[2017-12-15 16:11:06] Epoch 0084 mean train/dev loss: 105.3478 / 109.0375
[2017-12-15 16:11:11] Epoch 0085 mean train/dev loss: 105.3854 / 108.3537
[2017-12-15 16:11:15] Epoch 0086 mean train/dev loss: 105.3424 / 107.3520
[2017-12-15 16:11:15] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 16:11:15] 
                       *** Training finished *** 
[2017-12-15 16:11:16] Dev MSE: 107.3520
[2017-12-15 16:11:20] Training MSE: 105.5172
[2017-12-15 16:11:22] Experiment ffn.hl_100.lr_0.1.wd_0.1 logging ended.
