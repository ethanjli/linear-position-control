[2017-12-15 14:32:06] Experiment ffn.hl_20.lr_0.1.wd_0.1 logging started.
[2017-12-15 14:32:06] 
                       *** Starting Experiment ffn.hl_20.lr_0.1.wd_0.1 ***
                      
[2017-12-15 14:32:06] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 14:32:09] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 1)
                      )
[2017-12-15 14:32:09]  *** Training on GPU ***
[2017-12-15 14:32:15] Epoch 0001 mean train/dev loss: 15242.1199 / 532.5871
[2017-12-15 14:32:21] Epoch 0002 mean train/dev loss: 199.3993 / 282.2665
[2017-12-15 14:32:27] Epoch 0003 mean train/dev loss: 149.8521 / 208.8804
[2017-12-15 14:32:33] Epoch 0004 mean train/dev loss: 133.7197 / 182.6537
[2017-12-15 14:32:39] Epoch 0005 mean train/dev loss: 128.2772 / 188.1768
[2017-12-15 14:32:44] Epoch 0006 mean train/dev loss: 125.0106 / 137.4875
[2017-12-15 14:32:50] Epoch 0007 mean train/dev loss: 118.7894 / 128.8302
[2017-12-15 14:32:55] Epoch 0008 mean train/dev loss: 118.7384 / 143.8330
[2017-12-15 14:33:01] Epoch 0009 mean train/dev loss: 122.0900 / 149.2263
[2017-12-15 14:33:07] Epoch 0010 mean train/dev loss: 116.9963 / 136.1188
[2017-12-15 14:33:07] Checkpointing model at epoch 10 for ffn.hl_20.lr_0.1.wd_0.1
[2017-12-15 14:33:08] Model Checkpointing finished.
[2017-12-15 14:33:13] Epoch 0011 mean train/dev loss: 116.9860 / 126.4083
[2017-12-15 14:33:19] Epoch 0012 mean train/dev loss: 116.6547 / 132.1369
[2017-12-15 14:33:25] Epoch 0013 mean train/dev loss: 116.0211 / 114.8531
[2017-12-15 14:33:31] Epoch 0014 mean train/dev loss: 115.5382 / 120.0736
[2017-12-15 14:33:36] Epoch 0015 mean train/dev loss: 118.5598 / 131.4108
[2017-12-15 14:33:36] Learning rate decayed by 0.5000
[2017-12-15 14:33:42] Epoch 0016 mean train/dev loss: 111.8500 / 119.5507
[2017-12-15 14:33:49] Epoch 0017 mean train/dev loss: 111.8666 / 118.0339
[2017-12-15 14:33:55] Epoch 0018 mean train/dev loss: 112.2954 / 127.2377
[2017-12-15 14:34:00] Epoch 0019 mean train/dev loss: 112.7616 / 118.6071
[2017-12-15 14:34:07] Epoch 0020 mean train/dev loss: 112.3288 / 116.3352
[2017-12-15 14:34:07] Checkpointing model at epoch 20 for ffn.hl_20.lr_0.1.wd_0.1
[2017-12-15 14:34:07] Model Checkpointing finished.
[2017-12-15 14:34:13] Epoch 0021 mean train/dev loss: 112.1708 / 117.1153
[2017-12-15 14:34:18] Epoch 0022 mean train/dev loss: 112.2555 / 128.3410
[2017-12-15 14:34:25] Epoch 0023 mean train/dev loss: 112.8059 / 118.4729
[2017-12-15 14:34:30] Epoch 0024 mean train/dev loss: 111.6740 / 115.1427
[2017-12-15 14:34:36] Epoch 0025 mean train/dev loss: 111.0880 / 109.5994
[2017-12-15 14:34:42] Epoch 0026 mean train/dev loss: 110.4112 / 111.3048
[2017-12-15 14:34:49] Epoch 0027 mean train/dev loss: 110.4909 / 117.1182
[2017-12-15 14:34:55] Epoch 0028 mean train/dev loss: 110.5843 / 112.3036
[2017-12-15 14:35:01] Epoch 0029 mean train/dev loss: 110.7413 / 111.0012
[2017-12-15 14:35:07] Epoch 0030 mean train/dev loss: 110.3745 / 116.5988
[2017-12-15 14:35:07] Learning rate decayed by 0.5000
[2017-12-15 14:35:07] Checkpointing model at epoch 30 for ffn.hl_20.lr_0.1.wd_0.1
[2017-12-15 14:35:07] Model Checkpointing finished.
[2017-12-15 14:35:13] Epoch 0031 mean train/dev loss: 108.8646 / 107.5068
[2017-12-15 14:35:18] Epoch 0032 mean train/dev loss: 108.7246 / 112.7902
[2017-12-15 14:35:24] Epoch 0033 mean train/dev loss: 108.9018 / 110.4949
[2017-12-15 14:35:30] Epoch 0034 mean train/dev loss: 108.8978 / 109.4816
[2017-12-15 14:35:36] Epoch 0035 mean train/dev loss: 108.7120 / 112.5435
[2017-12-15 14:35:43] Epoch 0036 mean train/dev loss: 108.8363 / 108.0302
[2017-12-15 14:35:48] Epoch 0037 mean train/dev loss: 108.9568 / 106.6087
[2017-12-15 14:35:55] Epoch 0038 mean train/dev loss: 109.1328 / 107.5284
[2017-12-15 14:36:01] Epoch 0039 mean train/dev loss: 108.7734 / 107.1089
[2017-12-15 14:36:07] Epoch 0040 mean train/dev loss: 108.7512 / 106.1140
[2017-12-15 14:36:07] Checkpointing model at epoch 40 for ffn.hl_20.lr_0.1.wd_0.1
[2017-12-15 14:36:08] Model Checkpointing finished.
[2017-12-15 14:36:14] Epoch 0041 mean train/dev loss: 108.8630 / 107.5642
[2017-12-15 14:36:19] Epoch 0042 mean train/dev loss: 108.8318 / 107.8425
[2017-12-15 14:36:26] Epoch 0043 mean train/dev loss: 108.9577 / 105.9682
[2017-12-15 14:36:32] Epoch 0044 mean train/dev loss: 108.8250 / 112.5614
[2017-12-15 14:36:37] Epoch 0045 mean train/dev loss: 108.7850 / 109.5359
[2017-12-15 14:36:37] Learning rate decayed by 0.5000
[2017-12-15 14:36:43] Epoch 0046 mean train/dev loss: 107.8821 / 110.3590
[2017-12-15 14:36:49] Epoch 0047 mean train/dev loss: 107.8854 / 104.9214
[2017-12-15 14:36:55] Epoch 0048 mean train/dev loss: 108.0337 / 112.3309
[2017-12-15 14:37:00] Epoch 0049 mean train/dev loss: 107.9220 / 108.7771
[2017-12-15 14:37:06] Epoch 0050 mean train/dev loss: 107.8572 / 108.3287
[2017-12-15 14:37:06] Checkpointing model at epoch 50 for ffn.hl_20.lr_0.1.wd_0.1
[2017-12-15 14:37:06] Model Checkpointing finished.
[2017-12-15 14:37:11] Epoch 0051 mean train/dev loss: 108.0483 / 110.9849
[2017-12-15 14:37:17] Epoch 0052 mean train/dev loss: 107.9108 / 107.2334
[2017-12-15 14:37:24] Epoch 0053 mean train/dev loss: 108.0162 / 108.8944
[2017-12-15 14:37:30] Epoch 0054 mean train/dev loss: 107.9739 / 113.2173
[2017-12-15 14:37:35] Epoch 0055 mean train/dev loss: 107.8981 / 108.3635
[2017-12-15 14:37:41] Epoch 0056 mean train/dev loss: 107.8204 / 109.2321
[2017-12-15 14:37:47] Epoch 0057 mean train/dev loss: 107.7751 / 108.4669
[2017-12-15 14:37:53] Epoch 0058 mean train/dev loss: 107.9911 / 107.3296
[2017-12-15 14:37:59] Epoch 0059 mean train/dev loss: 107.9286 / 106.0181
[2017-12-15 14:38:05] Epoch 0060 mean train/dev loss: 107.8543 / 106.3455
[2017-12-15 14:38:05] Learning rate decayed by 0.5000
[2017-12-15 14:38:05] Checkpointing model at epoch 60 for ffn.hl_20.lr_0.1.wd_0.1
[2017-12-15 14:38:05] Model Checkpointing finished.
[2017-12-15 14:38:11] Epoch 0061 mean train/dev loss: 107.3661 / 108.4391
[2017-12-15 14:38:17] Epoch 0062 mean train/dev loss: 107.3470 / 106.7787
[2017-12-15 14:38:23] Epoch 0063 mean train/dev loss: 107.4412 / 106.7711
[2017-12-15 14:38:29] Epoch 0064 mean train/dev loss: 107.3550 / 106.7996
[2017-12-15 14:38:35] Epoch 0065 mean train/dev loss: 107.4416 / 108.1198
[2017-12-15 14:38:41] Epoch 0066 mean train/dev loss: 107.4059 / 106.9716
[2017-12-15 14:38:47] Epoch 0067 mean train/dev loss: 107.3827 / 106.5560
[2017-12-15 14:38:54] Epoch 0068 mean train/dev loss: 107.3709 / 108.6416
[2017-12-15 14:39:00] Epoch 0069 mean train/dev loss: 107.3670 / 108.1283
[2017-12-15 14:39:06] Epoch 0070 mean train/dev loss: 107.4423 / 107.4925
[2017-12-15 14:39:06] Checkpointing model at epoch 70 for ffn.hl_20.lr_0.1.wd_0.1
[2017-12-15 14:39:06] Model Checkpointing finished.
[2017-12-15 14:39:13] Epoch 0071 mean train/dev loss: 107.3869 / 106.6835
[2017-12-15 14:39:18] Epoch 0072 mean train/dev loss: 107.3693 / 108.1228
[2017-12-15 14:39:24] Epoch 0073 mean train/dev loss: 107.4355 / 104.9272
[2017-12-15 14:39:30] Epoch 0074 mean train/dev loss: 107.4361 / 109.6183
[2017-12-15 14:39:36] Epoch 0075 mean train/dev loss: 107.3627 / 107.2645
[2017-12-15 14:39:36] Learning rate decayed by 0.5000
[2017-12-15 14:39:43] Epoch 0076 mean train/dev loss: 107.1031 / 107.6080
[2017-12-15 14:39:48] Epoch 0077 mean train/dev loss: 107.0882 / 108.4101
[2017-12-15 14:39:55] Epoch 0078 mean train/dev loss: 107.1301 / 108.6256
[2017-12-15 14:40:01] Epoch 0079 mean train/dev loss: 107.1202 / 107.4454
[2017-12-15 14:40:06] Epoch 0080 mean train/dev loss: 107.1046 / 107.3444
[2017-12-15 14:40:06] Checkpointing model at epoch 80 for ffn.hl_20.lr_0.1.wd_0.1
[2017-12-15 14:40:07] Model Checkpointing finished.
[2017-12-15 14:40:13] Epoch 0081 mean train/dev loss: 107.1330 / 107.5365
[2017-12-15 14:40:19] Epoch 0082 mean train/dev loss: 107.1047 / 106.9170
[2017-12-15 14:40:24] Epoch 0083 mean train/dev loss: 107.0891 / 108.6735
[2017-12-15 14:40:30] Epoch 0084 mean train/dev loss: 107.0918 / 108.8451
[2017-12-15 14:40:36] Epoch 0085 mean train/dev loss: 107.0620 / 107.1237
[2017-12-15 14:40:41] Epoch 0086 mean train/dev loss: 107.0585 / 109.1760
[2017-12-15 14:40:47] Epoch 0087 mean train/dev loss: 107.0619 / 109.2008
[2017-12-15 14:40:52] Epoch 0088 mean train/dev loss: 107.0845 / 106.1624
[2017-12-15 14:40:52] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:40:52] 
                       *** Training finished *** 
[2017-12-15 14:40:53] Dev MSE: 106.1624
[2017-12-15 14:40:57] Training MSE: 107.0704
[2017-12-15 14:40:58] Experiment ffn.hl_20.lr_0.1.wd_0.1 logging ended.
