[2017-12-15 17:41:06] Experiment ffn.hl_50_50.lr_0.01.wd_0.1 logging started.
[2017-12-15 17:41:06] 
                       *** Starting Experiment ffn.hl_50_50.lr_0.01.wd_0.1 ***
                      
[2017-12-15 17:41:06] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 17:41:06] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 1)
                      )
[2017-12-15 17:41:06]  *** Training on GPU ***
[2017-12-15 17:41:14] Epoch 0001 mean train/dev loss: 22234.4004 / 294.0811
[2017-12-15 17:41:23] Epoch 0002 mean train/dev loss: 128.2113 / 140.2936
[2017-12-15 17:41:31] Epoch 0003 mean train/dev loss: 114.2936 / 133.3128
[2017-12-15 17:41:39] Epoch 0004 mean train/dev loss: 111.4511 / 130.4151
[2017-12-15 17:41:47] Epoch 0005 mean train/dev loss: 109.5425 / 137.5470
[2017-12-15 17:41:54] Epoch 0006 mean train/dev loss: 108.7773 / 123.6499
[2017-12-15 17:42:02] Epoch 0007 mean train/dev loss: 107.5683 / 112.6895
[2017-12-15 17:42:10] Epoch 0008 mean train/dev loss: 105.3203 / 120.2012
[2017-12-15 17:42:18] Epoch 0009 mean train/dev loss: 102.8970 / 128.0958
[2017-12-15 17:42:27] Epoch 0010 mean train/dev loss: 100.4784 / 136.4754
[2017-12-15 17:42:27] Checkpointing model at epoch 10 for ffn.hl_50_50.lr_0.01.wd_0.1
[2017-12-15 17:42:27] Model Checkpointing finished.
[2017-12-15 17:42:35] Epoch 0011 mean train/dev loss: 97.6283 / 128.6267
[2017-12-15 17:42:43] Epoch 0012 mean train/dev loss: 95.0389 / 128.4136
[2017-12-15 17:42:51] Epoch 0013 mean train/dev loss: 94.0265 / 128.7373
[2017-12-15 17:42:59] Epoch 0014 mean train/dev loss: 92.5389 / 113.4137
[2017-12-15 17:43:07] Epoch 0015 mean train/dev loss: 91.3711 / 128.1292
[2017-12-15 17:43:07] Learning rate decayed by 0.5000
[2017-12-15 17:43:15] Epoch 0016 mean train/dev loss: 86.8548 / 107.9053
[2017-12-15 17:43:23] Epoch 0017 mean train/dev loss: 86.6568 / 131.7253
[2017-12-15 17:43:31] Epoch 0018 mean train/dev loss: 86.1198 / 121.4805
[2017-12-15 17:43:39] Epoch 0019 mean train/dev loss: 85.9090 / 105.6678
[2017-12-15 17:43:47] Epoch 0020 mean train/dev loss: 85.1016 / 115.6247
[2017-12-15 17:43:47] Checkpointing model at epoch 20 for ffn.hl_50_50.lr_0.01.wd_0.1
[2017-12-15 17:43:47] Model Checkpointing finished.
[2017-12-15 17:43:55] Epoch 0021 mean train/dev loss: 84.9155 / 129.1579
[2017-12-15 17:44:04] Epoch 0022 mean train/dev loss: 83.7136 / 113.0300
[2017-12-15 17:44:12] Epoch 0023 mean train/dev loss: 83.9743 / 113.9015
[2017-12-15 17:44:20] Epoch 0024 mean train/dev loss: 82.8492 / 123.8910
[2017-12-15 17:44:28] Epoch 0025 mean train/dev loss: 82.7190 / 113.0665
[2017-12-15 17:44:35] Epoch 0026 mean train/dev loss: 82.2454 / 122.7071
[2017-12-15 17:44:44] Epoch 0027 mean train/dev loss: 82.1170 / 113.6985
[2017-12-15 17:44:51] Epoch 0028 mean train/dev loss: 81.0821 / 118.4182
[2017-12-15 17:44:59] Epoch 0029 mean train/dev loss: 81.1169 / 108.6437
[2017-12-15 17:45:07] Epoch 0030 mean train/dev loss: 80.5557 / 123.8427
[2017-12-15 17:45:07] Learning rate decayed by 0.5000
[2017-12-15 17:45:07] Checkpointing model at epoch 30 for ffn.hl_50_50.lr_0.01.wd_0.1
[2017-12-15 17:45:07] Model Checkpointing finished.
[2017-12-15 17:45:16] Epoch 0031 mean train/dev loss: 78.2626 / 113.3217
[2017-12-15 17:45:24] Epoch 0032 mean train/dev loss: 78.0834 / 101.5036
[2017-12-15 17:45:32] Epoch 0033 mean train/dev loss: 77.9262 / 113.4372
[2017-12-15 17:45:40] Epoch 0034 mean train/dev loss: 77.8355 / 118.2507
[2017-12-15 17:45:48] Epoch 0035 mean train/dev loss: 77.8494 / 118.8584
[2017-12-15 17:45:56] Epoch 0036 mean train/dev loss: 77.6321 / 114.6003
[2017-12-15 17:46:04] Epoch 0037 mean train/dev loss: 77.2507 / 121.0410
[2017-12-15 17:46:12] Epoch 0038 mean train/dev loss: 77.1692 / 132.3451
[2017-12-15 17:46:20] Epoch 0039 mean train/dev loss: 77.2336 / 134.3705
[2017-12-15 17:46:28] Epoch 0040 mean train/dev loss: 77.1662 / 127.3747
[2017-12-15 17:46:28] Checkpointing model at epoch 40 for ffn.hl_50_50.lr_0.01.wd_0.1
[2017-12-15 17:46:28] Model Checkpointing finished.
[2017-12-15 17:46:36] Epoch 0041 mean train/dev loss: 76.7153 / 117.0550
[2017-12-15 17:46:44] Epoch 0042 mean train/dev loss: 76.6090 / 138.5035
[2017-12-15 17:46:52] Epoch 0043 mean train/dev loss: 76.4287 / 130.7938
[2017-12-15 17:47:00] Epoch 0044 mean train/dev loss: 76.3549 / 112.9842
[2017-12-15 17:47:08] Epoch 0045 mean train/dev loss: 76.2511 / 113.4017
[2017-12-15 17:47:08] Learning rate decayed by 0.5000
[2017-12-15 17:47:16] Epoch 0046 mean train/dev loss: 75.1771 / 120.7812
[2017-12-15 17:47:24] Epoch 0047 mean train/dev loss: 75.1511 / 119.0369
[2017-12-15 17:47:32] Epoch 0048 mean train/dev loss: 75.0415 / 121.2815
[2017-12-15 17:47:40] Epoch 0049 mean train/dev loss: 74.9452 / 116.8003
[2017-12-15 17:47:48] Epoch 0050 mean train/dev loss: 74.9360 / 128.0579
[2017-12-15 17:47:48] Checkpointing model at epoch 50 for ffn.hl_50_50.lr_0.01.wd_0.1
[2017-12-15 17:47:48] Model Checkpointing finished.
[2017-12-15 17:47:57] Epoch 0051 mean train/dev loss: 74.7215 / 116.9591
[2017-12-15 17:48:05] Epoch 0052 mean train/dev loss: 74.6151 / 115.4943
[2017-12-15 17:48:14] Epoch 0053 mean train/dev loss: 74.4899 / 128.4545
[2017-12-15 17:48:21] Epoch 0054 mean train/dev loss: 74.3816 / 114.0203
[2017-12-15 17:48:29] Epoch 0055 mean train/dev loss: 74.3245 / 121.8198
[2017-12-15 17:48:37] Epoch 0056 mean train/dev loss: 74.2980 / 123.6604
[2017-12-15 17:48:46] Epoch 0057 mean train/dev loss: 74.1500 / 118.5450
[2017-12-15 17:48:54] Epoch 0058 mean train/dev loss: 74.1792 / 120.0007
[2017-12-15 17:49:02] Epoch 0059 mean train/dev loss: 73.9872 / 115.8169
[2017-12-15 17:49:10] Epoch 0060 mean train/dev loss: 73.7860 / 111.0155
[2017-12-15 17:49:10] Learning rate decayed by 0.5000
[2017-12-15 17:49:10] Checkpointing model at epoch 60 for ffn.hl_50_50.lr_0.01.wd_0.1
[2017-12-15 17:49:10] Model Checkpointing finished.
[2017-12-15 17:49:18] Epoch 0061 mean train/dev loss: 73.2588 / 124.2784
[2017-12-15 17:49:27] Epoch 0062 mean train/dev loss: 73.2299 / 116.8424
[2017-12-15 17:49:35] Epoch 0063 mean train/dev loss: 73.1708 / 123.5354
[2017-12-15 17:49:42] Epoch 0064 mean train/dev loss: 73.1537 / 116.9703
[2017-12-15 17:49:50] Epoch 0065 mean train/dev loss: 73.1713 / 121.1184
[2017-12-15 17:49:57] Epoch 0066 mean train/dev loss: 73.0744 / 111.4778
[2017-12-15 17:50:04] Epoch 0067 mean train/dev loss: 73.0248 / 111.5500
[2017-12-15 17:50:11] Epoch 0068 mean train/dev loss: 72.8512 / 111.0665
[2017-12-15 17:50:18] Epoch 0069 mean train/dev loss: 72.8719 / 107.7438
[2017-12-15 17:50:24] Epoch 0070 mean train/dev loss: 72.8136 / 108.1779
[2017-12-15 17:50:24] Checkpointing model at epoch 70 for ffn.hl_50_50.lr_0.01.wd_0.1
[2017-12-15 17:50:24] Model Checkpointing finished.
[2017-12-15 17:50:31] Epoch 0071 mean train/dev loss: 72.8251 / 112.0346
[2017-12-15 17:50:36] Epoch 0072 mean train/dev loss: 72.6781 / 110.3078
[2017-12-15 17:50:41] Epoch 0073 mean train/dev loss: 72.6207 / 113.6811
[2017-12-15 17:50:41] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:50:41] 
                       *** Training finished *** 
[2017-12-15 17:50:43] Dev MSE: 113.6811
[2017-12-15 17:50:47] Training MSE: 73.2841
[2017-12-15 17:50:49] Experiment ffn.hl_50_50.lr_0.01.wd_0.1 logging ended.
