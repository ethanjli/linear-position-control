[2017-12-15 18:20:01] Experiment ffn.hl_100_100.lr_0.01.wd_0.1 logging started.
[2017-12-15 18:20:01] 
                       *** Starting Experiment ffn.hl_100_100.lr_0.01.wd_0.1 ***
                      
[2017-12-15 18:20:01] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100, 100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 18:20:01] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 100)
                        (relu2): ReLU ()
                        (linear3): Linear (100 -> 1)
                      )
[2017-12-15 18:20:01]  *** Training on GPU ***
[2017-12-15 18:20:09] Epoch 0001 mean train/dev loss: 14410.4900 / 186.8715
[2017-12-15 18:20:18] Epoch 0002 mean train/dev loss: 121.3020 / 132.6942
[2017-12-15 18:20:25] Epoch 0003 mean train/dev loss: 112.0441 / 118.6618
[2017-12-15 18:20:33] Epoch 0004 mean train/dev loss: 108.0787 / 121.4486
[2017-12-15 18:20:40] Epoch 0005 mean train/dev loss: 104.9175 / 114.0252
[2017-12-15 18:20:48] Epoch 0006 mean train/dev loss: 103.1536 / 113.6197
[2017-12-15 18:20:56] Epoch 0007 mean train/dev loss: 100.8102 / 127.7072
[2017-12-15 18:21:04] Epoch 0008 mean train/dev loss: 97.8406 / 107.8544
[2017-12-15 18:21:12] Epoch 0009 mean train/dev loss: 96.3729 / 122.8145
[2017-12-15 18:21:20] Epoch 0010 mean train/dev loss: 93.8689 / 113.7414
[2017-12-15 18:21:20] Checkpointing model at epoch 10 for ffn.hl_100_100.lr_0.01.wd_0.1
[2017-12-15 18:21:20] Model Checkpointing finished.
[2017-12-15 18:21:29] Epoch 0011 mean train/dev loss: 92.4445 / 114.7755
[2017-12-15 18:21:37] Epoch 0012 mean train/dev loss: 89.4985 / 134.7865
[2017-12-15 18:21:44] Epoch 0013 mean train/dev loss: 89.2332 / 116.1888
[2017-12-15 18:21:52] Epoch 0014 mean train/dev loss: 86.8303 / 108.7030
[2017-12-15 18:22:00] Epoch 0015 mean train/dev loss: 84.7855 / 160.9226
[2017-12-15 18:22:00] Learning rate decayed by 0.5000
[2017-12-15 18:22:08] Epoch 0016 mean train/dev loss: 77.4403 / 135.0945
[2017-12-15 18:22:17] Epoch 0017 mean train/dev loss: 77.3267 / 147.5809
[2017-12-15 18:22:25] Epoch 0018 mean train/dev loss: 77.9568 / 118.4242
[2017-12-15 18:22:33] Epoch 0019 mean train/dev loss: 76.7445 / 131.3211
[2017-12-15 18:22:41] Epoch 0020 mean train/dev loss: 77.4122 / 123.9011
[2017-12-15 18:22:41] Checkpointing model at epoch 20 for ffn.hl_100_100.lr_0.01.wd_0.1
[2017-12-15 18:22:41] Model Checkpointing finished.
[2017-12-15 18:22:49] Epoch 0021 mean train/dev loss: 76.4429 / 110.1535
[2017-12-15 18:22:58] Epoch 0022 mean train/dev loss: 76.3997 / 102.0043
[2017-12-15 18:23:06] Epoch 0023 mean train/dev loss: 75.8675 / 111.0589
[2017-12-15 18:23:14] Epoch 0024 mean train/dev loss: 75.0686 / 108.4650
[2017-12-15 18:23:22] Epoch 0025 mean train/dev loss: 75.7842 / 116.8559
[2017-12-15 18:23:30] Epoch 0026 mean train/dev loss: 75.1194 / 118.7786
[2017-12-15 18:23:38] Epoch 0027 mean train/dev loss: 76.9018 / 123.8301
[2017-12-15 18:23:46] Epoch 0028 mean train/dev loss: 74.5511 / 89.3415
[2017-12-15 18:23:54] Epoch 0029 mean train/dev loss: 74.7241 / 108.3630
[2017-12-15 18:24:03] Epoch 0030 mean train/dev loss: 74.4058 / 93.7897
[2017-12-15 18:24:03] Learning rate decayed by 0.5000
[2017-12-15 18:24:03] Checkpointing model at epoch 30 for ffn.hl_100_100.lr_0.01.wd_0.1
[2017-12-15 18:24:03] Model Checkpointing finished.
[2017-12-15 18:24:11] Epoch 0031 mean train/dev loss: 71.4006 / 102.8935
[2017-12-15 18:24:19] Epoch 0032 mean train/dev loss: 71.2902 / 109.4707
[2017-12-15 18:24:28] Epoch 0033 mean train/dev loss: 71.4558 / 102.9426
[2017-12-15 18:24:35] Epoch 0034 mean train/dev loss: 71.7162 / 102.2838
[2017-12-15 18:24:43] Epoch 0035 mean train/dev loss: 71.4666 / 123.4598
[2017-12-15 18:24:51] Epoch 0036 mean train/dev loss: 71.2507 / 113.0599
[2017-12-15 18:24:59] Epoch 0037 mean train/dev loss: 71.2735 / 105.8565
[2017-12-15 18:25:07] Epoch 0038 mean train/dev loss: 71.3475 / 96.4062
[2017-12-15 18:25:15] Epoch 0039 mean train/dev loss: 70.9347 / 114.6628
[2017-12-15 18:25:23] Epoch 0040 mean train/dev loss: 71.2186 / 103.0882
[2017-12-15 18:25:23] Checkpointing model at epoch 40 for ffn.hl_100_100.lr_0.01.wd_0.1
[2017-12-15 18:25:24] Model Checkpointing finished.
[2017-12-15 18:25:32] Epoch 0041 mean train/dev loss: 70.8245 / 119.3438
[2017-12-15 18:25:39] Epoch 0042 mean train/dev loss: 71.1172 / 116.2844
[2017-12-15 18:25:47] Epoch 0043 mean train/dev loss: 70.9497 / 113.9884
[2017-12-15 18:25:55] Epoch 0044 mean train/dev loss: 70.8370 / 93.9908
[2017-12-15 18:26:03] Epoch 0045 mean train/dev loss: 70.8738 / 113.3853
[2017-12-15 18:26:03] Learning rate decayed by 0.5000
[2017-12-15 18:26:11] Epoch 0046 mean train/dev loss: 69.1678 / 115.8610
[2017-12-15 18:26:19] Epoch 0047 mean train/dev loss: 69.3231 / 113.5599
[2017-12-15 18:26:27] Epoch 0048 mean train/dev loss: 69.2858 / 115.2710
[2017-12-15 18:26:35] Epoch 0049 mean train/dev loss: 68.9544 / 104.6649
[2017-12-15 18:26:43] Epoch 0050 mean train/dev loss: 69.2135 / 122.7265
[2017-12-15 18:26:43] Checkpointing model at epoch 50 for ffn.hl_100_100.lr_0.01.wd_0.1
[2017-12-15 18:26:43] Model Checkpointing finished.
[2017-12-15 18:26:51] Epoch 0051 mean train/dev loss: 69.1190 / 106.1246
[2017-12-15 18:27:00] Epoch 0052 mean train/dev loss: 68.8886 / 123.5313
[2017-12-15 18:27:08] Epoch 0053 mean train/dev loss: 69.0297 / 113.3454
[2017-12-15 18:27:16] Epoch 0054 mean train/dev loss: 68.7980 / 114.1710
[2017-12-15 18:27:24] Epoch 0055 mean train/dev loss: 68.7504 / 96.1576
[2017-12-15 18:27:32] Epoch 0056 mean train/dev loss: 68.6066 / 113.4809
[2017-12-15 18:27:40] Epoch 0057 mean train/dev loss: 68.7305 / 121.0782
[2017-12-15 18:27:48] Epoch 0058 mean train/dev loss: 68.5452 / 104.1475
[2017-12-15 18:27:56] Epoch 0059 mean train/dev loss: 68.5495 / 116.9069
[2017-12-15 18:28:04] Epoch 0060 mean train/dev loss: 68.2718 / 127.6469
[2017-12-15 18:28:04] Learning rate decayed by 0.5000
[2017-12-15 18:28:04] Checkpointing model at epoch 60 for ffn.hl_100_100.lr_0.01.wd_0.1
[2017-12-15 18:28:04] Model Checkpointing finished.
[2017-12-15 18:28:13] Epoch 0061 mean train/dev loss: 67.6429 / 104.3221
[2017-12-15 18:28:21] Epoch 0062 mean train/dev loss: 67.7503 / 111.2453
[2017-12-15 18:28:29] Epoch 0063 mean train/dev loss: 67.6002 / 117.2123
[2017-12-15 18:28:37] Epoch 0064 mean train/dev loss: 67.6215 / 122.1784
[2017-12-15 18:28:45] Epoch 0065 mean train/dev loss: 67.6556 / 117.1664
[2017-12-15 18:28:53] Epoch 0066 mean train/dev loss: 67.5860 / 109.0414
[2017-12-15 18:29:01] Epoch 0067 mean train/dev loss: 67.5672 / 107.0148
[2017-12-15 18:29:09] Epoch 0068 mean train/dev loss: 67.5185 / 117.7236
[2017-12-15 18:29:17] Epoch 0069 mean train/dev loss: 67.5509 / 114.7782
[2017-12-15 18:29:17] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:29:17] 
                       *** Training finished *** 
[2017-12-15 18:29:19] Dev MSE: 114.7782
[2017-12-15 18:29:26] Training MSE: 67.2386
[2017-12-15 18:29:28] Experiment ffn.hl_100_100.lr_0.01.wd_0.1 logging ended.
