[2017-12-15 17:41:05] Experiment ffn.hl_50_50.lr_0.01.wd_0.001 logging started.
[2017-12-15 17:41:05] 
                       *** Starting Experiment ffn.hl_50_50.lr_0.01.wd_0.001 ***
                      
[2017-12-15 17:41:05] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50, 50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 17:41:05] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 50)
                        (relu2): ReLU ()
                        (linear3): Linear (50 -> 1)
                      )
[2017-12-15 17:41:05]  *** Training on GPU ***
[2017-12-15 17:41:14] Epoch 0001 mean train/dev loss: 20463.1498 / 620.6288
[2017-12-15 17:41:21] Epoch 0002 mean train/dev loss: 241.8895 / 297.8432
[2017-12-15 17:41:29] Epoch 0003 mean train/dev loss: 167.1332 / 199.4314
[2017-12-15 17:41:37] Epoch 0004 mean train/dev loss: 131.1753 / 171.7601
[2017-12-15 17:41:45] Epoch 0005 mean train/dev loss: 112.7131 / 154.9557
[2017-12-15 17:41:53] Epoch 0006 mean train/dev loss: 108.0476 / 149.5707
[2017-12-15 17:42:01] Epoch 0007 mean train/dev loss: 105.9532 / 141.3616
[2017-12-15 17:42:09] Epoch 0008 mean train/dev loss: 103.2613 / 136.9718
[2017-12-15 17:42:17] Epoch 0009 mean train/dev loss: 101.2563 / 149.6589
[2017-12-15 17:42:25] Epoch 0010 mean train/dev loss: 97.1876 / 186.4643
[2017-12-15 17:42:25] Checkpointing model at epoch 10 for ffn.hl_50_50.lr_0.01.wd_0.001
[2017-12-15 17:42:25] Model Checkpointing finished.
[2017-12-15 17:42:34] Epoch 0011 mean train/dev loss: 94.1634 / 118.1271
[2017-12-15 17:42:42] Epoch 0012 mean train/dev loss: 89.9379 / 115.1398
[2017-12-15 17:42:50] Epoch 0013 mean train/dev loss: 84.0859 / 111.5736
[2017-12-15 17:42:58] Epoch 0014 mean train/dev loss: 83.5013 / 133.5448
[2017-12-15 17:43:06] Epoch 0015 mean train/dev loss: 82.3474 / 120.0197
[2017-12-15 17:43:06] Learning rate decayed by 0.5000
[2017-12-15 17:43:14] Epoch 0016 mean train/dev loss: 76.1543 / 119.5138
[2017-12-15 17:43:22] Epoch 0017 mean train/dev loss: 75.7811 / 119.1602
[2017-12-15 17:43:30] Epoch 0018 mean train/dev loss: 75.1105 / 117.5384
[2017-12-15 17:43:38] Epoch 0019 mean train/dev loss: 75.0143 / 111.9859
[2017-12-15 17:43:46] Epoch 0020 mean train/dev loss: 74.4386 / 118.0555
[2017-12-15 17:43:46] Checkpointing model at epoch 20 for ffn.hl_50_50.lr_0.01.wd_0.001
[2017-12-15 17:43:46] Model Checkpointing finished.
[2017-12-15 17:43:54] Epoch 0021 mean train/dev loss: 73.8648 / 111.0414
[2017-12-15 17:44:02] Epoch 0022 mean train/dev loss: 73.9768 / 137.0137
[2017-12-15 17:44:10] Epoch 0023 mean train/dev loss: 72.8674 / 129.4362
[2017-12-15 17:44:18] Epoch 0024 mean train/dev loss: 72.1531 / 140.0657
[2017-12-15 17:44:26] Epoch 0025 mean train/dev loss: 72.3364 / 143.9658
[2017-12-15 17:44:35] Epoch 0026 mean train/dev loss: 72.1454 / 121.5547
[2017-12-15 17:44:43] Epoch 0027 mean train/dev loss: 71.2993 / 123.0035
[2017-12-15 17:44:51] Epoch 0028 mean train/dev loss: 70.8041 / 133.6660
[2017-12-15 17:44:59] Epoch 0029 mean train/dev loss: 71.2697 / 125.0197
[2017-12-15 17:45:07] Epoch 0030 mean train/dev loss: 70.5639 / 127.1732
[2017-12-15 17:45:07] Learning rate decayed by 0.5000
[2017-12-15 17:45:07] Checkpointing model at epoch 30 for ffn.hl_50_50.lr_0.01.wd_0.001
[2017-12-15 17:45:07] Model Checkpointing finished.
[2017-12-15 17:45:15] Epoch 0031 mean train/dev loss: 68.3452 / 125.5042
[2017-12-15 17:45:23] Epoch 0032 mean train/dev loss: 68.5152 / 134.0067
[2017-12-15 17:45:31] Epoch 0033 mean train/dev loss: 68.3860 / 138.1970
[2017-12-15 17:45:39] Epoch 0034 mean train/dev loss: 68.3376 / 124.8433
[2017-12-15 17:45:47] Epoch 0035 mean train/dev loss: 68.1085 / 140.2648
[2017-12-15 17:45:56] Epoch 0036 mean train/dev loss: 68.1737 / 124.9804
[2017-12-15 17:46:04] Epoch 0037 mean train/dev loss: 67.9118 / 131.3928
[2017-12-15 17:46:12] Epoch 0038 mean train/dev loss: 67.8497 / 128.0256
[2017-12-15 17:46:20] Epoch 0039 mean train/dev loss: 67.7123 / 133.3574
[2017-12-15 17:46:28] Epoch 0040 mean train/dev loss: 67.4068 / 133.0785
[2017-12-15 17:46:28] Checkpointing model at epoch 40 for ffn.hl_50_50.lr_0.01.wd_0.001
[2017-12-15 17:46:29] Model Checkpointing finished.
[2017-12-15 17:46:36] Epoch 0041 mean train/dev loss: 67.5977 / 130.1993
[2017-12-15 17:46:44] Epoch 0042 mean train/dev loss: 67.4480 / 145.0754
[2017-12-15 17:46:52] Epoch 0043 mean train/dev loss: 67.6533 / 129.5339
[2017-12-15 17:47:00] Epoch 0044 mean train/dev loss: 67.3125 / 124.6175
[2017-12-15 17:47:08] Epoch 0045 mean train/dev loss: 67.1028 / 127.6179
[2017-12-15 17:47:08] Learning rate decayed by 0.5000
[2017-12-15 17:47:17] Epoch 0046 mean train/dev loss: 66.1441 / 127.5430
[2017-12-15 17:47:25] Epoch 0047 mean train/dev loss: 66.2104 / 129.2621
[2017-12-15 17:47:33] Epoch 0048 mean train/dev loss: 66.1897 / 128.4388
[2017-12-15 17:47:41] Epoch 0049 mean train/dev loss: 66.0708 / 141.2011
[2017-12-15 17:47:49] Epoch 0050 mean train/dev loss: 66.0646 / 131.4104
[2017-12-15 17:47:49] Checkpointing model at epoch 50 for ffn.hl_50_50.lr_0.01.wd_0.001
[2017-12-15 17:47:49] Model Checkpointing finished.
[2017-12-15 17:47:57] Epoch 0051 mean train/dev loss: 66.0215 / 130.2699
[2017-12-15 17:48:05] Epoch 0052 mean train/dev loss: 66.0549 / 120.2018
[2017-12-15 17:48:13] Epoch 0053 mean train/dev loss: 65.9357 / 125.5820
[2017-12-15 17:48:21] Epoch 0054 mean train/dev loss: 65.8827 / 133.3284
[2017-12-15 17:48:29] Epoch 0055 mean train/dev loss: 65.8614 / 136.7737
[2017-12-15 17:48:38] Epoch 0056 mean train/dev loss: 65.8232 / 129.6681
[2017-12-15 17:48:45] Epoch 0057 mean train/dev loss: 65.7195 / 127.8288
[2017-12-15 17:48:54] Epoch 0058 mean train/dev loss: 65.6728 / 132.0697
[2017-12-15 17:49:02] Epoch 0059 mean train/dev loss: 65.6264 / 129.6513
[2017-12-15 17:49:10] Epoch 0060 mean train/dev loss: 65.6313 / 131.6169
[2017-12-15 17:49:10] Learning rate decayed by 0.5000
[2017-12-15 17:49:10] Checkpointing model at epoch 60 for ffn.hl_50_50.lr_0.01.wd_0.001
[2017-12-15 17:49:11] Model Checkpointing finished.
[2017-12-15 17:49:19] Epoch 0061 mean train/dev loss: 65.0641 / 130.3920
[2017-12-15 17:49:27] Epoch 0062 mean train/dev loss: 65.0320 / 126.1784
[2017-12-15 17:49:27] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:49:27] 
                       *** Training finished *** 
[2017-12-15 17:49:29] Dev MSE: 126.1784
[2017-12-15 17:49:36] Training MSE: 64.9573
[2017-12-15 17:49:39] Experiment ffn.hl_50_50.lr_0.01.wd_0.001 logging ended.
