[2017-12-15 14:04:00] Experiment ffn.hl_50.lr_0.1.wd_0.001 logging started.
[2017-12-15 14:04:00] 
                       *** Starting Experiment ffn.hl_50.lr_0.1.wd_0.001 ***
                      
[2017-12-15 14:04:00] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 14:04:03] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 1)
                      )
[2017-12-15 14:04:03]  *** Training on GPU ***
[2017-12-15 14:04:10] Epoch 0001 mean train/dev loss: 10990.7533 / 320.7559
[2017-12-15 14:04:17] Epoch 0002 mean train/dev loss: 135.6233 / 206.6652
[2017-12-15 14:04:24] Epoch 0003 mean train/dev loss: 117.6780 / 130.5669
[2017-12-15 14:04:32] Epoch 0004 mean train/dev loss: 115.2775 / 177.2138
[2017-12-15 14:04:39] Epoch 0005 mean train/dev loss: 112.5394 / 137.9562
[2017-12-15 14:04:46] Epoch 0006 mean train/dev loss: 111.9230 / 155.6404
[2017-12-15 14:04:53] Epoch 0007 mean train/dev loss: 111.8598 / 182.2990
[2017-12-15 14:05:00] Epoch 0008 mean train/dev loss: 113.3187 / 158.4433
[2017-12-15 14:05:07] Epoch 0009 mean train/dev loss: 110.9926 / 236.4602
[2017-12-15 14:05:15] Epoch 0010 mean train/dev loss: 113.7944 / 166.6580
[2017-12-15 14:05:15] Checkpointing model at epoch 10 for ffn.hl_50.lr_0.1.wd_0.001
[2017-12-15 14:05:15] Model Checkpointing finished.
[2017-12-15 14:05:23] Epoch 0011 mean train/dev loss: 109.2101 / 151.0549
[2017-12-15 14:05:30] Epoch 0012 mean train/dev loss: 110.7306 / 157.2231
[2017-12-15 14:05:37] Epoch 0013 mean train/dev loss: 108.6461 / 367.8243
[2017-12-15 14:05:44] Epoch 0014 mean train/dev loss: 109.2653 / 186.9414
[2017-12-15 14:05:51] Epoch 0015 mean train/dev loss: 106.8352 / 202.4179
[2017-12-15 14:05:51] Learning rate decayed by 0.5000
[2017-12-15 14:05:58] Epoch 0016 mean train/dev loss: 99.4172 / 184.2797
[2017-12-15 14:06:06] Epoch 0017 mean train/dev loss: 100.0248 / 127.3218
[2017-12-15 14:06:12] Epoch 0018 mean train/dev loss: 100.8388 / 152.0776
[2017-12-15 14:06:20] Epoch 0019 mean train/dev loss: 100.8803 / 180.3369
[2017-12-15 14:06:28] Epoch 0020 mean train/dev loss: 100.3745 / 172.2566
[2017-12-15 14:06:28] Checkpointing model at epoch 20 for ffn.hl_50.lr_0.1.wd_0.001
[2017-12-15 14:06:28] Model Checkpointing finished.
[2017-12-15 14:06:35] Epoch 0021 mean train/dev loss: 99.6433 / 172.1105
[2017-12-15 14:06:43] Epoch 0022 mean train/dev loss: 98.8049 / 204.3315
[2017-12-15 14:06:50] Epoch 0023 mean train/dev loss: 99.9946 / 189.1553
[2017-12-15 14:06:58] Epoch 0024 mean train/dev loss: 97.5300 / 156.5482
[2017-12-15 14:07:05] Epoch 0025 mean train/dev loss: 95.2782 / 156.8504
[2017-12-15 14:07:12] Epoch 0026 mean train/dev loss: 95.4159 / 185.1035
[2017-12-15 14:07:19] Epoch 0027 mean train/dev loss: 91.8975 / 218.1049
[2017-12-15 14:07:26] Epoch 0028 mean train/dev loss: 91.0176 / 192.1935
[2017-12-15 14:07:33] Epoch 0029 mean train/dev loss: 87.5662 / 184.8583
[2017-12-15 14:07:40] Epoch 0030 mean train/dev loss: 87.1381 / 169.6412
[2017-12-15 14:07:40] Learning rate decayed by 0.5000
[2017-12-15 14:07:40] Checkpointing model at epoch 30 for ffn.hl_50.lr_0.1.wd_0.001
[2017-12-15 14:07:41] Model Checkpointing finished.
[2017-12-15 14:07:48] Epoch 0031 mean train/dev loss: 81.2528 / 149.3494
[2017-12-15 14:07:55] Epoch 0032 mean train/dev loss: 80.5255 / 162.5431
[2017-12-15 14:08:03] Epoch 0033 mean train/dev loss: 80.0488 / 178.2438
[2017-12-15 14:08:10] Epoch 0034 mean train/dev loss: 79.6821 / 160.2575
[2017-12-15 14:08:17] Epoch 0035 mean train/dev loss: 79.5557 / 172.6309
[2017-12-15 14:08:25] Epoch 0036 mean train/dev loss: 79.1843 / 162.8251
[2017-12-15 14:08:32] Epoch 0037 mean train/dev loss: 78.9792 / 187.8829
[2017-12-15 14:08:39] Epoch 0038 mean train/dev loss: 78.9996 / 150.7137
[2017-12-15 14:08:46] Epoch 0039 mean train/dev loss: 78.2148 / 148.3242
[2017-12-15 14:08:53] Epoch 0040 mean train/dev loss: 78.3413 / 194.0728
[2017-12-15 14:08:53] Checkpointing model at epoch 40 for ffn.hl_50.lr_0.1.wd_0.001
[2017-12-15 14:08:54] Model Checkpointing finished.
[2017-12-15 14:09:01] Epoch 0041 mean train/dev loss: 78.6292 / 167.4314
[2017-12-15 14:09:09] Epoch 0042 mean train/dev loss: 77.8714 / 172.6189
[2017-12-15 14:09:16] Epoch 0043 mean train/dev loss: 77.7177 / 165.2486
[2017-12-15 14:09:23] Epoch 0044 mean train/dev loss: 77.6694 / 179.4773
[2017-12-15 14:09:31] Epoch 0045 mean train/dev loss: 77.4881 / 155.8690
[2017-12-15 14:09:31] Learning rate decayed by 0.5000
[2017-12-15 14:09:38] Epoch 0046 mean train/dev loss: 75.7183 / 153.1660
[2017-12-15 14:09:45] Epoch 0047 mean train/dev loss: 75.9786 / 156.2612
[2017-12-15 14:09:53] Epoch 0048 mean train/dev loss: 75.8465 / 154.4350
[2017-12-15 14:10:00] Epoch 0049 mean train/dev loss: 75.7761 / 156.2707
[2017-12-15 14:10:07] Epoch 0050 mean train/dev loss: 76.1759 / 169.2303
[2017-12-15 14:10:07] Checkpointing model at epoch 50 for ffn.hl_50.lr_0.1.wd_0.001
[2017-12-15 14:10:07] Model Checkpointing finished.
[2017-12-15 14:10:15] Epoch 0051 mean train/dev loss: 75.8420 / 166.3737
[2017-12-15 14:10:22] Epoch 0052 mean train/dev loss: 75.8672 / 171.5356
[2017-12-15 14:10:29] Epoch 0053 mean train/dev loss: 75.5558 / 151.7009
[2017-12-15 14:10:37] Epoch 0054 mean train/dev loss: 75.5278 / 145.3157
[2017-12-15 14:10:44] Epoch 0055 mean train/dev loss: 75.2369 / 164.8690
[2017-12-15 14:10:52] Epoch 0056 mean train/dev loss: 75.3002 / 154.3207
[2017-12-15 14:10:59] Epoch 0057 mean train/dev loss: 75.2225 / 144.6101
[2017-12-15 14:11:06] Epoch 0058 mean train/dev loss: 74.9232 / 143.6443
[2017-12-15 14:11:06] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 14:11:06] 
                       *** Training finished *** 
[2017-12-15 14:11:08] Dev MSE: 143.6443
[2017-12-15 14:11:14] Training MSE: 73.8227
[2017-12-15 14:11:16] Experiment ffn.hl_50.lr_0.1.wd_0.001 logging ended.
