[2017-12-15 17:14:00] Experiment ffn.hl_20_20_20.lr_0.01.wd_0.001 logging started.
[2017-12-15 17:14:00] 
                       *** Starting Experiment ffn.hl_20_20_20.lr_0.01.wd_0.001 ***
                      
[2017-12-15 17:14:00] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 17:14:00] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 20)
                        (relu3): ReLU ()
                        (linear4): Linear (20 -> 1)
                      )
[2017-12-15 17:14:00]  *** Training on GPU ***
[2017-12-15 17:14:09] Epoch 0001 mean train/dev loss: 20527.8821 / 409.6906
[2017-12-15 17:14:17] Epoch 0002 mean train/dev loss: 158.9975 / 193.0107
[2017-12-15 17:14:26] Epoch 0003 mean train/dev loss: 124.5761 / 181.4413
[2017-12-15 17:14:34] Epoch 0004 mean train/dev loss: 117.7471 / 141.4476
[2017-12-15 17:14:43] Epoch 0005 mean train/dev loss: 116.1847 / 134.6117
[2017-12-15 17:14:51] Epoch 0006 mean train/dev loss: 113.5367 / 122.4210
[2017-12-15 17:15:00] Epoch 0007 mean train/dev loss: 110.7845 / 127.2794
[2017-12-15 17:15:08] Epoch 0008 mean train/dev loss: 110.4661 / 139.7929
[2017-12-15 17:15:16] Epoch 0009 mean train/dev loss: 108.1837 / 146.4312
[2017-12-15 17:15:25] Epoch 0010 mean train/dev loss: 107.8387 / 136.7025
[2017-12-15 17:15:25] Checkpointing model at epoch 10 for ffn.hl_20_20_20.lr_0.01.wd_0.001
[2017-12-15 17:15:25] Model Checkpointing finished.
[2017-12-15 17:15:34] Epoch 0011 mean train/dev loss: 105.4335 / 138.7381
[2017-12-15 17:15:43] Epoch 0012 mean train/dev loss: 100.1171 / 132.4865
[2017-12-15 17:15:51] Epoch 0013 mean train/dev loss: 98.9889 / 122.0873
[2017-12-15 17:16:00] Epoch 0014 mean train/dev loss: 94.9008 / 218.3757
[2017-12-15 17:16:09] Epoch 0015 mean train/dev loss: 92.4698 / 209.2484
[2017-12-15 17:16:09] Learning rate decayed by 0.5000
[2017-12-15 17:16:17] Epoch 0016 mean train/dev loss: 85.9099 / 155.5256
[2017-12-15 17:16:27] Epoch 0017 mean train/dev loss: 85.5408 / 143.0491
[2017-12-15 17:16:36] Epoch 0018 mean train/dev loss: 85.0035 / 151.0474
[2017-12-15 17:16:45] Epoch 0019 mean train/dev loss: 84.7141 / 191.8416
[2017-12-15 17:16:54] Epoch 0020 mean train/dev loss: 84.8670 / 173.3389
[2017-12-15 17:16:54] Checkpointing model at epoch 20 for ffn.hl_20_20_20.lr_0.01.wd_0.001
[2017-12-15 17:16:54] Model Checkpointing finished.
[2017-12-15 17:17:04] Epoch 0021 mean train/dev loss: 83.6576 / 172.7950
[2017-12-15 17:17:13] Epoch 0022 mean train/dev loss: 84.1617 / 146.8736
[2017-12-15 17:17:22] Epoch 0023 mean train/dev loss: 84.0910 / 165.0324
[2017-12-15 17:17:30] Epoch 0024 mean train/dev loss: 84.0956 / 155.6118
[2017-12-15 17:17:40] Epoch 0025 mean train/dev loss: 83.5523 / 164.0676
[2017-12-15 17:17:49] Epoch 0026 mean train/dev loss: 83.6742 / 185.6942
[2017-12-15 17:17:58] Epoch 0027 mean train/dev loss: 82.9207 / 147.4362
[2017-12-15 17:18:07] Epoch 0028 mean train/dev loss: 82.8189 / 137.7702
[2017-12-15 17:18:16] Epoch 0029 mean train/dev loss: 82.7705 / 160.5627
[2017-12-15 17:18:25] Epoch 0030 mean train/dev loss: 82.5081 / 149.5021
[2017-12-15 17:18:25] Learning rate decayed by 0.5000
[2017-12-15 17:18:25] Checkpointing model at epoch 30 for ffn.hl_20_20_20.lr_0.01.wd_0.001
[2017-12-15 17:18:25] Model Checkpointing finished.
[2017-12-15 17:18:34] Epoch 0031 mean train/dev loss: 79.8607 / 165.4875
[2017-12-15 17:18:43] Epoch 0032 mean train/dev loss: 79.8503 / 167.0852
[2017-12-15 17:18:52] Epoch 0033 mean train/dev loss: 79.9700 / 150.3656
[2017-12-15 17:19:01] Epoch 0034 mean train/dev loss: 79.5813 / 141.4315
[2017-12-15 17:19:10] Epoch 0035 mean train/dev loss: 79.9584 / 138.7809
[2017-12-15 17:19:18] Epoch 0036 mean train/dev loss: 79.5587 / 163.4691
[2017-12-15 17:19:27] Epoch 0037 mean train/dev loss: 79.6639 / 178.0982
[2017-12-15 17:19:36] Epoch 0038 mean train/dev loss: 79.5449 / 171.1960
[2017-12-15 17:19:45] Epoch 0039 mean train/dev loss: 79.2555 / 174.0106
[2017-12-15 17:19:54] Epoch 0040 mean train/dev loss: 79.2038 / 161.2754
[2017-12-15 17:19:54] Checkpointing model at epoch 40 for ffn.hl_20_20_20.lr_0.01.wd_0.001
[2017-12-15 17:19:55] Model Checkpointing finished.
[2017-12-15 17:20:03] Epoch 0041 mean train/dev loss: 79.0930 / 172.4442
[2017-12-15 17:20:12] Epoch 0042 mean train/dev loss: 79.3630 / 165.0980
[2017-12-15 17:20:21] Epoch 0043 mean train/dev loss: 78.5652 / 155.3954
[2017-12-15 17:20:30] Epoch 0044 mean train/dev loss: 78.7050 / 181.8416
[2017-12-15 17:20:39] Epoch 0045 mean train/dev loss: 78.7940 / 192.2306
[2017-12-15 17:20:39] Learning rate decayed by 0.5000
[2017-12-15 17:20:48] Epoch 0046 mean train/dev loss: 77.6944 / 168.6848
[2017-12-15 17:20:57] Epoch 0047 mean train/dev loss: 77.5503 / 165.5110
[2017-12-15 17:21:06] Epoch 0048 mean train/dev loss: 77.6414 / 167.6116
[2017-12-15 17:21:15] Epoch 0049 mean train/dev loss: 77.4300 / 169.5468
[2017-12-15 17:21:24] Epoch 0050 mean train/dev loss: 77.3996 / 166.3945
[2017-12-15 17:21:24] Checkpointing model at epoch 50 for ffn.hl_20_20_20.lr_0.01.wd_0.001
[2017-12-15 17:21:24] Model Checkpointing finished.
[2017-12-15 17:21:34] Epoch 0051 mean train/dev loss: 77.4519 / 160.4840
[2017-12-15 17:21:43] Epoch 0052 mean train/dev loss: 77.3094 / 168.2274
[2017-12-15 17:21:52] Epoch 0053 mean train/dev loss: 77.2760 / 179.8616
[2017-12-15 17:22:01] Epoch 0054 mean train/dev loss: 77.0476 / 153.5092
[2017-12-15 17:22:01] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:22:01] 
                       *** Training finished *** 
[2017-12-15 17:22:02] Dev MSE: 153.5092
[2017-12-15 17:22:09] Training MSE: 78.2939
[2017-12-15 17:22:11] Experiment ffn.hl_20_20_20.lr_0.01.wd_0.001 logging ended.
