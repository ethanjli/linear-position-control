[2017-12-15 17:28:30] Experiment ffn.hl_50.lr_0.01.wd_0.01 logging started.
[2017-12-15 17:28:30] 
                       *** Starting Experiment ffn.hl_50.lr_0.01.wd_0.01 ***
                      
[2017-12-15 17:28:30] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [50]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.01  
[2017-12-15 17:28:30] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 50)
                        (relu1): ReLU ()
                        (linear2): Linear (50 -> 1)
                      )
[2017-12-15 17:28:30]  *** Training on GPU ***
[2017-12-15 17:28:38] Epoch 0001 mean train/dev loss: 70946.3674 / 2297.2988
[2017-12-15 17:28:45] Epoch 0002 mean train/dev loss: 621.3497 / 1253.2230
[2017-12-15 17:28:53] Epoch 0003 mean train/dev loss: 302.9249 / 580.9144
[2017-12-15 17:29:00] Epoch 0004 mean train/dev loss: 196.2168 / 274.5849
[2017-12-15 17:29:08] Epoch 0005 mean train/dev loss: 152.9771 / 212.7609
[2017-12-15 17:29:16] Epoch 0006 mean train/dev loss: 133.0080 / 201.3901
[2017-12-15 17:29:24] Epoch 0007 mean train/dev loss: 122.2296 / 167.0528
[2017-12-15 17:29:32] Epoch 0008 mean train/dev loss: 115.5109 / 176.5491
[2017-12-15 17:29:39] Epoch 0009 mean train/dev loss: 111.8231 / 135.0477
[2017-12-15 17:29:46] Epoch 0010 mean train/dev loss: 109.4010 / 138.3197
[2017-12-15 17:29:46] Checkpointing model at epoch 10 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:29:46] Model Checkpointing finished.
[2017-12-15 17:29:53] Epoch 0011 mean train/dev loss: 107.6758 / 134.6591
[2017-12-15 17:30:01] Epoch 0012 mean train/dev loss: 106.1589 / 131.5540
[2017-12-15 17:30:09] Epoch 0013 mean train/dev loss: 105.0471 / 140.4089
[2017-12-15 17:30:16] Epoch 0014 mean train/dev loss: 103.6971 / 155.0285
[2017-12-15 17:30:23] Epoch 0015 mean train/dev loss: 102.5572 / 122.4673
[2017-12-15 17:30:23] Learning rate decayed by 0.5000
[2017-12-15 17:30:30] Epoch 0016 mean train/dev loss: 100.7902 / 152.6794
[2017-12-15 17:30:38] Epoch 0017 mean train/dev loss: 100.3031 / 159.5158
[2017-12-15 17:30:45] Epoch 0018 mean train/dev loss: 99.7714 / 158.5966
[2017-12-15 17:30:52] Epoch 0019 mean train/dev loss: 99.0774 / 152.9086
[2017-12-15 17:30:59] Epoch 0020 mean train/dev loss: 98.3832 / 142.4804
[2017-12-15 17:30:59] Checkpointing model at epoch 20 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:30:59] Model Checkpointing finished.
[2017-12-15 17:31:07] Epoch 0021 mean train/dev loss: 97.7954 / 159.1043
[2017-12-15 17:31:14] Epoch 0022 mean train/dev loss: 97.4051 / 143.0253
[2017-12-15 17:31:21] Epoch 0023 mean train/dev loss: 96.7577 / 128.0379
[2017-12-15 17:31:29] Epoch 0024 mean train/dev loss: 96.2685 / 141.6945
[2017-12-15 17:31:36] Epoch 0025 mean train/dev loss: 95.7373 / 143.7486
[2017-12-15 17:31:44] Epoch 0026 mean train/dev loss: 95.3312 / 149.5966
[2017-12-15 17:31:50] Epoch 0027 mean train/dev loss: 94.7867 / 145.6781
[2017-12-15 17:31:57] Epoch 0028 mean train/dev loss: 94.2547 / 155.2964
[2017-12-15 17:32:05] Epoch 0029 mean train/dev loss: 93.7872 / 139.7620
[2017-12-15 17:32:12] Epoch 0030 mean train/dev loss: 93.3424 / 126.9563
[2017-12-15 17:32:12] Learning rate decayed by 0.5000
[2017-12-15 17:32:12] Checkpointing model at epoch 30 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:32:12] Model Checkpointing finished.
[2017-12-15 17:32:20] Epoch 0031 mean train/dev loss: 92.6338 / 132.7195
[2017-12-15 17:32:27] Epoch 0032 mean train/dev loss: 92.4402 / 140.7377
[2017-12-15 17:32:35] Epoch 0033 mean train/dev loss: 92.2039 / 136.7622
[2017-12-15 17:32:42] Epoch 0034 mean train/dev loss: 92.0366 / 140.1411
[2017-12-15 17:32:49] Epoch 0035 mean train/dev loss: 91.8253 / 137.5842
[2017-12-15 17:32:57] Epoch 0036 mean train/dev loss: 91.6234 / 137.0948
[2017-12-15 17:33:04] Epoch 0037 mean train/dev loss: 91.3686 / 137.9588
[2017-12-15 17:33:11] Epoch 0038 mean train/dev loss: 91.2440 / 127.2720
[2017-12-15 17:33:19] Epoch 0039 mean train/dev loss: 90.9802 / 133.5295
[2017-12-15 17:33:26] Epoch 0040 mean train/dev loss: 90.8846 / 138.0958
[2017-12-15 17:33:26] Checkpointing model at epoch 40 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:33:26] Model Checkpointing finished.
[2017-12-15 17:33:34] Epoch 0041 mean train/dev loss: 90.6242 / 134.5972
[2017-12-15 17:33:41] Epoch 0042 mean train/dev loss: 90.4961 / 128.6568
[2017-12-15 17:33:48] Epoch 0043 mean train/dev loss: 90.3702 / 130.0374
[2017-12-15 17:33:56] Epoch 0044 mean train/dev loss: 90.1656 / 132.3555
[2017-12-15 17:34:03] Epoch 0045 mean train/dev loss: 89.9536 / 127.2143
[2017-12-15 17:34:03] Learning rate decayed by 0.5000
[2017-12-15 17:34:10] Epoch 0046 mean train/dev loss: 89.5778 / 128.1669
[2017-12-15 17:34:18] Epoch 0047 mean train/dev loss: 89.5151 / 131.8795
[2017-12-15 17:34:25] Epoch 0048 mean train/dev loss: 89.2030 / 132.6882
[2017-12-15 17:34:32] Epoch 0049 mean train/dev loss: 89.1564 / 121.4281
[2017-12-15 17:34:40] Epoch 0050 mean train/dev loss: 89.0715 / 128.8440
[2017-12-15 17:34:40] Checkpointing model at epoch 50 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:34:40] Model Checkpointing finished.
[2017-12-15 17:34:47] Epoch 0051 mean train/dev loss: 88.8922 / 123.8385
[2017-12-15 17:34:55] Epoch 0052 mean train/dev loss: 88.8566 / 133.4285
[2017-12-15 17:35:02] Epoch 0053 mean train/dev loss: 88.7522 / 133.0064
[2017-12-15 17:35:10] Epoch 0054 mean train/dev loss: 88.6777 / 130.2415
[2017-12-15 17:35:17] Epoch 0055 mean train/dev loss: 88.6126 / 130.2288
[2017-12-15 17:35:24] Epoch 0056 mean train/dev loss: 88.5058 / 128.1702
[2017-12-15 17:35:31] Epoch 0057 mean train/dev loss: 88.4483 / 128.1845
[2017-12-15 17:35:39] Epoch 0058 mean train/dev loss: 88.3778 / 133.0332
[2017-12-15 17:35:46] Epoch 0059 mean train/dev loss: 88.2795 / 118.7922
[2017-12-15 17:35:53] Epoch 0060 mean train/dev loss: 88.2152 / 128.1244
[2017-12-15 17:35:53] Learning rate decayed by 0.5000
[2017-12-15 17:35:53] Checkpointing model at epoch 60 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:35:53] Model Checkpointing finished.
[2017-12-15 17:36:01] Epoch 0061 mean train/dev loss: 87.9533 / 132.0126
[2017-12-15 17:36:08] Epoch 0062 mean train/dev loss: 87.9193 / 124.0466
[2017-12-15 17:36:15] Epoch 0063 mean train/dev loss: 87.9050 / 129.4981
[2017-12-15 17:36:23] Epoch 0064 mean train/dev loss: 87.8556 / 129.8087
[2017-12-15 17:36:30] Epoch 0065 mean train/dev loss: 87.7968 / 131.5911
[2017-12-15 17:36:37] Epoch 0066 mean train/dev loss: 87.7732 / 133.6783
[2017-12-15 17:36:44] Epoch 0067 mean train/dev loss: 87.7674 / 124.0198
[2017-12-15 17:36:52] Epoch 0068 mean train/dev loss: 87.7102 / 131.3984
[2017-12-15 17:36:59] Epoch 0069 mean train/dev loss: 87.6695 / 130.5873
[2017-12-15 17:37:06] Epoch 0070 mean train/dev loss: 87.6141 / 128.3264
[2017-12-15 17:37:06] Checkpointing model at epoch 70 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:37:07] Model Checkpointing finished.
[2017-12-15 17:37:14] Epoch 0071 mean train/dev loss: 87.5943 / 127.6277
[2017-12-15 17:37:21] Epoch 0072 mean train/dev loss: 87.5469 / 128.8993
[2017-12-15 17:37:29] Epoch 0073 mean train/dev loss: 87.5054 / 122.5247
[2017-12-15 17:37:36] Epoch 0074 mean train/dev loss: 87.4482 / 129.8001
[2017-12-15 17:37:43] Epoch 0075 mean train/dev loss: 87.4124 / 127.6770
[2017-12-15 17:37:43] Learning rate decayed by 0.5000
[2017-12-15 17:37:51] Epoch 0076 mean train/dev loss: 87.2986 / 129.8318
[2017-12-15 17:37:58] Epoch 0077 mean train/dev loss: 87.2890 / 129.7984
[2017-12-15 17:38:05] Epoch 0078 mean train/dev loss: 87.2782 / 128.0942
[2017-12-15 17:38:13] Epoch 0079 mean train/dev loss: 87.2373 / 128.1003
[2017-12-15 17:38:20] Epoch 0080 mean train/dev loss: 87.2324 / 128.0229
[2017-12-15 17:38:20] Checkpointing model at epoch 80 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:38:21] Model Checkpointing finished.
[2017-12-15 17:38:28] Epoch 0081 mean train/dev loss: 87.2160 / 130.5305
[2017-12-15 17:38:35] Epoch 0082 mean train/dev loss: 87.1823 / 129.2076
[2017-12-15 17:38:43] Epoch 0083 mean train/dev loss: 87.1942 / 124.2588
[2017-12-15 17:38:49] Epoch 0084 mean train/dev loss: 87.1998 / 127.1967
[2017-12-15 17:38:57] Epoch 0085 mean train/dev loss: 87.1356 / 127.1969
[2017-12-15 17:39:03] Epoch 0086 mean train/dev loss: 87.1098 / 125.4464
[2017-12-15 17:39:10] Epoch 0087 mean train/dev loss: 87.0964 / 125.2125
[2017-12-15 17:39:15] Epoch 0088 mean train/dev loss: 87.0803 / 131.3975
[2017-12-15 17:39:20] Epoch 0089 mean train/dev loss: 87.0647 / 127.2106
[2017-12-15 17:39:25] Epoch 0090 mean train/dev loss: 87.0445 / 127.4034
[2017-12-15 17:39:25] Learning rate decayed by 0.5000
[2017-12-15 17:39:25] Checkpointing model at epoch 90 for ffn.hl_50.lr_0.01.wd_0.01
[2017-12-15 17:39:25] Model Checkpointing finished.
[2017-12-15 17:39:30] Epoch 0091 mean train/dev loss: 86.9750 / 128.0125
[2017-12-15 17:39:35] Epoch 0092 mean train/dev loss: 86.9808 / 126.8768
[2017-12-15 17:39:40] Epoch 0093 mean train/dev loss: 86.9647 / 125.6981
[2017-12-15 17:39:45] Epoch 0094 mean train/dev loss: 86.9508 / 127.1067
[2017-12-15 17:39:50] Epoch 0095 mean train/dev loss: 86.9623 / 127.7972
[2017-12-15 17:39:55] Epoch 0096 mean train/dev loss: 86.9205 / 126.6324
[2017-12-15 17:39:59] Epoch 0097 mean train/dev loss: 86.9302 / 128.0730
[2017-12-15 17:40:04] Epoch 0098 mean train/dev loss: 86.9400 / 129.4828
[2017-12-15 17:40:09] Epoch 0099 mean train/dev loss: 86.9129 / 126.5532
[2017-12-15 17:40:14] Epoch 0100 mean train/dev loss: 86.9172 / 127.5106
[2017-12-15 17:40:14] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:40:14] 
                       *** Training finished *** 
[2017-12-15 17:40:15] Dev MSE: 127.5106
[2017-12-15 17:40:20] Training MSE: 86.8878
[2017-12-15 17:40:21] Experiment ffn.hl_50.lr_0.01.wd_0.01 logging ended.
