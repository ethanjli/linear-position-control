[2017-12-15 16:13:33] Experiment ffn.hl_100_100.lr_0.1.wd_0.1 logging started.
[2017-12-15 16:13:33] 
                       *** Starting Experiment ffn.hl_100_100.lr_0.1.wd_0.1 ***
                      
[2017-12-15 16:13:33] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100, 100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 16:13:33] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 100)
                        (relu2): ReLU ()
                        (linear3): Linear (100 -> 1)
                      )
[2017-12-15 16:13:33]  *** Training on GPU ***
[2017-12-15 16:13:42] Epoch 0001 mean train/dev loss: 2959.2217 / 185.1217
[2017-12-15 16:13:50] Epoch 0002 mean train/dev loss: 239.7038 / 295.8504
[2017-12-15 16:13:59] Epoch 0003 mean train/dev loss: 186.7901 / 224.0801
[2017-12-15 16:14:07] Epoch 0004 mean train/dev loss: 399.6341 / 164.8849
[2017-12-15 16:14:14] Epoch 0005 mean train/dev loss: 115.5334 / 151.5544
[2017-12-15 16:14:22] Epoch 0006 mean train/dev loss: 129.5215 / 193.7956
[2017-12-15 16:14:31] Epoch 0007 mean train/dev loss: 140.0441 / 146.7518
[2017-12-15 16:14:38] Epoch 0008 mean train/dev loss: 230.3118 / 127.6354
[2017-12-15 16:14:46] Epoch 0009 mean train/dev loss: 110.0530 / 132.2348
[2017-12-15 16:14:54] Epoch 0010 mean train/dev loss: 130.5749 / 139.9439
[2017-12-15 16:14:54] Checkpointing model at epoch 10 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:14:55] Model Checkpointing finished.
[2017-12-15 16:15:03] Epoch 0011 mean train/dev loss: 116.3481 / 124.3650
[2017-12-15 16:15:11] Epoch 0012 mean train/dev loss: 116.1236 / 144.1709
[2017-12-15 16:15:19] Epoch 0013 mean train/dev loss: 117.8585 / 126.9674
[2017-12-15 16:15:27] Epoch 0014 mean train/dev loss: 113.9533 / 159.4361
[2017-12-15 16:15:35] Epoch 0015 mean train/dev loss: 115.2261 / 130.7091
[2017-12-15 16:15:35] Learning rate decayed by 0.5000
[2017-12-15 16:15:43] Epoch 0016 mean train/dev loss: 87.4354 / 113.3101
[2017-12-15 16:15:51] Epoch 0017 mean train/dev loss: 86.6779 / 129.7755
[2017-12-15 16:15:59] Epoch 0018 mean train/dev loss: 89.0177 / 103.1355
[2017-12-15 16:16:07] Epoch 0019 mean train/dev loss: 89.8156 / 119.8635
[2017-12-15 16:16:15] Epoch 0020 mean train/dev loss: 90.0569 / 111.6598
[2017-12-15 16:16:15] Checkpointing model at epoch 20 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:16:16] Model Checkpointing finished.
[2017-12-15 16:16:24] Epoch 0021 mean train/dev loss: 95.5899 / 111.8687
[2017-12-15 16:16:31] Epoch 0022 mean train/dev loss: 88.7455 / 124.7742
[2017-12-15 16:16:39] Epoch 0023 mean train/dev loss: 91.0974 / 115.1542
[2017-12-15 16:16:47] Epoch 0024 mean train/dev loss: 92.8005 / 137.9258
[2017-12-15 16:16:55] Epoch 0025 mean train/dev loss: 90.3072 / 127.6691
[2017-12-15 16:17:02] Epoch 0026 mean train/dev loss: 88.3392 / 125.7942
[2017-12-15 16:17:10] Epoch 0027 mean train/dev loss: 92.6155 / 126.3153
[2017-12-15 16:17:18] Epoch 0028 mean train/dev loss: 86.8331 / 154.4600
[2017-12-15 16:17:26] Epoch 0029 mean train/dev loss: 87.6503 / 119.7102
[2017-12-15 16:17:34] Epoch 0030 mean train/dev loss: 88.8041 / 151.0049
[2017-12-15 16:17:34] Learning rate decayed by 0.5000
[2017-12-15 16:17:34] Checkpointing model at epoch 30 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:17:34] Model Checkpointing finished.
[2017-12-15 16:17:42] Epoch 0031 mean train/dev loss: 79.9958 / 131.1341
[2017-12-15 16:17:50] Epoch 0032 mean train/dev loss: 79.7754 / 154.9490
[2017-12-15 16:17:58] Epoch 0033 mean train/dev loss: 79.5851 / 100.3936
[2017-12-15 16:18:06] Epoch 0034 mean train/dev loss: 79.3554 / 151.4917
[2017-12-15 16:18:14] Epoch 0035 mean train/dev loss: 78.8925 / 125.6625
[2017-12-15 16:18:22] Epoch 0036 mean train/dev loss: 78.9589 / 177.4444
[2017-12-15 16:18:30] Epoch 0037 mean train/dev loss: 77.9682 / 149.3345
[2017-12-15 16:18:39] Epoch 0038 mean train/dev loss: 77.7002 / 144.3797
[2017-12-15 16:18:46] Epoch 0039 mean train/dev loss: 77.1656 / 114.4439
[2017-12-15 16:18:54] Epoch 0040 mean train/dev loss: 76.8590 / 120.7553
[2017-12-15 16:18:54] Checkpointing model at epoch 40 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:18:55] Model Checkpointing finished.
[2017-12-15 16:19:02] Epoch 0041 mean train/dev loss: 75.9241 / 121.7627
[2017-12-15 16:19:10] Epoch 0042 mean train/dev loss: 76.2923 / 127.3499
[2017-12-15 16:19:18] Epoch 0043 mean train/dev loss: 76.3690 / 97.3097
[2017-12-15 16:19:27] Epoch 0044 mean train/dev loss: 75.0709 / 147.9512
[2017-12-15 16:19:35] Epoch 0045 mean train/dev loss: 74.9704 / 120.5919
[2017-12-15 16:19:35] Learning rate decayed by 0.5000
[2017-12-15 16:19:43] Epoch 0046 mean train/dev loss: 70.7034 / 105.6269
[2017-12-15 16:19:51] Epoch 0047 mean train/dev loss: 70.2789 / 100.6489
[2017-12-15 16:19:59] Epoch 0048 mean train/dev loss: 70.7379 / 93.7427
[2017-12-15 16:20:07] Epoch 0049 mean train/dev loss: 69.6534 / 93.0712
[2017-12-15 16:20:16] Epoch 0050 mean train/dev loss: 70.1396 / 106.0754
[2017-12-15 16:20:16] Checkpointing model at epoch 50 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:20:16] Model Checkpointing finished.
[2017-12-15 16:20:24] Epoch 0051 mean train/dev loss: 69.9152 / 119.9280
[2017-12-15 16:20:32] Epoch 0052 mean train/dev loss: 69.6781 / 90.3711
[2017-12-15 16:20:41] Epoch 0053 mean train/dev loss: 69.6200 / 95.6292
[2017-12-15 16:20:48] Epoch 0054 mean train/dev loss: 69.5591 / 99.9878
[2017-12-15 16:20:56] Epoch 0055 mean train/dev loss: 69.0513 / 93.4138
[2017-12-15 16:21:05] Epoch 0056 mean train/dev loss: 68.7175 / 107.7640
[2017-12-15 16:21:13] Epoch 0057 mean train/dev loss: 69.0429 / 105.3835
[2017-12-15 16:21:20] Epoch 0058 mean train/dev loss: 69.1518 / 87.2362
[2017-12-15 16:21:28] Epoch 0059 mean train/dev loss: 68.6075 / 103.8912
[2017-12-15 16:21:37] Epoch 0060 mean train/dev loss: 68.4670 / 90.9954
[2017-12-15 16:21:37] Learning rate decayed by 0.5000
[2017-12-15 16:21:37] Checkpointing model at epoch 60 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:21:37] Model Checkpointing finished.
[2017-12-15 16:21:45] Epoch 0061 mean train/dev loss: 66.1144 / 96.8586
[2017-12-15 16:21:53] Epoch 0062 mean train/dev loss: 66.2375 / 87.5959
[2017-12-15 16:22:01] Epoch 0063 mean train/dev loss: 66.0985 / 88.2524
[2017-12-15 16:22:09] Epoch 0064 mean train/dev loss: 65.8733 / 101.7359
[2017-12-15 16:22:17] Epoch 0065 mean train/dev loss: 65.8603 / 97.3047
[2017-12-15 16:22:24] Epoch 0066 mean train/dev loss: 65.9036 / 98.7703
[2017-12-15 16:22:32] Epoch 0067 mean train/dev loss: 65.8146 / 93.5815
[2017-12-15 16:22:40] Epoch 0068 mean train/dev loss: 65.8053 / 92.3474
[2017-12-15 16:22:48] Epoch 0069 mean train/dev loss: 66.1082 / 90.1153
[2017-12-15 16:22:56] Epoch 0070 mean train/dev loss: 65.6252 / 96.8313
[2017-12-15 16:22:56] Checkpointing model at epoch 70 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:22:57] Model Checkpointing finished.
[2017-12-15 16:23:04] Epoch 0071 mean train/dev loss: 65.4444 / 89.5651
[2017-12-15 16:23:12] Epoch 0072 mean train/dev loss: 65.4399 / 101.8111
[2017-12-15 16:23:20] Epoch 0073 mean train/dev loss: 65.3969 / 101.9324
[2017-12-15 16:23:28] Epoch 0074 mean train/dev loss: 65.2339 / 95.8619
[2017-12-15 16:23:36] Epoch 0075 mean train/dev loss: 65.4012 / 89.9877
[2017-12-15 16:23:36] Learning rate decayed by 0.5000
[2017-12-15 16:23:44] Epoch 0076 mean train/dev loss: 64.0395 / 86.6926
[2017-12-15 16:23:52] Epoch 0077 mean train/dev loss: 63.9900 / 80.3368
[2017-12-15 16:24:00] Epoch 0078 mean train/dev loss: 63.8879 / 86.6577
[2017-12-15 16:24:08] Epoch 0079 mean train/dev loss: 63.9262 / 93.0560
[2017-12-15 16:24:16] Epoch 0080 mean train/dev loss: 63.8947 / 86.2873
[2017-12-15 16:24:16] Checkpointing model at epoch 80 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:24:16] Model Checkpointing finished.
[2017-12-15 16:24:24] Epoch 0081 mean train/dev loss: 63.8781 / 87.1138
[2017-12-15 16:24:33] Epoch 0082 mean train/dev loss: 63.9112 / 84.8887
[2017-12-15 16:24:40] Epoch 0083 mean train/dev loss: 63.7432 / 85.7951
[2017-12-15 16:24:49] Epoch 0084 mean train/dev loss: 63.9618 / 86.0194
[2017-12-15 16:24:57] Epoch 0085 mean train/dev loss: 63.7412 / 98.6905
[2017-12-15 16:25:05] Epoch 0086 mean train/dev loss: 63.7598 / 83.3022
[2017-12-15 16:25:13] Epoch 0087 mean train/dev loss: 63.6499 / 88.4103
[2017-12-15 16:25:21] Epoch 0088 mean train/dev loss: 63.5634 / 81.0274
[2017-12-15 16:25:29] Epoch 0089 mean train/dev loss: 63.7044 / 81.3130
[2017-12-15 16:25:37] Epoch 0090 mean train/dev loss: 63.5568 / 86.2980
[2017-12-15 16:25:37] Learning rate decayed by 0.5000
[2017-12-15 16:25:37] Checkpointing model at epoch 90 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:25:37] Model Checkpointing finished.
[2017-12-15 16:25:45] Epoch 0091 mean train/dev loss: 63.0042 / 84.2297
[2017-12-15 16:25:53] Epoch 0092 mean train/dev loss: 63.0001 / 84.1572
[2017-12-15 16:26:01] Epoch 0093 mean train/dev loss: 62.9502 / 85.4887
[2017-12-15 16:26:09] Epoch 0094 mean train/dev loss: 62.9358 / 91.8793
[2017-12-15 16:26:17] Epoch 0095 mean train/dev loss: 62.9006 / 87.1104
[2017-12-15 16:26:25] Epoch 0096 mean train/dev loss: 62.8372 / 83.6484
[2017-12-15 16:26:33] Epoch 0097 mean train/dev loss: 62.7380 / 85.5016
[2017-12-15 16:26:40] Epoch 0098 mean train/dev loss: 62.5790 / 82.8969
[2017-12-15 16:26:47] Epoch 0099 mean train/dev loss: 62.5239 / 82.9262
[2017-12-15 16:26:54] Epoch 0100 mean train/dev loss: 62.3465 / 82.7194
[2017-12-15 16:26:54] Checkpointing model at epoch 100 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:26:54] Model Checkpointing finished.
[2017-12-15 16:27:01] Epoch 0101 mean train/dev loss: 62.2599 / 92.6060
[2017-12-15 16:27:08] Epoch 0102 mean train/dev loss: 62.3157 / 87.8538
[2017-12-15 16:27:15] Epoch 0103 mean train/dev loss: 62.1082 / 81.8878
[2017-12-15 16:27:22] Epoch 0104 mean train/dev loss: 62.0488 / 91.5189
[2017-12-15 16:27:27] Epoch 0105 mean train/dev loss: 61.9967 / 82.8235
[2017-12-15 16:27:27] Learning rate decayed by 0.5000
[2017-12-15 16:27:32] Epoch 0106 mean train/dev loss: 61.5846 / 83.6991
[2017-12-15 16:27:38] Epoch 0107 mean train/dev loss: 61.5363 / 83.0596
[2017-12-15 16:27:43] Epoch 0108 mean train/dev loss: 61.5118 / 88.1677
[2017-12-15 16:27:48] Epoch 0109 mean train/dev loss: 61.4484 / 83.6530
[2017-12-15 16:27:54] Epoch 0110 mean train/dev loss: 61.4022 / 85.1695
[2017-12-15 16:27:54] Checkpointing model at epoch 110 for ffn.hl_100_100.lr_0.1.wd_0.1
[2017-12-15 16:27:54] Model Checkpointing finished.
[2017-12-15 16:27:59] Epoch 0111 mean train/dev loss: 61.3640 / 86.2130
[2017-12-15 16:28:05] Epoch 0112 mean train/dev loss: 61.3555 / 89.0379
[2017-12-15 16:28:10] Epoch 0113 mean train/dev loss: 61.3010 / 85.1415
[2017-12-15 16:28:15] Epoch 0114 mean train/dev loss: 61.2948 / 86.1750
[2017-12-15 16:28:21] Epoch 0115 mean train/dev loss: 61.2633 / 82.4777
[2017-12-15 16:28:26] Epoch 0116 mean train/dev loss: 61.2500 / 89.9880
[2017-12-15 16:28:31] Epoch 0117 mean train/dev loss: 61.1979 / 86.5209
[2017-12-15 16:28:37] Epoch 0118 mean train/dev loss: 61.1165 / 91.0057
[2017-12-15 16:28:37] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 16:28:37] 
                       *** Training finished *** 
[2017-12-15 16:28:38] Dev MSE: 91.0057
[2017-12-15 16:28:44] Training MSE: 61.1546
[2017-12-15 16:28:45] Experiment ffn.hl_100_100.lr_0.1.wd_0.1 logging ended.
