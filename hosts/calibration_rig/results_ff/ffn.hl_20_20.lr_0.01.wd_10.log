[2017-12-15 17:03:45] Experiment ffn.hl_20_20.lr_0.01.wd_10 logging started.
[2017-12-15 17:03:45] 
                       *** Starting Experiment ffn.hl_20_20.lr_0.01.wd_10 ***
                      
[2017-12-15 17:03:45] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 10  
[2017-12-15 17:03:45] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 1)
                      )
[2017-12-15 17:03:45]  *** Training on GPU ***
[2017-12-15 17:03:53] Epoch 0001 mean train/dev loss: 34583.7265 / 827.4030
[2017-12-15 17:04:01] Epoch 0002 mean train/dev loss: 281.1165 / 228.0988
[2017-12-15 17:04:09] Epoch 0003 mean train/dev loss: 160.5008 / 171.4527
[2017-12-15 17:04:17] Epoch 0004 mean train/dev loss: 136.2623 / 138.4852
[2017-12-15 17:04:25] Epoch 0005 mean train/dev loss: 126.2712 / 126.6719
[2017-12-15 17:04:33] Epoch 0006 mean train/dev loss: 121.8591 / 119.9134
[2017-12-15 17:04:41] Epoch 0007 mean train/dev loss: 119.5701 / 127.3913
[2017-12-15 17:04:48] Epoch 0008 mean train/dev loss: 118.7683 / 115.8080
[2017-12-15 17:04:56] Epoch 0009 mean train/dev loss: 118.3415 / 112.7052
[2017-12-15 17:05:04] Epoch 0010 mean train/dev loss: 118.7093 / 122.0880
[2017-12-15 17:05:04] Checkpointing model at epoch 10 for ffn.hl_20_20.lr_0.01.wd_10
[2017-12-15 17:05:04] Model Checkpointing finished.
[2017-12-15 17:05:12] Epoch 0011 mean train/dev loss: 118.4144 / 112.2020
[2017-12-15 17:05:20] Epoch 0012 mean train/dev loss: 117.7597 / 111.6866
[2017-12-15 17:05:28] Epoch 0013 mean train/dev loss: 117.9876 / 111.8820
[2017-12-15 17:05:36] Epoch 0014 mean train/dev loss: 118.1695 / 115.9206
[2017-12-15 17:05:43] Epoch 0015 mean train/dev loss: 118.0587 / 117.9807
[2017-12-15 17:05:43] Learning rate decayed by 0.5000
[2017-12-15 17:05:51] Epoch 0016 mean train/dev loss: 116.4030 / 111.7527
[2017-12-15 17:06:00] Epoch 0017 mean train/dev loss: 116.2436 / 119.3748
[2017-12-15 17:06:07] Epoch 0018 mean train/dev loss: 116.4037 / 112.0405
[2017-12-15 17:06:16] Epoch 0019 mean train/dev loss: 116.2861 / 113.1701
[2017-12-15 17:06:24] Epoch 0020 mean train/dev loss: 116.4090 / 108.3468
[2017-12-15 17:06:24] Checkpointing model at epoch 20 for ffn.hl_20_20.lr_0.01.wd_10
[2017-12-15 17:06:24] Model Checkpointing finished.
[2017-12-15 17:06:33] Epoch 0021 mean train/dev loss: 116.3006 / 112.0642
[2017-12-15 17:06:41] Epoch 0022 mean train/dev loss: 116.4644 / 111.5066
[2017-12-15 17:06:49] Epoch 0023 mean train/dev loss: 116.2342 / 113.6307
[2017-12-15 17:06:56] Epoch 0024 mean train/dev loss: 116.4735 / 112.7774
[2017-12-15 17:07:05] Epoch 0025 mean train/dev loss: 116.3608 / 112.0923
[2017-12-15 17:07:13] Epoch 0026 mean train/dev loss: 116.3780 / 116.9868
[2017-12-15 17:07:21] Epoch 0027 mean train/dev loss: 116.3547 / 114.7976
[2017-12-15 17:07:29] Epoch 0028 mean train/dev loss: 116.2634 / 111.3338
[2017-12-15 17:07:37] Epoch 0029 mean train/dev loss: 116.0755 / 112.4635
[2017-12-15 17:07:45] Epoch 0030 mean train/dev loss: 116.2706 / 113.4185
[2017-12-15 17:07:45] Learning rate decayed by 0.5000
[2017-12-15 17:07:45] Checkpointing model at epoch 30 for ffn.hl_20_20.lr_0.01.wd_10
[2017-12-15 17:07:45] Model Checkpointing finished.
[2017-12-15 17:07:53] Epoch 0031 mean train/dev loss: 115.3733 / 115.9976
[2017-12-15 17:08:01] Epoch 0032 mean train/dev loss: 115.4004 / 110.6010
[2017-12-15 17:08:09] Epoch 0033 mean train/dev loss: 115.3727 / 111.8975
[2017-12-15 17:08:17] Epoch 0034 mean train/dev loss: 115.5152 / 109.2504
[2017-12-15 17:08:25] Epoch 0035 mean train/dev loss: 115.3785 / 110.4110
[2017-12-15 17:08:33] Epoch 0036 mean train/dev loss: 115.3638 / 110.4365
[2017-12-15 17:08:41] Epoch 0037 mean train/dev loss: 115.4502 / 110.4144
[2017-12-15 17:08:49] Epoch 0038 mean train/dev loss: 115.4823 / 111.3169
[2017-12-15 17:08:58] Epoch 0039 mean train/dev loss: 115.4352 / 110.8671
[2017-12-15 17:09:05] Epoch 0040 mean train/dev loss: 115.3817 / 111.6107
[2017-12-15 17:09:05] Checkpointing model at epoch 40 for ffn.hl_20_20.lr_0.01.wd_10
[2017-12-15 17:09:06] Model Checkpointing finished.
[2017-12-15 17:09:13] Epoch 0041 mean train/dev loss: 115.3473 / 112.2164
[2017-12-15 17:09:21] Epoch 0042 mean train/dev loss: 115.5479 / 109.9138
[2017-12-15 17:09:30] Epoch 0043 mean train/dev loss: 115.4590 / 111.4297
[2017-12-15 17:09:38] Epoch 0044 mean train/dev loss: 115.3530 / 110.8426
[2017-12-15 17:09:46] Epoch 0045 mean train/dev loss: 115.3620 / 113.0729
[2017-12-15 17:09:46] Learning rate decayed by 0.5000
[2017-12-15 17:09:54] Epoch 0046 mean train/dev loss: 114.9818 / 110.2688
[2017-12-15 17:10:02] Epoch 0047 mean train/dev loss: 114.9338 / 110.2267
[2017-12-15 17:10:10] Epoch 0048 mean train/dev loss: 114.8772 / 109.8510
[2017-12-15 17:10:18] Epoch 0049 mean train/dev loss: 114.9843 / 109.8938
[2017-12-15 17:10:26] Epoch 0050 mean train/dev loss: 114.9273 / 110.0220
[2017-12-15 17:10:26] Checkpointing model at epoch 50 for ffn.hl_20_20.lr_0.01.wd_10
[2017-12-15 17:10:27] Model Checkpointing finished.
[2017-12-15 17:10:35] Epoch 0051 mean train/dev loss: 115.0216 / 112.3127
[2017-12-15 17:10:43] Epoch 0052 mean train/dev loss: 114.9015 / 109.9370
[2017-12-15 17:10:51] Epoch 0053 mean train/dev loss: 114.9464 / 111.0249
[2017-12-15 17:10:59] Epoch 0054 mean train/dev loss: 114.9516 / 110.1563
[2017-12-15 17:11:06] Epoch 0055 mean train/dev loss: 114.9614 / 109.6110
[2017-12-15 17:11:15] Epoch 0056 mean train/dev loss: 114.9449 / 111.3966
[2017-12-15 17:11:22] Epoch 0057 mean train/dev loss: 114.8776 / 110.6968
[2017-12-15 17:11:27] Epoch 0058 mean train/dev loss: 114.9446 / 109.5836
[2017-12-15 17:11:32] Epoch 0059 mean train/dev loss: 114.9773 / 110.3502
[2017-12-15 17:11:38] Epoch 0060 mean train/dev loss: 114.9011 / 111.9145
[2017-12-15 17:11:38] Learning rate decayed by 0.5000
[2017-12-15 17:11:38] Checkpointing model at epoch 60 for ffn.hl_20_20.lr_0.01.wd_10
[2017-12-15 17:11:38] Model Checkpointing finished.
[2017-12-15 17:11:44] Epoch 0061 mean train/dev loss: 114.6734 / 109.5541
[2017-12-15 17:11:44] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:11:44] 
                       *** Training finished *** 
[2017-12-15 17:11:45] Dev MSE: 109.5541
[2017-12-15 17:11:50] Training MSE: 114.1271
[2017-12-15 17:11:51] Experiment ffn.hl_20_20.lr_0.01.wd_10 logging ended.
