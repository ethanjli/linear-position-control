[2017-12-15 18:07:54] Experiment ffn.hl_100.lr_0.01.wd_0.1 logging started.
[2017-12-15 18:07:54] 
                       *** Starting Experiment ffn.hl_100.lr_0.01.wd_0.1 ***
                      
[2017-12-15 18:07:54] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [100]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 18:07:54] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 100)
                        (relu1): ReLU ()
                        (linear2): Linear (100 -> 1)
                      )
[2017-12-15 18:07:54]  *** Training on GPU ***
[2017-12-15 18:08:01] Epoch 0001 mean train/dev loss: 51784.9245 / 1514.6890
[2017-12-15 18:08:08] Epoch 0002 mean train/dev loss: 458.5932 / 548.1809
[2017-12-15 18:08:16] Epoch 0003 mean train/dev loss: 240.2844 / 328.0083
[2017-12-15 18:08:24] Epoch 0004 mean train/dev loss: 174.6502 / 257.7036
[2017-12-15 18:08:31] Epoch 0005 mean train/dev loss: 142.7735 / 197.4193
[2017-12-15 18:08:39] Epoch 0006 mean train/dev loss: 126.2661 / 173.3605
[2017-12-15 18:08:47] Epoch 0007 mean train/dev loss: 118.7838 / 154.5212
[2017-12-15 18:08:54] Epoch 0008 mean train/dev loss: 114.1393 / 140.7274
[2017-12-15 18:09:02] Epoch 0009 mean train/dev loss: 110.8017 / 125.1008
[2017-12-15 18:09:09] Epoch 0010 mean train/dev loss: 108.9540 / 124.2851
[2017-12-15 18:09:09] Checkpointing model at epoch 10 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:09:09] Model Checkpointing finished.
[2017-12-15 18:09:17] Epoch 0011 mean train/dev loss: 107.5456 / 116.8680
[2017-12-15 18:09:24] Epoch 0012 mean train/dev loss: 106.5405 / 116.8237
[2017-12-15 18:09:31] Epoch 0013 mean train/dev loss: 105.6869 / 118.7663
[2017-12-15 18:09:39] Epoch 0014 mean train/dev loss: 104.9666 / 124.1801
[2017-12-15 18:09:46] Epoch 0015 mean train/dev loss: 105.1922 / 118.4237
[2017-12-15 18:09:46] Learning rate decayed by 0.5000
[2017-12-15 18:09:54] Epoch 0016 mean train/dev loss: 103.4987 / 112.5049
[2017-12-15 18:10:01] Epoch 0017 mean train/dev loss: 103.4140 / 110.2688
[2017-12-15 18:10:08] Epoch 0018 mean train/dev loss: 103.2867 / 113.9104
[2017-12-15 18:10:16] Epoch 0019 mean train/dev loss: 103.2633 / 110.1949
[2017-12-15 18:10:23] Epoch 0020 mean train/dev loss: 103.2168 / 106.4581
[2017-12-15 18:10:23] Checkpointing model at epoch 20 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:10:23] Model Checkpointing finished.
[2017-12-15 18:10:31] Epoch 0021 mean train/dev loss: 103.2252 / 114.2039
[2017-12-15 18:10:38] Epoch 0022 mean train/dev loss: 103.2425 / 114.2697
[2017-12-15 18:10:45] Epoch 0023 mean train/dev loss: 103.0900 / 110.2677
[2017-12-15 18:10:53] Epoch 0024 mean train/dev loss: 103.1838 / 106.8429
[2017-12-15 18:11:00] Epoch 0025 mean train/dev loss: 103.4175 / 114.1275
[2017-12-15 18:11:08] Epoch 0026 mean train/dev loss: 103.4981 / 109.1106
[2017-12-15 18:11:15] Epoch 0027 mean train/dev loss: 103.2767 / 110.1549
[2017-12-15 18:11:22] Epoch 0028 mean train/dev loss: 103.5213 / 109.1028
[2017-12-15 18:11:29] Epoch 0029 mean train/dev loss: 103.4193 / 110.3315
[2017-12-15 18:11:36] Epoch 0030 mean train/dev loss: 103.5965 / 109.0487
[2017-12-15 18:11:36] Learning rate decayed by 0.5000
[2017-12-15 18:11:36] Checkpointing model at epoch 30 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:11:37] Model Checkpointing finished.
[2017-12-15 18:11:44] Epoch 0031 mean train/dev loss: 102.7966 / 105.9122
[2017-12-15 18:11:51] Epoch 0032 mean train/dev loss: 102.7956 / 105.9619
[2017-12-15 18:11:59] Epoch 0033 mean train/dev loss: 102.9031 / 107.2569
[2017-12-15 18:12:06] Epoch 0034 mean train/dev loss: 102.8760 / 104.3641
[2017-12-15 18:12:13] Epoch 0035 mean train/dev loss: 102.9283 / 108.1152
[2017-12-15 18:12:20] Epoch 0036 mean train/dev loss: 102.9701 / 107.5916
[2017-12-15 18:12:28] Epoch 0037 mean train/dev loss: 103.0229 / 106.8584
[2017-12-15 18:12:35] Epoch 0038 mean train/dev loss: 103.0863 / 105.9988
[2017-12-15 18:12:43] Epoch 0039 mean train/dev loss: 103.1155 / 108.3225
[2017-12-15 18:12:50] Epoch 0040 mean train/dev loss: 103.2339 / 108.5597
[2017-12-15 18:12:50] Checkpointing model at epoch 40 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:12:50] Model Checkpointing finished.
[2017-12-15 18:12:57] Epoch 0041 mean train/dev loss: 103.1932 / 105.3381
[2017-12-15 18:13:05] Epoch 0042 mean train/dev loss: 103.2412 / 108.3413
[2017-12-15 18:13:12] Epoch 0043 mean train/dev loss: 103.2796 / 105.9543
[2017-12-15 18:13:19] Epoch 0044 mean train/dev loss: 103.2818 / 106.7753
[2017-12-15 18:13:25] Epoch 0045 mean train/dev loss: 103.3818 / 105.4544
[2017-12-15 18:13:25] Learning rate decayed by 0.5000
[2017-12-15 18:13:33] Epoch 0046 mean train/dev loss: 103.0205 / 104.9477
[2017-12-15 18:13:40] Epoch 0047 mean train/dev loss: 103.0224 / 107.2800
[2017-12-15 18:13:47] Epoch 0048 mean train/dev loss: 103.1114 / 106.4348
[2017-12-15 18:13:55] Epoch 0049 mean train/dev loss: 103.0977 / 103.9984
[2017-12-15 18:14:02] Epoch 0050 mean train/dev loss: 103.1616 / 104.8205
[2017-12-15 18:14:02] Checkpointing model at epoch 50 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:14:02] Model Checkpointing finished.
[2017-12-15 18:14:10] Epoch 0051 mean train/dev loss: 103.1434 / 108.0514
[2017-12-15 18:14:17] Epoch 0052 mean train/dev loss: 103.1694 / 104.1833
[2017-12-15 18:14:23] Epoch 0053 mean train/dev loss: 103.1550 / 104.1831
[2017-12-15 18:14:29] Epoch 0054 mean train/dev loss: 103.2168 / 106.4120
[2017-12-15 18:14:35] Epoch 0055 mean train/dev loss: 103.1988 / 107.5323
[2017-12-15 18:14:41] Epoch 0056 mean train/dev loss: 103.1890 / 107.1008
[2017-12-15 18:14:47] Epoch 0057 mean train/dev loss: 103.2654 / 105.7822
[2017-12-15 18:14:53] Epoch 0058 mean train/dev loss: 103.2515 / 104.5528
[2017-12-15 18:14:59] Epoch 0059 mean train/dev loss: 103.2600 / 104.0666
[2017-12-15 18:15:05] Epoch 0060 mean train/dev loss: 103.2567 / 104.0144
[2017-12-15 18:15:05] Learning rate decayed by 0.5000
[2017-12-15 18:15:05] Checkpointing model at epoch 60 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:15:06] Model Checkpointing finished.
[2017-12-15 18:15:12] Epoch 0061 mean train/dev loss: 103.1088 / 104.6745
[2017-12-15 18:15:18] Epoch 0062 mean train/dev loss: 103.1075 / 105.6494
[2017-12-15 18:15:25] Epoch 0063 mean train/dev loss: 103.1273 / 105.6331
[2017-12-15 18:15:31] Epoch 0064 mean train/dev loss: 103.1646 / 105.4889
[2017-12-15 18:15:38] Epoch 0065 mean train/dev loss: 103.1509 / 106.0353
[2017-12-15 18:15:44] Epoch 0066 mean train/dev loss: 103.1512 / 103.7285
[2017-12-15 18:15:51] Epoch 0067 mean train/dev loss: 103.1471 / 104.2524
[2017-12-15 18:15:57] Epoch 0068 mean train/dev loss: 103.1639 / 105.5473
[2017-12-15 18:16:04] Epoch 0069 mean train/dev loss: 103.1659 / 104.7781
[2017-12-15 18:16:10] Epoch 0070 mean train/dev loss: 103.1836 / 103.6157
[2017-12-15 18:16:10] Checkpointing model at epoch 70 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:16:10] Model Checkpointing finished.
[2017-12-15 18:16:16] Epoch 0071 mean train/dev loss: 103.2024 / 105.1819
[2017-12-15 18:16:21] Epoch 0072 mean train/dev loss: 103.2378 / 105.4569
[2017-12-15 18:16:27] Epoch 0073 mean train/dev loss: 103.2347 / 103.2020
[2017-12-15 18:16:32] Epoch 0074 mean train/dev loss: 103.2283 / 105.2778
[2017-12-15 18:16:37] Epoch 0075 mean train/dev loss: 103.2117 / 105.4753
[2017-12-15 18:16:37] Learning rate decayed by 0.5000
[2017-12-15 18:16:42] Epoch 0076 mean train/dev loss: 103.1365 / 104.7245
[2017-12-15 18:16:46] Epoch 0077 mean train/dev loss: 103.1350 / 104.3936
[2017-12-15 18:16:51] Epoch 0078 mean train/dev loss: 103.1437 / 105.2400
[2017-12-15 18:16:56] Epoch 0079 mean train/dev loss: 103.1365 / 104.0584
[2017-12-15 18:17:01] Epoch 0080 mean train/dev loss: 103.1343 / 104.7170
[2017-12-15 18:17:01] Checkpointing model at epoch 80 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:17:01] Model Checkpointing finished.
[2017-12-15 18:17:06] Epoch 0081 mean train/dev loss: 103.1242 / 105.3244
[2017-12-15 18:17:11] Epoch 0082 mean train/dev loss: 103.1688 / 105.1865
[2017-12-15 18:17:15] Epoch 0083 mean train/dev loss: 103.1923 / 105.0100
[2017-12-15 18:17:21] Epoch 0084 mean train/dev loss: 103.1544 / 105.0163
[2017-12-15 18:17:25] Epoch 0085 mean train/dev loss: 103.1831 / 104.2200
[2017-12-15 18:17:30] Epoch 0086 mean train/dev loss: 103.1662 / 104.9480
[2017-12-15 18:17:35] Epoch 0087 mean train/dev loss: 103.1695 / 104.5123
[2017-12-15 18:17:40] Epoch 0088 mean train/dev loss: 103.1661 / 104.8627
[2017-12-15 18:17:45] Epoch 0089 mean train/dev loss: 103.1936 / 104.8008
[2017-12-15 18:17:50] Epoch 0090 mean train/dev loss: 103.1882 / 105.7569
[2017-12-15 18:17:50] Learning rate decayed by 0.5000
[2017-12-15 18:17:50] Checkpointing model at epoch 90 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:17:50] Model Checkpointing finished.
[2017-12-15 18:17:55] Epoch 0091 mean train/dev loss: 103.1396 / 104.7440
[2017-12-15 18:18:00] Epoch 0092 mean train/dev loss: 103.1413 / 104.8772
[2017-12-15 18:18:05] Epoch 0093 mean train/dev loss: 103.1475 / 105.2423
[2017-12-15 18:18:10] Epoch 0094 mean train/dev loss: 103.1526 / 105.1199
[2017-12-15 18:18:15] Epoch 0095 mean train/dev loss: 103.1247 / 104.1238
[2017-12-15 18:18:20] Epoch 0096 mean train/dev loss: 103.1531 / 105.1304
[2017-12-15 18:18:25] Epoch 0097 mean train/dev loss: 103.1580 / 105.1083
[2017-12-15 18:18:30] Epoch 0098 mean train/dev loss: 103.1620 / 105.5648
[2017-12-15 18:18:35] Epoch 0099 mean train/dev loss: 103.1472 / 104.7688
[2017-12-15 18:18:40] Epoch 0100 mean train/dev loss: 103.1509 / 104.7667
[2017-12-15 18:18:40] Checkpointing model at epoch 100 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:18:41] Model Checkpointing finished.
[2017-12-15 18:18:46] Epoch 0101 mean train/dev loss: 103.1479 / 105.6291
[2017-12-15 18:18:51] Epoch 0102 mean train/dev loss: 103.1664 / 104.9789
[2017-12-15 18:18:56] Epoch 0103 mean train/dev loss: 103.1683 / 104.4162
[2017-12-15 18:19:01] Epoch 0104 mean train/dev loss: 103.1575 / 105.3292
[2017-12-15 18:19:06] Epoch 0105 mean train/dev loss: 103.1654 / 105.1331
[2017-12-15 18:19:06] Learning rate decayed by 0.5000
[2017-12-15 18:19:11] Epoch 0106 mean train/dev loss: 103.1397 / 105.2905
[2017-12-15 18:19:16] Epoch 0107 mean train/dev loss: 103.1289 / 105.5471
[2017-12-15 18:19:21] Epoch 0108 mean train/dev loss: 103.1366 / 105.1309
[2017-12-15 18:19:26] Epoch 0109 mean train/dev loss: 103.1479 / 105.0226
[2017-12-15 18:19:31] Epoch 0110 mean train/dev loss: 103.1450 / 104.5823
[2017-12-15 18:19:31] Checkpointing model at epoch 110 for ffn.hl_100.lr_0.01.wd_0.1
[2017-12-15 18:19:31] Model Checkpointing finished.
[2017-12-15 18:19:36] Epoch 0111 mean train/dev loss: 103.1269 / 105.1153
[2017-12-15 18:19:41] Epoch 0112 mean train/dev loss: 103.1480 / 105.0411
[2017-12-15 18:19:46] Epoch 0113 mean train/dev loss: 103.1409 / 104.5814
[2017-12-15 18:19:51] Epoch 0114 mean train/dev loss: 103.1399 / 104.7530
[2017-12-15 18:19:51] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 18:19:51] 
                       *** Training finished *** 
[2017-12-15 18:19:52] Dev MSE: 104.7530
[2017-12-15 18:19:56] Training MSE: 103.1337
[2017-12-15 18:19:58] Experiment ffn.hl_100.lr_0.01.wd_0.1 logging ended.
