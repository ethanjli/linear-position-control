[2017-12-15 17:03:44] Experiment ffn.hl_20_20.lr_0.01.wd_0.1 logging started.
[2017-12-15 17:03:44] 
                       *** Starting Experiment ffn.hl_20_20.lr_0.01.wd_0.1 ***
                      
[2017-12-15 17:03:44] Hyper parameters
                      [               batch_size] 1024  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_ff  
                      [               early_stop] 40  
                      [            hidden_layers] [20, 20]  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 200  
                      [                 use_cuda] True  
                      [             weight_decay] 0.1  
[2017-12-15 17:03:44] Model architecture
                      Sequential (
                        (linear1): Linear (12 -> 20)
                        (relu1): ReLU ()
                        (linear2): Linear (20 -> 20)
                        (relu2): ReLU ()
                        (linear3): Linear (20 -> 1)
                      )
[2017-12-15 17:03:44]  *** Training on GPU ***
[2017-12-15 17:03:52] Epoch 0001 mean train/dev loss: 36152.0934 / 405.7805
[2017-12-15 17:04:00] Epoch 0002 mean train/dev loss: 171.1435 / 195.5260
[2017-12-15 17:04:09] Epoch 0003 mean train/dev loss: 125.9272 / 145.3969
[2017-12-15 17:04:17] Epoch 0004 mean train/dev loss: 115.5390 / 134.5134
[2017-12-15 17:04:24] Epoch 0005 mean train/dev loss: 112.4497 / 123.8367
[2017-12-15 17:04:32] Epoch 0006 mean train/dev loss: 110.7407 / 124.5611
[2017-12-15 17:04:40] Epoch 0007 mean train/dev loss: 109.6090 / 123.0164
[2017-12-15 17:04:48] Epoch 0008 mean train/dev loss: 108.4616 / 121.2940
[2017-12-15 17:04:56] Epoch 0009 mean train/dev loss: 107.1110 / 131.2868
[2017-12-15 17:05:05] Epoch 0010 mean train/dev loss: 104.5454 / 129.1058
[2017-12-15 17:05:05] Checkpointing model at epoch 10 for ffn.hl_20_20.lr_0.01.wd_0.1
[2017-12-15 17:05:05] Model Checkpointing finished.
[2017-12-15 17:05:13] Epoch 0011 mean train/dev loss: 101.8422 / 127.6968
[2017-12-15 17:05:21] Epoch 0012 mean train/dev loss: 99.9473 / 120.1690
[2017-12-15 17:05:29] Epoch 0013 mean train/dev loss: 97.8135 / 113.6300
[2017-12-15 17:05:37] Epoch 0014 mean train/dev loss: 95.1558 / 105.6891
[2017-12-15 17:05:45] Epoch 0015 mean train/dev loss: 92.2338 / 108.7058
[2017-12-15 17:05:45] Learning rate decayed by 0.5000
[2017-12-15 17:05:53] Epoch 0016 mean train/dev loss: 89.0221 / 111.0972
[2017-12-15 17:06:01] Epoch 0017 mean train/dev loss: 88.2554 / 128.3310
[2017-12-15 17:06:10] Epoch 0018 mean train/dev loss: 87.4925 / 116.9777
[2017-12-15 17:06:18] Epoch 0019 mean train/dev loss: 86.6827 / 145.2382
[2017-12-15 17:06:26] Epoch 0020 mean train/dev loss: 85.6746 / 132.7507
[2017-12-15 17:06:26] Checkpointing model at epoch 20 for ffn.hl_20_20.lr_0.01.wd_0.1
[2017-12-15 17:06:26] Model Checkpointing finished.
[2017-12-15 17:06:34] Epoch 0021 mean train/dev loss: 84.9672 / 125.3391
[2017-12-15 17:06:42] Epoch 0022 mean train/dev loss: 84.3830 / 126.8065
[2017-12-15 17:06:50] Epoch 0023 mean train/dev loss: 83.6713 / 144.7455
[2017-12-15 17:06:59] Epoch 0024 mean train/dev loss: 83.4701 / 117.0800
[2017-12-15 17:07:07] Epoch 0025 mean train/dev loss: 83.1343 / 141.4454
[2017-12-15 17:07:15] Epoch 0026 mean train/dev loss: 82.6469 / 170.0453
[2017-12-15 17:07:23] Epoch 0027 mean train/dev loss: 82.2314 / 134.5291
[2017-12-15 17:07:31] Epoch 0028 mean train/dev loss: 81.4889 / 124.3364
[2017-12-15 17:07:39] Epoch 0029 mean train/dev loss: 80.7179 / 138.3583
[2017-12-15 17:07:46] Epoch 0030 mean train/dev loss: 80.6508 / 148.0269
[2017-12-15 17:07:46] Learning rate decayed by 0.5000
[2017-12-15 17:07:46] Checkpointing model at epoch 30 for ffn.hl_20_20.lr_0.01.wd_0.1
[2017-12-15 17:07:47] Model Checkpointing finished.
[2017-12-15 17:07:55] Epoch 0031 mean train/dev loss: 79.1269 / 120.2519
[2017-12-15 17:08:03] Epoch 0032 mean train/dev loss: 79.0808 / 142.7314
[2017-12-15 17:08:11] Epoch 0033 mean train/dev loss: 79.1258 / 124.1199
[2017-12-15 17:08:19] Epoch 0034 mean train/dev loss: 78.9679 / 126.1985
[2017-12-15 17:08:27] Epoch 0035 mean train/dev loss: 79.0368 / 117.9477
[2017-12-15 17:08:36] Epoch 0036 mean train/dev loss: 78.9531 / 125.2753
[2017-12-15 17:08:44] Epoch 0037 mean train/dev loss: 78.7347 / 120.8702
[2017-12-15 17:08:52] Epoch 0038 mean train/dev loss: 78.6711 / 132.6913
[2017-12-15 17:09:00] Epoch 0039 mean train/dev loss: 78.7107 / 129.2253
[2017-12-15 17:09:08] Epoch 0040 mean train/dev loss: 78.5210 / 126.0190
[2017-12-15 17:09:08] Checkpointing model at epoch 40 for ffn.hl_20_20.lr_0.01.wd_0.1
[2017-12-15 17:09:08] Model Checkpointing finished.
[2017-12-15 17:09:16] Epoch 0041 mean train/dev loss: 78.3821 / 128.2205
[2017-12-15 17:09:24] Epoch 0042 mean train/dev loss: 78.4416 / 117.4116
[2017-12-15 17:09:32] Epoch 0043 mean train/dev loss: 78.1890 / 128.1467
[2017-12-15 17:09:40] Epoch 0044 mean train/dev loss: 78.1640 / 120.9269
[2017-12-15 17:09:48] Epoch 0045 mean train/dev loss: 78.1580 / 131.9776
[2017-12-15 17:09:48] Learning rate decayed by 0.5000
[2017-12-15 17:09:56] Epoch 0046 mean train/dev loss: 77.4291 / 125.3335
[2017-12-15 17:10:04] Epoch 0047 mean train/dev loss: 77.4613 / 117.2022
[2017-12-15 17:10:12] Epoch 0048 mean train/dev loss: 77.4952 / 121.1729
[2017-12-15 17:10:20] Epoch 0049 mean train/dev loss: 77.3799 / 124.7114
[2017-12-15 17:10:28] Epoch 0050 mean train/dev loss: 77.4442 / 123.2014
[2017-12-15 17:10:28] Checkpointing model at epoch 50 for ffn.hl_20_20.lr_0.01.wd_0.1
[2017-12-15 17:10:28] Model Checkpointing finished.
[2017-12-15 17:10:36] Epoch 0051 mean train/dev loss: 77.3689 / 115.0769
[2017-12-15 17:10:44] Epoch 0052 mean train/dev loss: 77.3563 / 116.5795
[2017-12-15 17:10:52] Epoch 0053 mean train/dev loss: 77.2791 / 125.8844
[2017-12-15 17:11:00] Epoch 0054 mean train/dev loss: 77.2758 / 112.5908
[2017-12-15 17:11:07] Epoch 0055 mean train/dev loss: 77.2333 / 121.0637
[2017-12-15 17:11:07] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 17:11:07] 
                       *** Training finished *** 
[2017-12-15 17:11:09] Dev MSE: 121.0637
[2017-12-15 17:11:15] Training MSE: 76.9827
[2017-12-15 17:11:18] Experiment ffn.hl_20_20.lr_0.01.wd_0.1 logging ended.
