[2017-12-15 01:02:33] Experiment lstm.hs_50.nl_2.lr_0.1.wd_0.001.rl_40 logging started.
[2017-12-15 01:02:33] 
                       *** Starting Experiment lstm.hs_50.nl_2.lr_0.1.wd_0.001.rl_40 ***
                      
[2017-12-15 01:02:33] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 50  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 300  
                      [               num_layers] 2  
                      [        regression_layers] [40]  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 01:02:36] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 50, num_layers=2, batch_first=True)
                        (linear1): Linear (50 -> 40)
                        (final): Linear (40 -> 1)
                      )
[2017-12-15 01:02:36]  *** Training on GPU ***
[2017-12-15 01:03:44] Epoch 0001 mean train/dev loss: 82750.4837 / 26342.2949
[2017-12-15 01:03:44] Checkpointing model...
[2017-12-15 01:03:44] Model Checkpointing finished.
[2017-12-15 01:04:48] Epoch 0002 mean train/dev loss: 9815.2545 / 4477.3740
[2017-12-15 01:04:48] Checkpointing model...
[2017-12-15 01:04:48] Model Checkpointing finished.
[2017-12-15 01:05:52] Epoch 0003 mean train/dev loss: 9399.6480 / 7205.4116
[2017-12-15 01:05:52] Checkpointing model...
[2017-12-15 01:05:52] Model Checkpointing finished.
[2017-12-15 01:06:53] Epoch 0004 mean train/dev loss: 4129.5769 / 2509.9089
[2017-12-15 01:06:53] Checkpointing model...
[2017-12-15 01:06:53] Model Checkpointing finished.
[2017-12-15 01:07:55] Epoch 0005 mean train/dev loss: 1666.4955 / 1274.0909
[2017-12-15 01:07:55] Checkpointing model...
[2017-12-15 01:07:55] Model Checkpointing finished.
[2017-12-15 01:08:59] Epoch 0006 mean train/dev loss: 2707.5814 / 2102.6841
[2017-12-15 01:10:02] Epoch 0007 mean train/dev loss: 3117.3633 / 2203.8459
[2017-12-15 01:11:06] Epoch 0008 mean train/dev loss: 1287.9623 / 1036.8439
[2017-12-15 01:12:10] Epoch 0009 mean train/dev loss: 737.8206 / 852.8562
[2017-12-15 01:13:15] Epoch 0010 mean train/dev loss: 818.3729 / 1172.2808
[2017-12-15 01:13:15] Checkpointing model...
[2017-12-15 01:13:16] Model Checkpointing finished.
[2017-12-15 01:14:19] Epoch 0011 mean train/dev loss: 686.2206 / 708.9936
[2017-12-15 01:15:23] Epoch 0012 mean train/dev loss: 586.7407 / 647.5770
[2017-12-15 01:16:27] Epoch 0013 mean train/dev loss: 622.8842 / 788.9230
[2017-12-15 01:17:30] Epoch 0014 mean train/dev loss: 634.8920 / 879.4191
[2017-12-15 01:18:36] Epoch 0015 mean train/dev loss: 571.1071 / 781.0073
[2017-12-15 01:18:36] Learning rate decayed by 0.5000
[2017-12-15 01:18:36] Checkpointing model...
[2017-12-15 01:18:36] Model Checkpointing finished.
[2017-12-15 01:19:41] Epoch 0016 mean train/dev loss: 434.7191 / 666.2921
[2017-12-15 01:20:44] Epoch 0017 mean train/dev loss: 468.4819 / 736.2885
[2017-12-15 01:21:47] Epoch 0018 mean train/dev loss: 422.4223 / 870.7747
[2017-12-15 01:22:47] Epoch 0019 mean train/dev loss: 468.8888 / 773.2010
[2017-12-15 01:23:46] Epoch 0020 mean train/dev loss: 426.6543 / 407.4482
[2017-12-15 01:23:46] Checkpointing model...
[2017-12-15 01:23:47] Model Checkpointing finished.
[2017-12-15 01:24:52] Epoch 0021 mean train/dev loss: 304.7730 / 517.9272
[2017-12-15 01:25:57] Epoch 0022 mean train/dev loss: 332.0908 / 889.4487
[2017-12-15 01:27:00] Epoch 0023 mean train/dev loss: 405.7572 / 1083.6116
[2017-12-15 01:28:02] Epoch 0024 mean train/dev loss: 304.7110 / 658.2681
[2017-12-15 01:29:03] Epoch 0025 mean train/dev loss: 288.9547 / 424.4536
[2017-12-15 01:29:03] Checkpointing model...
[2017-12-15 01:29:03] Model Checkpointing finished.
[2017-12-15 01:30:11] Epoch 0026 mean train/dev loss: 277.9920 / 607.0790
[2017-12-15 01:31:15] Epoch 0027 mean train/dev loss: 271.6988 / 619.0850
[2017-12-15 01:32:19] Epoch 0028 mean train/dev loss: 396.8643 / 643.6371
[2017-12-15 01:33:25] Epoch 0029 mean train/dev loss: 297.2667 / 658.1564
[2017-12-15 01:34:27] Epoch 0030 mean train/dev loss: 297.1960 / 575.6519
[2017-12-15 01:34:27] Learning rate decayed by 0.5000
[2017-12-15 01:34:27] Checkpointing model...
[2017-12-15 01:34:27] Model Checkpointing finished.
[2017-12-15 01:35:27] Epoch 0031 mean train/dev loss: 335.5749 / 544.7989
[2017-12-15 01:36:32] Epoch 0032 mean train/dev loss: 267.8418 / 397.8833
[2017-12-15 01:37:34] Epoch 0033 mean train/dev loss: 221.8196 / 531.3152
[2017-12-15 01:38:34] Epoch 0034 mean train/dev loss: 204.7825 / 473.2623
[2017-12-15 01:39:41] Epoch 0035 mean train/dev loss: 182.5574 / 409.6711
[2017-12-15 01:39:41] Checkpointing model...
[2017-12-15 01:39:41] Model Checkpointing finished.
[2017-12-15 01:40:44] Epoch 0036 mean train/dev loss: 212.7045 / 568.1310
[2017-12-15 01:41:43] Epoch 0037 mean train/dev loss: 187.0251 / 494.1781
[2017-12-15 01:42:44] Epoch 0038 mean train/dev loss: 231.1490 / 585.2584
[2017-12-15 01:43:49] Epoch 0039 mean train/dev loss: 213.0784 / 620.3652
[2017-12-15 01:44:52] Epoch 0040 mean train/dev loss: 178.2154 / 416.2005
[2017-12-15 01:44:52] Checkpointing model...
[2017-12-15 01:44:52] Model Checkpointing finished.
[2017-12-15 01:45:55] Epoch 0041 mean train/dev loss: 157.0873 / 466.8798
[2017-12-15 01:46:59] Epoch 0042 mean train/dev loss: 146.6026 / 484.0053
[2017-12-15 01:48:00] Epoch 0043 mean train/dev loss: 143.0935 / 382.6772
[2017-12-15 01:49:00] Epoch 0044 mean train/dev loss: 145.9492 / 565.5684
[2017-12-15 01:50:05] Epoch 0045 mean train/dev loss: 162.5722 / 425.9456
[2017-12-15 01:50:05] Learning rate decayed by 0.5000
[2017-12-15 01:50:05] Checkpointing model...
[2017-12-15 01:50:05] Model Checkpointing finished.
[2017-12-15 01:51:08] Epoch 0046 mean train/dev loss: 130.4978 / 380.7228
[2017-12-15 01:52:10] Epoch 0047 mean train/dev loss: 114.6640 / 374.6866
[2017-12-15 01:53:14] Epoch 0048 mean train/dev loss: 109.4550 / 434.4384
[2017-12-15 01:54:17] Epoch 0049 mean train/dev loss: 105.4753 / 442.1345
[2017-12-15 01:55:20] Epoch 0050 mean train/dev loss: 104.5007 / 417.9417
[2017-12-15 01:55:20] Checkpointing model...
[2017-12-15 01:55:20] Model Checkpointing finished.
[2017-12-15 01:56:23] Epoch 0051 mean train/dev loss: 104.7750 / 438.6416
[2017-12-15 01:57:25] Epoch 0052 mean train/dev loss: 103.4354 / 438.8302
[2017-12-15 01:58:25] Epoch 0053 mean train/dev loss: 97.6667 / 431.5811
[2017-12-15 01:59:27] Epoch 0054 mean train/dev loss: 105.7451 / 423.4891
[2017-12-15 02:00:28] Epoch 0055 mean train/dev loss: 97.2657 / 444.6216
[2017-12-15 02:00:28] Checkpointing model...
[2017-12-15 02:00:28] Model Checkpointing finished.
[2017-12-15 02:01:30] Epoch 0056 mean train/dev loss: 97.6092 / 418.3173
[2017-12-15 02:02:30] Epoch 0057 mean train/dev loss: 91.3607 / 412.8554
[2017-12-15 02:03:34] Epoch 0058 mean train/dev loss: 92.7641 / 445.0869
[2017-12-15 02:04:35] Epoch 0059 mean train/dev loss: 88.4081 / 437.0929
[2017-12-15 02:05:37] Epoch 0060 mean train/dev loss: 87.6031 / 423.1187
[2017-12-15 02:05:37] Learning rate decayed by 0.5000
[2017-12-15 02:05:37] Checkpointing model...
[2017-12-15 02:05:38] Model Checkpointing finished.
[2017-12-15 02:06:39] Epoch 0061 mean train/dev loss: 86.3165 / 408.9872
[2017-12-15 02:07:39] Epoch 0062 mean train/dev loss: 81.6661 / 404.0055
[2017-12-15 02:08:40] Epoch 0063 mean train/dev loss: 80.8240 / 422.9225
[2017-12-15 02:09:42] Epoch 0064 mean train/dev loss: 81.8350 / 419.9254
[2017-12-15 02:10:44] Epoch 0065 mean train/dev loss: 81.8208 / 416.9806
[2017-12-15 02:10:44] Checkpointing model...
[2017-12-15 02:10:45] Model Checkpointing finished.
[2017-12-15 02:11:45] Epoch 0066 mean train/dev loss: 80.5804 / 404.8264
[2017-12-15 02:12:48] Epoch 0067 mean train/dev loss: 83.6138 / 407.0450
[2017-12-15 02:13:54] Epoch 0068 mean train/dev loss: 82.0292 / 401.5388
[2017-12-15 02:14:54] Epoch 0069 mean train/dev loss: 78.7761 / 396.7548
[2017-12-15 02:15:56] Epoch 0070 mean train/dev loss: 78.1726 / 399.9478
[2017-12-15 02:15:56] Checkpointing model...
[2017-12-15 02:15:56] Model Checkpointing finished.
[2017-12-15 02:16:58] Epoch 0071 mean train/dev loss: 78.2252 / 406.3304
[2017-12-15 02:18:00] Epoch 0072 mean train/dev loss: 78.4300 / 416.1808
[2017-12-15 02:19:02] Epoch 0073 mean train/dev loss: 77.0437 / 408.8661
[2017-12-15 02:20:05] Epoch 0074 mean train/dev loss: 78.7404 / 389.1482
[2017-12-15 02:21:06] Epoch 0075 mean train/dev loss: 76.7261 / 392.6946
[2017-12-15 02:21:06] Learning rate decayed by 0.5000
[2017-12-15 02:21:06] Checkpointing model...
[2017-12-15 02:21:06] Model Checkpointing finished.
[2017-12-15 02:22:07] Epoch 0076 mean train/dev loss: 73.3562 / 399.8154
[2017-12-15 02:23:06] Epoch 0077 mean train/dev loss: 71.0170 / 406.1478
[2017-12-15 02:24:10] Epoch 0078 mean train/dev loss: 70.4852 / 398.6568
[2017-12-15 02:25:11] Epoch 0079 mean train/dev loss: 70.3312 / 408.7934
[2017-12-15 02:26:12] Epoch 0080 mean train/dev loss: 69.4767 / 393.0835
[2017-12-15 02:26:12] Checkpointing model...
[2017-12-15 02:26:13] Model Checkpointing finished.
[2017-12-15 02:27:17] Epoch 0081 mean train/dev loss: 69.9604 / 390.6351
[2017-12-15 02:28:21] Epoch 0082 mean train/dev loss: 70.5702 / 401.2601
[2017-12-15 02:29:24] Epoch 0083 mean train/dev loss: 69.3052 / 400.2321
[2017-12-15 02:30:27] Epoch 0084 mean train/dev loss: 69.4196 / 403.8295
[2017-12-15 02:31:26] Epoch 0085 mean train/dev loss: 69.7739 / 407.9702
[2017-12-15 02:31:26] Checkpointing model...
[2017-12-15 02:31:27] Model Checkpointing finished.
[2017-12-15 02:32:28] Epoch 0086 mean train/dev loss: 69.8949 / 389.4193
[2017-12-15 02:33:29] Epoch 0087 mean train/dev loss: 70.3366 / 390.9268
[2017-12-15 02:34:35] Epoch 0088 mean train/dev loss: 68.7714 / 388.1913
[2017-12-15 02:34:35] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 02:34:35] 
                       *** Training finished *** 
[2017-12-15 02:34:41] Dev MSE: 388.1913
[2017-12-15 02:35:25] Experiment lstm.hs_50.nl_2.lr_0.1.wd_0.001.rl_40 logging ended.
