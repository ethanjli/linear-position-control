[2017-12-14 15:35:51] Experiment lstm.hs_50.nl_1.lr_0.1.wd_0.001.rl_linear logging started.
[2017-12-14 15:35:51] 
                       *** Starting Experiment lstm.hs_50.nl_1.lr_0.1.wd_0.001.rl_linear ***
                      
[2017-12-14 15:35:51] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 50  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 300  
                      [               num_layers] 1  
                      [        regression_layers] None  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-14 15:35:54] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 50, batch_first=True)
                        (final): Linear (50 -> 1)
                      )
[2017-12-14 15:35:54]  *** Training on GPU ***
[2017-12-14 15:36:55] Epoch 0001 mean train/dev loss: 254711.8332 / 188500.1406
[2017-12-14 15:36:55] Checkpointing model...
[2017-12-14 15:36:56] Model Checkpointing finished.
[2017-12-14 15:37:56] Epoch 0002 mean train/dev loss: 143237.4847 / 109998.0781
[2017-12-14 15:37:56] Checkpointing model...
[2017-12-14 15:37:57] Model Checkpointing finished.
[2017-12-14 15:39:00] Epoch 0003 mean train/dev loss: 80805.3199 / 60409.1836
[2017-12-14 15:39:00] Checkpointing model...
[2017-12-14 15:39:01] Model Checkpointing finished.
[2017-12-14 15:40:01] Epoch 0004 mean train/dev loss: 45913.2826 / 36644.6445
[2017-12-14 15:40:01] Checkpointing model...
[2017-12-14 15:40:01] Model Checkpointing finished.
[2017-12-14 15:41:02] Epoch 0005 mean train/dev loss: 24220.5874 / 25802.4922
[2017-12-14 15:41:02] Checkpointing model...
[2017-12-14 15:41:02] Model Checkpointing finished.
[2017-12-14 15:42:04] Epoch 0006 mean train/dev loss: 12875.3186 / 9077.3301
[2017-12-14 15:43:07] Epoch 0007 mean train/dev loss: 8393.9804 / 8579.2471
[2017-12-14 15:44:08] Epoch 0008 mean train/dev loss: 5353.4423 / 4465.5571
[2017-12-14 15:45:12] Epoch 0009 mean train/dev loss: 3780.6337 / 3240.7759
[2017-12-14 15:46:13] Epoch 0010 mean train/dev loss: 2667.8030 / 2258.9333
[2017-12-14 15:46:13] Checkpointing model...
[2017-12-14 15:46:13] Model Checkpointing finished.
[2017-12-14 15:47:10] Epoch 0011 mean train/dev loss: 1924.2910 / 1548.5341
[2017-12-14 15:48:11] Epoch 0012 mean train/dev loss: 2233.0956 / 2762.6387
[2017-12-14 15:49:09] Epoch 0013 mean train/dev loss: 11970.5515 / 10878.2559
[2017-12-14 15:50:08] Epoch 0014 mean train/dev loss: 17482.1312 / 40045.2773
[2017-12-14 15:51:09] Epoch 0015 mean train/dev loss: 29779.9620 / 26675.3574
[2017-12-14 15:51:09] Learning rate decayed by 0.5000
[2017-12-14 15:51:09] Checkpointing model...
[2017-12-14 15:51:09] Model Checkpointing finished.
[2017-12-14 15:52:12] Epoch 0016 mean train/dev loss: 21008.4699 / 16225.1738
[2017-12-14 15:53:16] Epoch 0017 mean train/dev loss: 26966.2182 / 28657.5703
[2017-12-14 15:54:18] Epoch 0018 mean train/dev loss: 25946.8803 / 24220.6465
[2017-12-14 15:55:18] Epoch 0019 mean train/dev loss: 35950.9905 / 48592.5898
[2017-12-14 15:56:20] Epoch 0020 mean train/dev loss: 40666.6656 / 32841.2461
[2017-12-14 15:56:20] Checkpointing model...
[2017-12-14 15:56:20] Model Checkpointing finished.
[2017-12-14 15:57:19] Epoch 0021 mean train/dev loss: 28056.7688 / 25690.0273
[2017-12-14 15:58:23] Epoch 0022 mean train/dev loss: 24355.2215 / 23138.8359
[2017-12-14 15:59:25] Epoch 0023 mean train/dev loss: 21690.3320 / 21514.9434
[2017-12-14 16:00:27] Epoch 0024 mean train/dev loss: 20416.2761 / 20228.7754
[2017-12-14 16:01:26] Epoch 0025 mean train/dev loss: 19413.9190 / 19235.5762
[2017-12-14 16:01:26] Checkpointing model...
[2017-12-14 16:01:27] Model Checkpointing finished.
[2017-12-14 16:02:31] Epoch 0026 mean train/dev loss: 20294.9487 / 21467.4414
[2017-12-14 16:03:34] Epoch 0027 mean train/dev loss: 20580.3415 / 20313.5293
[2017-12-14 16:04:35] Epoch 0028 mean train/dev loss: 19488.8154 / 19999.4258
[2017-12-14 16:05:38] Epoch 0029 mean train/dev loss: 18786.6093 / 18909.3867
[2017-12-14 16:06:38] Epoch 0030 mean train/dev loss: 18514.3768 / 20248.6895
[2017-12-14 16:06:38] Learning rate decayed by 0.5000
[2017-12-14 16:06:38] Checkpointing model...
[2017-12-14 16:06:38] Model Checkpointing finished.
[2017-12-14 16:07:39] Epoch 0031 mean train/dev loss: 20857.0210 / 20972.4531
[2017-12-14 16:08:42] Epoch 0032 mean train/dev loss: 22405.8429 / 22350.6562
[2017-12-14 16:09:44] Epoch 0033 mean train/dev loss: 24276.5722 / 26848.8105
[2017-12-14 16:10:45] Epoch 0034 mean train/dev loss: 25075.7280 / 24485.0156
[2017-12-14 16:11:46] Epoch 0035 mean train/dev loss: 23047.7190 / 23675.5254
[2017-12-14 16:11:46] Checkpointing model...
[2017-12-14 16:11:47] Model Checkpointing finished.
[2017-12-14 16:12:52] Epoch 0036 mean train/dev loss: 26048.8002 / 29226.1172
[2017-12-14 16:13:54] Epoch 0037 mean train/dev loss: 28117.6028 / 26571.0078
[2017-12-14 16:14:54] Epoch 0038 mean train/dev loss: 25033.7292 / 23966.4160
[2017-12-14 16:15:53] Epoch 0039 mean train/dev loss: 23172.9729 / 23584.1816
[2017-12-14 16:16:55] Epoch 0040 mean train/dev loss: 22741.9550 / 22625.4570
[2017-12-14 16:16:55] Checkpointing model...
[2017-12-14 16:16:55] Model Checkpointing finished.
[2017-12-14 16:17:58] Epoch 0041 mean train/dev loss: 21590.3256 / 21302.0293
[2017-12-14 16:18:59] Epoch 0042 mean train/dev loss: 20695.4248 / 20798.8926
[2017-12-14 16:20:00] Epoch 0043 mean train/dev loss: 20085.3931 / 20365.4883
[2017-12-14 16:21:02] Epoch 0044 mean train/dev loss: 19766.1381 / 19660.3320
[2017-12-14 16:22:04] Epoch 0045 mean train/dev loss: 19095.3156 / 19192.4727
[2017-12-14 16:22:04] Learning rate decayed by 0.5000
[2017-12-14 16:22:04] Checkpointing model...
[2017-12-14 16:22:04] Model Checkpointing finished.
[2017-12-14 16:23:06] Epoch 0046 mean train/dev loss: 18752.0335 / 19058.3984
[2017-12-14 16:24:07] Epoch 0047 mean train/dev loss: 18657.6948 / 18744.3203
[2017-12-14 16:25:09] Epoch 0048 mean train/dev loss: 18209.6173 / 18284.5449
[2017-12-14 16:26:13] Epoch 0049 mean train/dev loss: 17631.8645 / 17467.1836
[2017-12-14 16:27:14] Epoch 0050 mean train/dev loss: 16874.3525 / 17006.7598
[2017-12-14 16:27:14] Checkpointing model...
[2017-12-14 16:27:14] Model Checkpointing finished.
[2017-12-14 16:28:14] Epoch 0051 mean train/dev loss: 16910.5561 / 17304.3320
[2017-12-14 16:29:15] Epoch 0052 mean train/dev loss: 16669.7493 / 16907.6016
[2017-12-14 16:29:15] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-14 16:29:15] 
                       *** Training finished *** 
[2017-12-14 16:29:21] Dev MSE: 16907.6016
[2017-12-14 16:30:15] Training MSE: 16548.8145
[2017-12-14 16:30:15] Experiment lstm.hs_50.nl_1.lr_0.1.wd_0.001.rl_linear logging ended.
