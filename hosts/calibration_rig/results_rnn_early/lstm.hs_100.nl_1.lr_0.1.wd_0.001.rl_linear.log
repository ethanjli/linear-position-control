[2017-12-14 15:35:51] Experiment lstm.hs_100.nl_1.lr_0.1.wd_0.001.rl_linear logging started.
[2017-12-14 15:35:51] 
                       *** Starting Experiment lstm.hs_100.nl_1.lr_0.1.wd_0.001.rl_linear ***
                      
[2017-12-14 15:35:51] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 100  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 300  
                      [               num_layers] 1  
                      [        regression_layers] None  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-14 15:35:54] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 100, batch_first=True)
                        (final): Linear (100 -> 1)
                      )
[2017-12-14 15:35:54]  *** Training on GPU ***
[2017-12-14 15:36:56] Epoch 0001 mean train/dev loss: 206354.1438 / 114034.0859
[2017-12-14 15:36:56] Checkpointing model...
[2017-12-14 15:36:57] Model Checkpointing finished.
[2017-12-14 15:37:56] Epoch 0002 mean train/dev loss: 79569.8325 / 58369.9297
[2017-12-14 15:37:56] Checkpointing model...
[2017-12-14 15:37:57] Model Checkpointing finished.
[2017-12-14 15:38:57] Epoch 0003 mean train/dev loss: 45261.6434 / 35275.1875
[2017-12-14 15:38:57] Checkpointing model...
[2017-12-14 15:38:57] Model Checkpointing finished.
[2017-12-14 15:40:00] Epoch 0004 mean train/dev loss: 24938.1050 / 28383.8965
[2017-12-14 15:40:00] Checkpointing model...
[2017-12-14 15:40:00] Model Checkpointing finished.
[2017-12-14 15:41:01] Epoch 0005 mean train/dev loss: 29213.7909 / 19129.8125
[2017-12-14 15:41:01] Checkpointing model...
[2017-12-14 15:41:02] Model Checkpointing finished.
[2017-12-14 15:42:05] Epoch 0006 mean train/dev loss: 15629.5672 / 20141.3145
[2017-12-14 15:43:07] Epoch 0007 mean train/dev loss: 19273.9564 / 14918.6670
[2017-12-14 15:44:11] Epoch 0008 mean train/dev loss: 17642.7965 / 38220.0039
[2017-12-14 15:45:12] Epoch 0009 mean train/dev loss: 23402.7218 / 14111.6709
[2017-12-14 15:46:13] Epoch 0010 mean train/dev loss: 13384.1649 / 8203.3359
[2017-12-14 15:46:13] Checkpointing model...
[2017-12-14 15:46:14] Model Checkpointing finished.
[2017-12-14 15:47:15] Epoch 0011 mean train/dev loss: 13396.3819 / 56402.5156
[2017-12-14 15:48:19] Epoch 0012 mean train/dev loss: 39886.9523 / 70100.8906
[2017-12-14 15:49:22] Epoch 0013 mean train/dev loss: 52546.5701 / 37795.4023
[2017-12-14 15:50:26] Epoch 0014 mean train/dev loss: 34168.8957 / 33156.0508
[2017-12-14 15:51:27] Epoch 0015 mean train/dev loss: 27123.4848 / 25664.9902
[2017-12-14 15:51:27] Learning rate decayed by 0.5000
[2017-12-14 15:51:27] Checkpointing model...
[2017-12-14 15:51:27] Model Checkpointing finished.
[2017-12-14 15:52:30] Epoch 0016 mean train/dev loss: 23357.6307 / 22912.0215
[2017-12-14 15:53:31] Epoch 0017 mean train/dev loss: 24074.9531 / 20997.7285
[2017-12-14 15:54:32] Epoch 0018 mean train/dev loss: 23214.0774 / 20952.6797
[2017-12-14 15:55:32] Epoch 0019 mean train/dev loss: 24912.5720 / 22286.5996
[2017-12-14 15:56:34] Epoch 0020 mean train/dev loss: 20412.4345 / 20187.0840
[2017-12-14 15:56:34] Checkpointing model...
[2017-12-14 15:56:34] Model Checkpointing finished.
[2017-12-14 15:57:37] Epoch 0021 mean train/dev loss: 19732.2627 / 27721.4902
[2017-12-14 15:58:41] Epoch 0022 mean train/dev loss: 19901.9214 / 17699.5176
[2017-12-14 15:59:42] Epoch 0023 mean train/dev loss: 13033.8254 / 11775.4141
[2017-12-14 16:00:48] Epoch 0024 mean train/dev loss: 10504.7989 / 10570.0537
[2017-12-14 16:01:52] Epoch 0025 mean train/dev loss: 9309.5120 / 9302.9717
[2017-12-14 16:01:52] Checkpointing model...
[2017-12-14 16:01:52] Model Checkpointing finished.
[2017-12-14 16:02:51] Epoch 0026 mean train/dev loss: 8354.3340 / 8386.3809
[2017-12-14 16:03:52] Epoch 0027 mean train/dev loss: 14621.6833 / 15450.3662
[2017-12-14 16:04:53] Epoch 0028 mean train/dev loss: 15636.8805 / 15612.6641
[2017-12-14 16:05:56] Epoch 0029 mean train/dev loss: 13208.7689 / 12228.2559
[2017-12-14 16:06:59] Epoch 0030 mean train/dev loss: 10368.0172 / 9647.7734
[2017-12-14 16:06:59] Learning rate decayed by 0.5000
[2017-12-14 16:06:59] Checkpointing model...
[2017-12-14 16:07:00] Model Checkpointing finished.
[2017-12-14 16:08:00] Epoch 0031 mean train/dev loss: 9129.3085 / 9061.1797
[2017-12-14 16:09:01] Epoch 0032 mean train/dev loss: 11667.7582 / 12720.3057
[2017-12-14 16:10:03] Epoch 0033 mean train/dev loss: 12852.1650 / 13206.2100
[2017-12-14 16:11:05] Epoch 0034 mean train/dev loss: 18470.8169 / 20098.5039
[2017-12-14 16:12:03] Epoch 0035 mean train/dev loss: 22013.6028 / 43049.2891
[2017-12-14 16:12:03] Checkpointing model...
[2017-12-14 16:12:03] Model Checkpointing finished.
[2017-12-14 16:13:05] Epoch 0036 mean train/dev loss: 46617.0190 / 46744.6836
[2017-12-14 16:14:04] Epoch 0037 mean train/dev loss: 42015.0552 / 40906.3320
[2017-12-14 16:15:07] Epoch 0038 mean train/dev loss: 36489.6764 / 35823.3047
[2017-12-14 16:16:10] Epoch 0039 mean train/dev loss: 46919.6044 / 46932.2969
[2017-12-14 16:17:12] Epoch 0040 mean train/dev loss: 44053.9637 / 42172.9180
[2017-12-14 16:17:12] Checkpointing model...
[2017-12-14 16:17:12] Model Checkpointing finished.
[2017-12-14 16:18:15] Epoch 0041 mean train/dev loss: 39870.2417 / 40008.8047
[2017-12-14 16:19:19] Epoch 0042 mean train/dev loss: 36316.5086 / 36094.8516
[2017-12-14 16:20:21] Epoch 0043 mean train/dev loss: 34542.9735 / 34612.3281
[2017-12-14 16:21:22] Epoch 0044 mean train/dev loss: 32700.7791 / 32719.2051
[2017-12-14 16:22:26] Epoch 0045 mean train/dev loss: 31146.7936 / 31675.1270
[2017-12-14 16:22:26] Learning rate decayed by 0.5000
[2017-12-14 16:22:26] Checkpointing model...
[2017-12-14 16:22:26] Model Checkpointing finished.
[2017-12-14 16:23:25] Epoch 0046 mean train/dev loss: 30476.2513 / 31111.4844
[2017-12-14 16:24:26] Epoch 0047 mean train/dev loss: 29997.1647 / 30625.7070
[2017-12-14 16:25:28] Epoch 0048 mean train/dev loss: 29590.3536 / 30275.0879
[2017-12-14 16:26:27] Epoch 0049 mean train/dev loss: 29202.6252 / 29783.2070
[2017-12-14 16:27:30] Epoch 0050 mean train/dev loss: 28785.6083 / 29659.4258
[2017-12-14 16:27:30] Checkpointing model...
[2017-12-14 16:27:31] Model Checkpointing finished.
[2017-12-14 16:28:30] Epoch 0051 mean train/dev loss: 28466.0145 / 29120.9863
[2017-12-14 16:28:30] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-14 16:28:30] 
                       *** Training finished *** 
[2017-12-14 16:28:35] Dev MSE: 29120.9863
[2017-12-14 16:29:25] Training MSE: 28265.2832
[2017-12-14 16:29:25] Experiment lstm.hs_100.nl_1.lr_0.1.wd_0.001.rl_linear logging ended.
