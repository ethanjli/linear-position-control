[2017-12-15 01:02:33] Experiment lstm.hs_100.nl_2.lr_0.1.wd_0.001.rl_40 logging started.
[2017-12-15 01:02:33] 
                       *** Starting Experiment lstm.hs_100.nl_2.lr_0.1.wd_0.001.rl_40 ***
                      
[2017-12-15 01:02:33] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 100  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 300  
                      [               num_layers] 2  
                      [        regression_layers] [40]  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 01:02:36] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 100, num_layers=2, batch_first=True)
                        (linear1): Linear (100 -> 40)
                        (final): Linear (40 -> 1)
                      )
[2017-12-15 01:02:36]  *** Training on GPU ***
[2017-12-15 01:03:41] Epoch 0001 mean train/dev loss: 86949.2801 / 39691.8125
[2017-12-15 01:03:41] Checkpointing model...
[2017-12-15 01:03:42] Model Checkpointing finished.
[2017-12-15 01:04:47] Epoch 0002 mean train/dev loss: 15627.0669 / 7286.6333
[2017-12-15 01:04:47] Checkpointing model...
[2017-12-15 01:04:47] Model Checkpointing finished.
[2017-12-15 01:05:50] Epoch 0003 mean train/dev loss: 19727.6698 / 10842.8076
[2017-12-15 01:05:50] Checkpointing model...
[2017-12-15 01:05:50] Model Checkpointing finished.
[2017-12-15 01:06:53] Epoch 0004 mean train/dev loss: 26978.5280 / 41278.5703
[2017-12-15 01:06:53] Checkpointing model...
[2017-12-15 01:06:53] Model Checkpointing finished.
[2017-12-15 01:07:57] Epoch 0005 mean train/dev loss: 40842.3279 / 22482.1445
[2017-12-15 01:07:57] Checkpointing model...
[2017-12-15 01:07:57] Model Checkpointing finished.
[2017-12-15 01:09:01] Epoch 0006 mean train/dev loss: 19908.1226 / 12960.4658
[2017-12-15 01:10:06] Epoch 0007 mean train/dev loss: 8469.6431 / 8277.2988
[2017-12-15 01:11:07] Epoch 0008 mean train/dev loss: 6970.2437 / 6980.2485
[2017-12-15 01:12:11] Epoch 0009 mean train/dev loss: 225779.2075 / 326331.9062
[2017-12-15 01:13:15] Epoch 0010 mean train/dev loss: 316818.5666 / 314452.6875
[2017-12-15 01:13:15] Checkpointing model...
[2017-12-15 01:13:15] Model Checkpointing finished.
[2017-12-15 01:14:21] Epoch 0011 mean train/dev loss: 306854.3453 / 305503.1875
[2017-12-15 01:15:23] Epoch 0012 mean train/dev loss: 298546.3897 / 297845.7500
[2017-12-15 01:16:28] Epoch 0013 mean train/dev loss: 291667.3562 / 291042.3125
[2017-12-15 01:17:33] Epoch 0014 mean train/dev loss: 285414.8855 / 284771.1875
[2017-12-15 01:18:35] Epoch 0015 mean train/dev loss: 278939.6413 / 278971.1875
[2017-12-15 01:18:35] Learning rate decayed by 0.5000
[2017-12-15 01:18:35] Checkpointing model...
[2017-12-15 01:18:35] Model Checkpointing finished.
[2017-12-15 01:19:41] Epoch 0016 mean train/dev loss: 274930.4525 / 276203.9062
[2017-12-15 01:20:47] Epoch 0017 mean train/dev loss: 272431.4155 / 273548.6562
[2017-12-15 01:21:50] Epoch 0018 mean train/dev loss: 269619.4254 / 270977.5312
[2017-12-15 01:22:56] Epoch 0019 mean train/dev loss: 267161.2994 / 268482.0312
[2017-12-15 01:23:59] Epoch 0020 mean train/dev loss: 264171.7570 / 266064.1562
[2017-12-15 01:23:59] Checkpointing model...
[2017-12-15 01:24:00] Model Checkpointing finished.
[2017-12-15 01:25:06] Epoch 0021 mean train/dev loss: 262189.5169 / 263693.3125
[2017-12-15 01:26:14] Epoch 0022 mean train/dev loss: 259696.8052 / 261377.7031
[2017-12-15 01:27:21] Epoch 0023 mean train/dev loss: 257296.0373 / 259094.2656
[2017-12-15 01:28:24] Epoch 0024 mean train/dev loss: 255208.0555 / 256871.9531
[2017-12-15 01:29:27] Epoch 0025 mean train/dev loss: 253122.9742 / 254682.0156
[2017-12-15 01:29:27] Checkpointing model...
[2017-12-15 01:29:27] Model Checkpointing finished.
[2017-12-15 01:30:32] Epoch 0026 mean train/dev loss: 251045.9209 / 252531.0000
[2017-12-15 01:31:37] Epoch 0027 mean train/dev loss: 248651.6483 / 250399.2500
[2017-12-15 01:32:41] Epoch 0028 mean train/dev loss: 246520.3450 / 248318.0625
[2017-12-15 01:33:45] Epoch 0029 mean train/dev loss: 244463.4483 / 246258.1875
[2017-12-15 01:34:52] Epoch 0030 mean train/dev loss: 242616.2653 / 244223.6250
[2017-12-15 01:34:52] Learning rate decayed by 0.5000
[2017-12-15 01:34:52] Checkpointing model...
[2017-12-15 01:34:52] Model Checkpointing finished.
[2017-12-15 01:35:58] Epoch 0031 mean train/dev loss: 240885.5287 / 243219.9062
[2017-12-15 01:37:01] Epoch 0032 mean train/dev loss: 239769.0246 / 242217.0000
[2017-12-15 01:38:04] Epoch 0033 mean train/dev loss: 238596.4672 / 241226.9219
[2017-12-15 01:39:11] Epoch 0034 mean train/dev loss: 237884.2666 / 240244.3125
[2017-12-15 01:40:17] Epoch 0035 mean train/dev loss: 236857.1349 / 239268.2031
[2017-12-15 01:40:17] Checkpointing model...
[2017-12-15 01:40:17] Model Checkpointing finished.
[2017-12-15 01:41:19] Epoch 0036 mean train/dev loss: 236098.4566 / 238288.1250
[2017-12-15 01:42:25] Epoch 0037 mean train/dev loss: 235314.8715 / 237326.9219
[2017-12-15 01:43:30] Epoch 0038 mean train/dev loss: 234286.7210 / 236362.1562
[2017-12-15 01:44:34] Epoch 0039 mean train/dev loss: 233002.8192 / 235404.0781
[2017-12-15 01:45:39] Epoch 0040 mean train/dev loss: 231498.5529 / 234450.9531
[2017-12-15 01:45:39] Checkpointing model...
[2017-12-15 01:45:39] Model Checkpointing finished.
[2017-12-15 01:46:42] Epoch 0041 mean train/dev loss: 231132.3568 / 233508.8750
[2017-12-15 01:47:48] Epoch 0042 mean train/dev loss: 230545.2101 / 232565.2344
[2017-12-15 01:48:53] Epoch 0043 mean train/dev loss: 229621.6024 / 231627.5156
[2017-12-15 01:49:54] Epoch 0044 mean train/dev loss: 228502.5973 / 230693.1406
[2017-12-15 01:50:58] Epoch 0045 mean train/dev loss: 227797.2038 / 229761.6094
[2017-12-15 01:50:58] Learning rate decayed by 0.5000
[2017-12-15 01:50:58] Checkpointing model...
[2017-12-15 01:50:58] Model Checkpointing finished.
[2017-12-15 01:52:00] Epoch 0046 mean train/dev loss: 226819.1173 / 229297.9219
[2017-12-15 01:53:03] Epoch 0047 mean train/dev loss: 226092.6409 / 228837.1406
[2017-12-15 01:54:07] Epoch 0048 mean train/dev loss: 225945.7392 / 228377.8906
[2017-12-15 01:55:10] Epoch 0049 mean train/dev loss: 225507.6234 / 227917.4219
[2017-12-15 01:55:10] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 01:55:10] 
                       *** Training finished *** 
[2017-12-15 01:55:16] Dev MSE: 227917.4219
[2017-12-15 01:56:08] Training MSE: 225207.5000
[2017-12-15 01:56:08] Experiment lstm.hs_100.nl_2.lr_0.1.wd_0.001.rl_40 logging ended.
