[2017-12-14 16:30:16] Experiment lstm.hs_20.nl_1.lr_0.01.wd_0.001.rl_40_40 logging started.
[2017-12-14 16:30:16] 
                       *** Starting Experiment lstm.hs_20.nl_1.lr_0.01.wd_0.001.rl_40_40 ***
                      
[2017-12-14 16:30:16] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 20  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 300  
                      [               num_layers] 1  
                      [        regression_layers] [40, 40]  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-14 16:30:16] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 20, batch_first=True)
                        (linear1): Linear (20 -> 40)
                        (linear2): Linear (40 -> 40)
                        (final): Linear (40 -> 1)
                      )
[2017-12-14 16:30:16]  *** Training on GPU ***
[2017-12-14 16:31:20] Epoch 0001 mean train/dev loss: 185224.5235 / 68024.3672
[2017-12-14 16:31:20] Checkpointing model...
[2017-12-14 16:31:20] Model Checkpointing finished.
[2017-12-14 16:32:23] Epoch 0002 mean train/dev loss: 30636.1367 / 2668.7590
[2017-12-14 16:32:23] Checkpointing model...
[2017-12-14 16:32:24] Model Checkpointing finished.
[2017-12-14 16:33:25] Epoch 0003 mean train/dev loss: 1402.1023 / 789.3158
[2017-12-14 16:33:25] Checkpointing model...
[2017-12-14 16:33:26] Model Checkpointing finished.
[2017-12-14 16:34:28] Epoch 0004 mean train/dev loss: 623.4973 / 603.3167
[2017-12-14 16:34:28] Checkpointing model...
[2017-12-14 16:34:28] Model Checkpointing finished.
[2017-12-14 16:35:32] Epoch 0005 mean train/dev loss: 461.4000 / 457.0591
[2017-12-14 16:35:32] Checkpointing model...
[2017-12-14 16:35:33] Model Checkpointing finished.
[2017-12-14 16:36:32] Epoch 0006 mean train/dev loss: 381.0055 / 403.5287
[2017-12-14 16:37:34] Epoch 0007 mean train/dev loss: 327.3924 / 317.0338
[2017-12-14 16:38:37] Epoch 0008 mean train/dev loss: 303.4600 / 298.8623
[2017-12-14 16:39:37] Epoch 0009 mean train/dev loss: 291.7988 / 423.2318
[2017-12-14 16:40:39] Epoch 0010 mean train/dev loss: 276.1982 / 269.9547
[2017-12-14 16:40:39] Checkpointing model...
[2017-12-14 16:40:40] Model Checkpointing finished.
[2017-12-14 16:41:42] Epoch 0011 mean train/dev loss: 231.0433 / 259.3248
[2017-12-14 16:42:43] Epoch 0012 mean train/dev loss: 210.5539 / 249.0089
[2017-12-14 16:43:45] Epoch 0013 mean train/dev loss: 202.0933 / 246.4298
[2017-12-14 16:44:44] Epoch 0014 mean train/dev loss: 221.0033 / 244.8175
[2017-12-14 16:45:46] Epoch 0015 mean train/dev loss: 204.5776 / 227.0783
[2017-12-14 16:45:46] Learning rate decayed by 0.5000
[2017-12-14 16:45:46] Checkpointing model...
[2017-12-14 16:45:47] Model Checkpointing finished.
[2017-12-14 16:46:50] Epoch 0016 mean train/dev loss: 170.7096 / 200.0929
[2017-12-14 16:47:52] Epoch 0017 mean train/dev loss: 173.3030 / 272.3489
[2017-12-14 16:48:52] Epoch 0018 mean train/dev loss: 174.5835 / 201.0852
[2017-12-14 16:49:53] Epoch 0019 mean train/dev loss: 167.3226 / 198.9683
[2017-12-14 16:50:57] Epoch 0020 mean train/dev loss: 152.1174 / 194.6251
[2017-12-14 16:50:57] Checkpointing model...
[2017-12-14 16:50:58] Model Checkpointing finished.
[2017-12-14 16:51:58] Epoch 0021 mean train/dev loss: 153.1049 / 199.7876
[2017-12-14 16:52:59] Epoch 0022 mean train/dev loss: 159.1736 / 201.4473
[2017-12-14 16:54:04] Epoch 0023 mean train/dev loss: 159.6303 / 178.1585
[2017-12-14 16:55:04] Epoch 0024 mean train/dev loss: 140.0164 / 183.9880
[2017-12-14 16:56:07] Epoch 0025 mean train/dev loss: 138.3714 / 174.2906
[2017-12-14 16:56:07] Checkpointing model...
[2017-12-14 16:56:07] Model Checkpointing finished.
[2017-12-14 16:57:11] Epoch 0026 mean train/dev loss: 139.6257 / 170.9396
[2017-12-14 16:58:13] Epoch 0027 mean train/dev loss: 140.7076 / 163.7086
[2017-12-14 16:59:17] Epoch 0028 mean train/dev loss: 137.7833 / 181.2657
[2017-12-14 17:00:16] Epoch 0029 mean train/dev loss: 150.1217 / 162.7627
[2017-12-14 17:01:13] Epoch 0030 mean train/dev loss: 135.0709 / 176.6639
[2017-12-14 17:01:13] Learning rate decayed by 0.5000
[2017-12-14 17:01:13] Checkpointing model...
[2017-12-14 17:01:13] Model Checkpointing finished.
[2017-12-14 17:02:15] Epoch 0031 mean train/dev loss: 123.7616 / 153.6700
[2017-12-14 17:03:16] Epoch 0032 mean train/dev loss: 123.2543 / 156.6929
[2017-12-14 17:04:17] Epoch 0033 mean train/dev loss: 123.0467 / 166.9249
[2017-12-14 17:05:16] Epoch 0034 mean train/dev loss: 118.8703 / 154.8280
[2017-12-14 17:06:18] Epoch 0035 mean train/dev loss: 142.1382 / 153.5419
[2017-12-14 17:06:18] Checkpointing model...
[2017-12-14 17:06:18] Model Checkpointing finished.
[2017-12-14 17:07:14] Epoch 0036 mean train/dev loss: 118.7386 / 148.7101
[2017-12-14 17:08:15] Epoch 0037 mean train/dev loss: 121.2248 / 158.5679
[2017-12-14 17:09:17] Epoch 0038 mean train/dev loss: 116.5669 / 153.8659
[2017-12-14 17:10:16] Epoch 0039 mean train/dev loss: 118.7644 / 169.1450
[2017-12-14 17:11:19] Epoch 0040 mean train/dev loss: 118.8201 / 152.8472
[2017-12-14 17:11:19] Checkpointing model...
[2017-12-14 17:11:20] Model Checkpointing finished.
[2017-12-14 17:12:22] Epoch 0041 mean train/dev loss: 115.6087 / 158.2464
[2017-12-14 17:13:22] Epoch 0042 mean train/dev loss: 113.9458 / 160.1537
[2017-12-14 17:14:23] Epoch 0043 mean train/dev loss: 118.1116 / 170.8296
[2017-12-14 17:15:21] Epoch 0044 mean train/dev loss: 114.3872 / 142.6840
[2017-12-14 17:16:22] Epoch 0045 mean train/dev loss: 114.6160 / 153.5552
[2017-12-14 17:16:22] Learning rate decayed by 0.5000
[2017-12-14 17:16:22] Checkpointing model...
[2017-12-14 17:16:22] Model Checkpointing finished.
[2017-12-14 17:17:24] Epoch 0046 mean train/dev loss: 108.0034 / 142.8410
[2017-12-14 17:18:21] Epoch 0047 mean train/dev loss: 105.6441 / 146.5688
[2017-12-14 17:19:22] Epoch 0048 mean train/dev loss: 107.1790 / 149.6854
[2017-12-14 17:20:21] Epoch 0049 mean train/dev loss: 106.1188 / 139.4131
[2017-12-14 17:21:22] Epoch 0050 mean train/dev loss: 107.1034 / 149.5026
[2017-12-14 17:21:22] Checkpointing model...
[2017-12-14 17:21:22] Model Checkpointing finished.
[2017-12-14 17:22:24] Epoch 0051 mean train/dev loss: 106.9974 / 147.8082
[2017-12-14 17:23:24] Epoch 0052 mean train/dev loss: 110.5783 / 141.4884
[2017-12-14 17:24:24] Epoch 0053 mean train/dev loss: 104.5701 / 141.3839
[2017-12-14 17:25:26] Epoch 0054 mean train/dev loss: 105.5243 / 142.7765
[2017-12-14 17:26:26] Epoch 0055 mean train/dev loss: 102.2060 / 135.4987
[2017-12-14 17:26:26] Checkpointing model...
[2017-12-14 17:26:27] Model Checkpointing finished.
[2017-12-14 17:27:29] Epoch 0056 mean train/dev loss: 104.2732 / 143.9269
[2017-12-14 17:28:31] Epoch 0057 mean train/dev loss: 102.5112 / 150.4049
[2017-12-14 17:29:31] Epoch 0058 mean train/dev loss: 104.8514 / 133.9559
[2017-12-14 17:30:33] Epoch 0059 mean train/dev loss: 104.2048 / 145.2255
[2017-12-14 17:31:36] Epoch 0060 mean train/dev loss: 102.8921 / 142.8732
[2017-12-14 17:31:36] Learning rate decayed by 0.5000
[2017-12-14 17:31:36] Checkpointing model...
[2017-12-14 17:31:37] Model Checkpointing finished.
[2017-12-14 17:32:39] Epoch 0061 mean train/dev loss: 99.5042 / 143.5156
[2017-12-14 17:33:40] Epoch 0062 mean train/dev loss: 98.7428 / 133.6169
[2017-12-14 17:34:42] Epoch 0063 mean train/dev loss: 98.4819 / 146.5456
[2017-12-14 17:35:43] Epoch 0064 mean train/dev loss: 102.0916 / 141.8154
[2017-12-14 17:36:44] Epoch 0065 mean train/dev loss: 100.5842 / 134.0859
[2017-12-14 17:36:44] Checkpointing model...
[2017-12-14 17:36:45] Model Checkpointing finished.
[2017-12-14 17:37:47] Epoch 0066 mean train/dev loss: 98.6957 / 141.4306
[2017-12-14 17:38:46] Epoch 0067 mean train/dev loss: 97.7308 / 134.2228
[2017-12-14 17:39:47] Epoch 0068 mean train/dev loss: 98.0686 / 139.1664
[2017-12-14 17:40:49] Epoch 0069 mean train/dev loss: 98.4691 / 134.2471
[2017-12-14 17:41:54] Epoch 0070 mean train/dev loss: 98.7760 / 136.7385
[2017-12-14 17:41:54] Checkpointing model...
[2017-12-14 17:41:54] Model Checkpointing finished.
[2017-12-14 17:42:55] Epoch 0071 mean train/dev loss: 99.0346 / 133.9475
[2017-12-14 17:43:58] Epoch 0072 mean train/dev loss: 98.2804 / 137.4499
[2017-12-14 17:45:03] Epoch 0073 mean train/dev loss: 97.9721 / 140.7150
[2017-12-14 17:46:03] Epoch 0074 mean train/dev loss: 97.4855 / 136.6231
[2017-12-14 17:47:05] Epoch 0075 mean train/dev loss: 97.3261 / 137.8135
[2017-12-14 17:47:05] Learning rate decayed by 0.5000
[2017-12-14 17:47:05] Checkpointing model...
[2017-12-14 17:47:05] Model Checkpointing finished.
[2017-12-14 17:48:08] Epoch 0076 mean train/dev loss: 95.3257 / 141.4190
[2017-12-14 17:49:11] Epoch 0077 mean train/dev loss: 96.4227 / 136.3738
[2017-12-14 17:50:14] Epoch 0078 mean train/dev loss: 95.6995 / 134.2976
[2017-12-14 17:51:16] Epoch 0079 mean train/dev loss: 95.0659 / 137.7609
[2017-12-14 17:52:18] Epoch 0080 mean train/dev loss: 94.9767 / 142.6794
[2017-12-14 17:52:18] Checkpointing model...
[2017-12-14 17:52:18] Model Checkpointing finished.
[2017-12-14 17:53:22] Epoch 0081 mean train/dev loss: 95.2421 / 139.4305
[2017-12-14 17:54:23] Epoch 0082 mean train/dev loss: 95.0654 / 137.4056
[2017-12-14 17:55:25] Epoch 0083 mean train/dev loss: 95.0415 / 136.7331
[2017-12-14 17:56:26] Epoch 0084 mean train/dev loss: 94.3114 / 131.6973
[2017-12-14 17:57:25] Epoch 0085 mean train/dev loss: 94.4889 / 141.3316
[2017-12-14 17:57:25] Checkpointing model...
[2017-12-14 17:57:25] Model Checkpointing finished.
[2017-12-14 17:58:30] Epoch 0086 mean train/dev loss: 94.6659 / 139.4308
[2017-12-14 17:59:34] Epoch 0087 mean train/dev loss: 94.1511 / 135.2549
[2017-12-14 18:00:33] Epoch 0088 mean train/dev loss: 95.7032 / 132.9477
[2017-12-14 18:01:35] Epoch 0089 mean train/dev loss: 94.3868 / 134.7839
[2017-12-14 18:02:36] Epoch 0090 mean train/dev loss: 94.1131 / 137.2935
[2017-12-14 18:02:36] Learning rate decayed by 0.5000
[2017-12-14 18:02:36] Checkpointing model...
[2017-12-14 18:02:36] Model Checkpointing finished.
[2017-12-14 18:03:37] Epoch 0091 mean train/dev loss: 93.1509 / 136.5122
[2017-12-14 18:04:36] Epoch 0092 mean train/dev loss: 93.0539 / 136.9493
[2017-12-14 18:05:36] Epoch 0093 mean train/dev loss: 93.0098 / 136.4573
[2017-12-14 18:06:35] Epoch 0094 mean train/dev loss: 93.1994 / 140.1562
[2017-12-14 18:07:34] Epoch 0095 mean train/dev loss: 92.5780 / 134.0909
[2017-12-14 18:07:34] Checkpointing model...
[2017-12-14 18:07:35] Model Checkpointing finished.
[2017-12-14 18:08:36] Epoch 0096 mean train/dev loss: 93.1531 / 134.1257
[2017-12-14 18:09:36] Epoch 0097 mean train/dev loss: 92.9337 / 138.0515
[2017-12-14 18:10:35] Epoch 0098 mean train/dev loss: 92.7410 / 133.8542
[2017-12-14 18:11:37] Epoch 0099 mean train/dev loss: 93.2768 / 135.8232
[2017-12-14 18:12:41] Epoch 0100 mean train/dev loss: 92.8571 / 135.3198
[2017-12-14 18:12:41] Checkpointing model...
[2017-12-14 18:12:41] Model Checkpointing finished.
[2017-12-14 18:13:45] Epoch 0101 mean train/dev loss: 92.4792 / 133.9561
[2017-12-14 18:14:46] Epoch 0102 mean train/dev loss: 92.9897 / 139.0057
[2017-12-14 18:15:45] Epoch 0103 mean train/dev loss: 92.4121 / 137.7581
[2017-12-14 18:16:48] Epoch 0104 mean train/dev loss: 92.8765 / 137.5608
[2017-12-14 18:17:50] Epoch 0105 mean train/dev loss: 91.9298 / 138.6477
[2017-12-14 18:17:50] Learning rate decayed by 0.5000
[2017-12-14 18:17:50] Checkpointing model...
[2017-12-14 18:17:51] Model Checkpointing finished.
[2017-12-14 18:18:53] Epoch 0106 mean train/dev loss: 92.5420 / 133.8632
[2017-12-14 18:19:54] Epoch 0107 mean train/dev loss: 92.0566 / 136.0267
[2017-12-14 18:20:51] Epoch 0108 mean train/dev loss: 91.9072 / 136.7746
[2017-12-14 18:21:55] Epoch 0109 mean train/dev loss: 91.6598 / 136.3209
[2017-12-14 18:22:57] Epoch 0110 mean train/dev loss: 91.9932 / 138.5451
[2017-12-14 18:22:57] Checkpointing model...
[2017-12-14 18:22:57] Model Checkpointing finished.
[2017-12-14 18:23:58] Epoch 0111 mean train/dev loss: 91.7390 / 135.9663
[2017-12-14 18:24:59] Epoch 0112 mean train/dev loss: 91.4856 / 134.6842
[2017-12-14 18:26:02] Epoch 0113 mean train/dev loss: 91.8247 / 137.1470
[2017-12-14 18:27:04] Epoch 0114 mean train/dev loss: 91.7631 / 137.4548
[2017-12-14 18:28:06] Epoch 0115 mean train/dev loss: 91.5516 / 135.3780
[2017-12-14 18:28:06] Checkpointing model...
[2017-12-14 18:28:06] Model Checkpointing finished.
[2017-12-14 18:29:09] Epoch 0116 mean train/dev loss: 91.4705 / 140.2208
[2017-12-14 18:30:10] Epoch 0117 mean train/dev loss: 91.7995 / 136.1675
[2017-12-14 18:31:14] Epoch 0118 mean train/dev loss: 91.5733 / 135.9208
[2017-12-14 18:32:16] Epoch 0119 mean train/dev loss: 91.3357 / 137.0794
[2017-12-14 18:33:19] Epoch 0120 mean train/dev loss: 91.1387 / 139.3829
[2017-12-14 18:33:19] Learning rate decayed by 0.5000
[2017-12-14 18:33:19] Checkpointing model...
[2017-12-14 18:33:20] Model Checkpointing finished.
[2017-12-14 18:34:24] Epoch 0121 mean train/dev loss: 90.8570 / 136.0460
[2017-12-14 18:35:27] Epoch 0122 mean train/dev loss: 90.9067 / 137.1561
[2017-12-14 18:36:29] Epoch 0123 mean train/dev loss: 90.9822 / 138.1824
[2017-12-14 18:37:32] Epoch 0124 mean train/dev loss: 91.1009 / 136.7585
[2017-12-14 18:38:37] Epoch 0125 mean train/dev loss: 90.9577 / 137.4085
[2017-12-14 18:38:37] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-14 18:38:37] 
                       *** Training finished *** 
[2017-12-14 18:38:42] Dev MSE: 137.4085
[2017-12-14 18:39:35] Training MSE: 90.6925
[2017-12-14 18:39:37] Experiment lstm.hs_20.nl_1.lr_0.01.wd_0.001.rl_40_40 logging ended.
