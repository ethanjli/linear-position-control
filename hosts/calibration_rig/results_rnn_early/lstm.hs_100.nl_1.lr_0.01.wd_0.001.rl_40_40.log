[2017-12-14 20:29:18] Experiment lstm.hs_100.nl_1.lr_0.01.wd_0.001.rl_40_40 logging started.
[2017-12-14 20:29:18] 
                       *** Starting Experiment lstm.hs_100.nl_1.lr_0.01.wd_0.001.rl_40_40 ***
                      
[2017-12-14 20:29:18] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 100  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 300  
                      [               num_layers] 1  
                      [        regression_layers] [40, 40]  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-14 20:29:18] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 100, batch_first=True)
                        (linear1): Linear (100 -> 40)
                        (linear2): Linear (40 -> 40)
                        (final): Linear (40 -> 1)
                      )
[2017-12-14 20:29:18]  *** Training on GPU ***
[2017-12-14 20:30:21] Epoch 0001 mean train/dev loss: 131788.2891 / 63153.8789
[2017-12-14 20:30:21] Checkpointing model...
[2017-12-14 20:30:22] Model Checkpointing finished.
[2017-12-14 20:31:28] Epoch 0002 mean train/dev loss: 50492.2427 / 43594.5234
[2017-12-14 20:31:28] Checkpointing model...
[2017-12-14 20:31:28] Model Checkpointing finished.
[2017-12-14 20:32:32] Epoch 0003 mean train/dev loss: 40802.1237 / 36444.9023
[2017-12-14 20:32:32] Checkpointing model...
[2017-12-14 20:32:33] Model Checkpointing finished.
[2017-12-14 20:33:35] Epoch 0004 mean train/dev loss: 24192.0563 / 10474.1689
[2017-12-14 20:33:35] Checkpointing model...
[2017-12-14 20:33:35] Model Checkpointing finished.
[2017-12-14 20:34:39] Epoch 0005 mean train/dev loss: 7302.9839 / 11611.1172
[2017-12-14 20:34:39] Checkpointing model...
[2017-12-14 20:34:40] Model Checkpointing finished.
[2017-12-14 20:35:44] Epoch 0006 mean train/dev loss: 5589.4102 / 2724.8638
[2017-12-14 20:36:48] Epoch 0007 mean train/dev loss: 1811.4305 / 1631.1985
[2017-12-14 20:37:55] Epoch 0008 mean train/dev loss: 1125.8029 / 747.8152
[2017-12-14 20:39:01] Epoch 0009 mean train/dev loss: 845.0344 / 841.3713
[2017-12-14 20:40:05] Epoch 0010 mean train/dev loss: 720.6446 / 417.0681
[2017-12-14 20:40:05] Checkpointing model...
[2017-12-14 20:40:05] Model Checkpointing finished.
[2017-12-14 20:41:07] Epoch 0011 mean train/dev loss: 523.3757 / 647.1862
[2017-12-14 20:42:10] Epoch 0012 mean train/dev loss: 631.7290 / 413.8057
[2017-12-14 20:43:10] Epoch 0013 mean train/dev loss: 381.7665 / 415.5627
[2017-12-14 20:44:12] Epoch 0014 mean train/dev loss: 386.5862 / 358.5708
[2017-12-14 20:45:16] Epoch 0015 mean train/dev loss: 371.8074 / 411.1308
[2017-12-14 20:45:16] Learning rate decayed by 0.5000
[2017-12-14 20:45:16] Checkpointing model...
[2017-12-14 20:45:16] Model Checkpointing finished.
[2017-12-14 20:46:19] Epoch 0016 mean train/dev loss: 296.5348 / 278.3253
[2017-12-14 20:47:25] Epoch 0017 mean train/dev loss: 280.6398 / 288.7688
[2017-12-14 20:48:30] Epoch 0018 mean train/dev loss: 255.8734 / 293.9281
[2017-12-14 20:49:33] Epoch 0019 mean train/dev loss: 255.8154 / 356.8094
[2017-12-14 20:50:41] Epoch 0020 mean train/dev loss: 288.0094 / 322.4721
[2017-12-14 20:50:41] Checkpointing model...
[2017-12-14 20:50:41] Model Checkpointing finished.
[2017-12-14 20:51:42] Epoch 0021 mean train/dev loss: 294.5186 / 462.5421
[2017-12-14 20:52:48] Epoch 0022 mean train/dev loss: 313.0627 / 270.2815
[2017-12-14 20:53:50] Epoch 0023 mean train/dev loss: 248.2406 / 259.9555
[2017-12-14 20:54:51] Epoch 0024 mean train/dev loss: 243.9667 / 279.5856
[2017-12-14 20:55:56] Epoch 0025 mean train/dev loss: 310.0857 / 532.9808
[2017-12-14 20:55:56] Checkpointing model...
[2017-12-14 20:55:56] Model Checkpointing finished.
[2017-12-14 20:56:59] Epoch 0026 mean train/dev loss: 291.4761 / 256.8939
[2017-12-14 20:58:04] Epoch 0027 mean train/dev loss: 251.1612 / 283.7505
[2017-12-14 20:59:08] Epoch 0028 mean train/dev loss: 212.9953 / 314.7962
[2017-12-14 21:00:10] Epoch 0029 mean train/dev loss: 206.5577 / 239.0450
[2017-12-14 21:01:12] Epoch 0030 mean train/dev loss: 186.5489 / 218.8153
[2017-12-14 21:01:12] Learning rate decayed by 0.5000
[2017-12-14 21:01:12] Checkpointing model...
[2017-12-14 21:01:12] Model Checkpointing finished.
[2017-12-14 21:02:15] Epoch 0031 mean train/dev loss: 176.0284 / 212.9129
[2017-12-14 21:03:19] Epoch 0032 mean train/dev loss: 179.7799 / 208.0308
[2017-12-14 21:04:25] Epoch 0033 mean train/dev loss: 176.8218 / 212.0870
[2017-12-14 21:05:30] Epoch 0034 mean train/dev loss: 180.1972 / 217.1922
[2017-12-14 21:06:32] Epoch 0035 mean train/dev loss: 172.8407 / 217.1935
[2017-12-14 21:06:32] Checkpointing model...
[2017-12-14 21:06:32] Model Checkpointing finished.
[2017-12-14 21:07:33] Epoch 0036 mean train/dev loss: 183.2589 / 205.7556
[2017-12-14 21:08:33] Epoch 0037 mean train/dev loss: 172.1695 / 200.9820
[2017-12-14 21:09:34] Epoch 0038 mean train/dev loss: 175.4544 / 200.8475
[2017-12-14 21:10:37] Epoch 0039 mean train/dev loss: 178.1860 / 207.2036
[2017-12-14 21:11:42] Epoch 0040 mean train/dev loss: 168.2886 / 198.4572
[2017-12-14 21:11:42] Checkpointing model...
[2017-12-14 21:11:42] Model Checkpointing finished.
[2017-12-14 21:12:47] Epoch 0041 mean train/dev loss: 185.3987 / 209.7700
[2017-12-14 21:13:48] Epoch 0042 mean train/dev loss: 176.3222 / 213.2745
[2017-12-14 21:14:50] Epoch 0043 mean train/dev loss: 174.2052 / 218.2153
[2017-12-14 21:15:55] Epoch 0044 mean train/dev loss: 188.9781 / 293.3165
[2017-12-14 21:17:00] Epoch 0045 mean train/dev loss: 200.6861 / 248.4509
[2017-12-14 21:17:00] Learning rate decayed by 0.5000
[2017-12-14 21:17:00] Checkpointing model...
[2017-12-14 21:17:01] Model Checkpointing finished.
[2017-12-14 21:18:03] Epoch 0046 mean train/dev loss: 162.8848 / 190.7497
[2017-12-14 21:19:06] Epoch 0047 mean train/dev loss: 158.3888 / 193.0103
[2017-12-14 21:20:08] Epoch 0048 mean train/dev loss: 158.7848 / 199.7819
[2017-12-14 21:21:11] Epoch 0049 mean train/dev loss: 161.4392 / 205.2005
[2017-12-14 21:22:13] Epoch 0050 mean train/dev loss: 167.3003 / 189.3839
[2017-12-14 21:22:13] Checkpointing model...
[2017-12-14 21:22:13] Model Checkpointing finished.
[2017-12-14 21:23:16] Epoch 0051 mean train/dev loss: 168.3678 / 205.9444
[2017-12-14 21:24:18] Epoch 0052 mean train/dev loss: 158.8085 / 186.2754
[2017-12-14 21:25:17] Epoch 0053 mean train/dev loss: 153.4439 / 188.2091
[2017-12-14 21:26:20] Epoch 0054 mean train/dev loss: 151.8827 / 188.4308
[2017-12-14 21:27:21] Epoch 0055 mean train/dev loss: 152.4051 / 191.9692
[2017-12-14 21:27:21] Checkpointing model...
[2017-12-14 21:27:22] Model Checkpointing finished.
[2017-12-14 21:28:24] Epoch 0056 mean train/dev loss: 155.6037 / 180.3139
[2017-12-14 21:29:25] Epoch 0057 mean train/dev loss: 150.7070 / 196.7167
[2017-12-14 21:30:31] Epoch 0058 mean train/dev loss: 150.8383 / 182.2238
[2017-12-14 21:31:35] Epoch 0059 mean train/dev loss: 152.1722 / 190.8334
[2017-12-14 21:32:39] Epoch 0060 mean train/dev loss: 149.6837 / 188.6040
[2017-12-14 21:32:39] Learning rate decayed by 0.5000
[2017-12-14 21:32:39] Checkpointing model...
[2017-12-14 21:32:39] Model Checkpointing finished.
[2017-12-14 21:33:41] Epoch 0061 mean train/dev loss: 146.7978 / 176.1413
[2017-12-14 21:34:43] Epoch 0062 mean train/dev loss: 144.3638 / 183.0626
[2017-12-14 21:35:44] Epoch 0063 mean train/dev loss: 144.2742 / 183.4881
[2017-12-14 21:36:46] Epoch 0064 mean train/dev loss: 143.3444 / 175.9561
[2017-12-14 21:37:48] Epoch 0065 mean train/dev loss: 143.7363 / 179.9497
[2017-12-14 21:37:48] Checkpointing model...
[2017-12-14 21:37:48] Model Checkpointing finished.
[2017-12-14 21:38:50] Epoch 0066 mean train/dev loss: 144.9923 / 189.0486
[2017-12-14 21:39:54] Epoch 0067 mean train/dev loss: 143.9916 / 183.0936
[2017-12-14 21:40:58] Epoch 0068 mean train/dev loss: 142.0593 / 178.2338
[2017-12-14 21:42:00] Epoch 0069 mean train/dev loss: 141.0468 / 201.0938
[2017-12-14 21:43:03] Epoch 0070 mean train/dev loss: 144.1743 / 175.7190
[2017-12-14 21:43:03] Checkpointing model...
[2017-12-14 21:43:03] Model Checkpointing finished.
[2017-12-14 21:44:07] Epoch 0071 mean train/dev loss: 144.7511 / 178.5776
[2017-12-14 21:45:09] Epoch 0072 mean train/dev loss: 155.7945 / 194.8827
[2017-12-14 21:46:09] Epoch 0073 mean train/dev loss: 140.7635 / 177.9895
[2017-12-14 21:47:10] Epoch 0074 mean train/dev loss: 140.6843 / 169.8894
[2017-12-14 21:48:11] Epoch 0075 mean train/dev loss: 139.6959 / 169.9032
[2017-12-14 21:48:11] Learning rate decayed by 0.5000
[2017-12-14 21:48:11] Checkpointing model...
[2017-12-14 21:48:12] Model Checkpointing finished.
[2017-12-14 21:49:16] Epoch 0076 mean train/dev loss: 136.5470 / 171.2001
[2017-12-14 21:50:19] Epoch 0077 mean train/dev loss: 138.6697 / 169.6940
[2017-12-14 21:51:21] Epoch 0078 mean train/dev loss: 135.1499 / 170.3503
[2017-12-14 21:52:24] Epoch 0079 mean train/dev loss: 135.3992 / 168.7462
[2017-12-14 21:53:25] Epoch 0080 mean train/dev loss: 134.6570 / 172.0384
[2017-12-14 21:53:25] Checkpointing model...
[2017-12-14 21:53:26] Model Checkpointing finished.
[2017-12-14 21:54:26] Epoch 0081 mean train/dev loss: 135.3711 / 168.5042
[2017-12-14 21:55:31] Epoch 0082 mean train/dev loss: 143.9048 / 176.7465
[2017-12-14 21:56:37] Epoch 0083 mean train/dev loss: 135.0489 / 182.4777
[2017-12-14 21:57:43] Epoch 0084 mean train/dev loss: 145.2010 / 173.9014
[2017-12-14 21:58:45] Epoch 0085 mean train/dev loss: 135.0365 / 168.8722
[2017-12-14 21:58:45] Checkpointing model...
[2017-12-14 21:58:45] Model Checkpointing finished.
[2017-12-14 21:59:47] Epoch 0086 mean train/dev loss: 133.2564 / 168.3791
[2017-12-14 22:00:48] Epoch 0087 mean train/dev loss: 136.1220 / 169.6932
[2017-12-14 22:01:50] Epoch 0088 mean train/dev loss: 137.2533 / 168.2782
[2017-12-14 22:02:56] Epoch 0089 mean train/dev loss: 135.0069 / 172.4263
[2017-12-14 22:03:59] Epoch 0090 mean train/dev loss: 133.5736 / 165.8670
[2017-12-14 22:03:59] Learning rate decayed by 0.5000
[2017-12-14 22:03:59] Checkpointing model...
[2017-12-14 22:04:00] Model Checkpointing finished.
[2017-12-14 22:05:02] Epoch 0091 mean train/dev loss: 131.6599 / 170.4760
[2017-12-14 22:06:05] Epoch 0092 mean train/dev loss: 131.3666 / 166.0974
[2017-12-14 22:07:09] Epoch 0093 mean train/dev loss: 145.8337 / 173.0244
[2017-12-14 22:08:10] Epoch 0094 mean train/dev loss: 131.7731 / 168.8221
[2017-12-14 22:09:14] Epoch 0095 mean train/dev loss: 130.3187 / 172.6238
[2017-12-14 22:09:14] Checkpointing model...
[2017-12-14 22:09:14] Model Checkpointing finished.
[2017-12-14 22:10:18] Epoch 0096 mean train/dev loss: 130.6500 / 167.2141
[2017-12-14 22:11:23] Epoch 0097 mean train/dev loss: 130.9544 / 167.0202
[2017-12-14 22:12:28] Epoch 0098 mean train/dev loss: 130.0867 / 164.6512
[2017-12-14 22:13:31] Epoch 0099 mean train/dev loss: 145.0134 / 165.7685
[2017-12-14 22:14:36] Epoch 0100 mean train/dev loss: 130.5757 / 166.3539
[2017-12-14 22:14:36] Checkpointing model...
[2017-12-14 22:14:36] Model Checkpointing finished.
[2017-12-14 22:15:40] Epoch 0101 mean train/dev loss: 130.4015 / 166.5857
[2017-12-14 22:16:44] Epoch 0102 mean train/dev loss: 130.0973 / 164.4697
[2017-12-14 22:17:45] Epoch 0103 mean train/dev loss: 143.6637 / 166.9811
[2017-12-14 22:18:46] Epoch 0104 mean train/dev loss: 131.4314 / 164.6091
[2017-12-14 22:19:49] Epoch 0105 mean train/dev loss: 130.6381 / 168.2882
[2017-12-14 22:19:49] Learning rate decayed by 0.5000
[2017-12-14 22:19:49] Checkpointing model...
[2017-12-14 22:19:50] Model Checkpointing finished.
[2017-12-14 22:20:54] Epoch 0106 mean train/dev loss: 128.7531 / 164.0095
[2017-12-14 22:21:59] Epoch 0107 mean train/dev loss: 128.5491 / 171.0552
[2017-12-14 22:23:01] Epoch 0108 mean train/dev loss: 128.8209 / 165.9743
[2017-12-14 22:24:03] Epoch 0109 mean train/dev loss: 128.6223 / 164.9767
[2017-12-14 22:25:05] Epoch 0110 mean train/dev loss: 128.3715 / 170.3574
[2017-12-14 22:25:05] Checkpointing model...
[2017-12-14 22:25:06] Model Checkpointing finished.
[2017-12-14 22:26:08] Epoch 0111 mean train/dev loss: 128.7100 / 169.9621
[2017-12-14 22:27:12] Epoch 0112 mean train/dev loss: 128.5040 / 163.9576
[2017-12-14 22:28:10] Epoch 0113 mean train/dev loss: 127.8095 / 162.7208
[2017-12-14 22:29:03] Epoch 0114 mean train/dev loss: 137.3002 / 165.3089
[2017-12-14 22:29:58] Epoch 0115 mean train/dev loss: 127.7714 / 165.1453
[2017-12-14 22:29:58] Checkpointing model...
[2017-12-14 22:29:58] Model Checkpointing finished.
[2017-12-14 22:30:49] Epoch 0116 mean train/dev loss: 128.2294 / 167.2924
[2017-12-14 22:31:39] Epoch 0117 mean train/dev loss: 127.5680 / 166.6883
[2017-12-14 22:32:34] Epoch 0118 mean train/dev loss: 127.3222 / 167.5791
[2017-12-14 22:33:29] Epoch 0119 mean train/dev loss: 128.0593 / 168.4304
[2017-12-14 22:34:15] Epoch 0120 mean train/dev loss: 127.9144 / 167.0770
[2017-12-14 22:34:15] Learning rate decayed by 0.5000
[2017-12-14 22:34:15] Checkpointing model...
[2017-12-14 22:34:15] Model Checkpointing finished.
[2017-12-14 22:34:55] Epoch 0121 mean train/dev loss: 126.7492 / 162.8571
[2017-12-14 22:35:35] Epoch 0122 mean train/dev loss: 127.4189 / 165.7372
[2017-12-14 22:36:15] Epoch 0123 mean train/dev loss: 140.0854 / 163.8855
[2017-12-14 22:36:55] Epoch 0124 mean train/dev loss: 126.9720 / 166.5907
[2017-12-14 22:37:35] Epoch 0125 mean train/dev loss: 126.9694 / 165.5379
[2017-12-14 22:37:35] Checkpointing model...
[2017-12-14 22:37:36] Model Checkpointing finished.
[2017-12-14 22:38:16] Epoch 0126 mean train/dev loss: 126.8493 / 167.7334
[2017-12-14 22:38:56] Epoch 0127 mean train/dev loss: 126.9775 / 163.2115
[2017-12-14 22:39:36] Epoch 0128 mean train/dev loss: 127.2754 / 164.6012
[2017-12-14 22:40:16] Epoch 0129 mean train/dev loss: 127.0075 / 163.0896
[2017-12-14 22:40:55] Epoch 0130 mean train/dev loss: 126.7814 / 165.0876
[2017-12-14 22:40:55] Checkpointing model...
[2017-12-14 22:40:55] Model Checkpointing finished.
[2017-12-14 22:41:35] Epoch 0131 mean train/dev loss: 127.0923 / 162.6128
[2017-12-14 22:42:16] Epoch 0132 mean train/dev loss: 126.8014 / 164.3416
[2017-12-14 22:42:56] Epoch 0133 mean train/dev loss: 126.6527 / 165.3736
[2017-12-14 22:43:36] Epoch 0134 mean train/dev loss: 140.0343 / 163.4174
[2017-12-14 22:44:16] Epoch 0135 mean train/dev loss: 126.4105 / 163.3281
[2017-12-14 22:44:16] Learning rate decayed by 0.5000
[2017-12-14 22:44:16] Checkpointing model...
[2017-12-14 22:44:17] Model Checkpointing finished.
[2017-12-14 22:44:57] Epoch 0136 mean train/dev loss: 126.2387 / 164.7416
[2017-12-14 22:45:38] Epoch 0137 mean train/dev loss: 125.9898 / 165.2227
[2017-12-14 22:46:18] Epoch 0138 mean train/dev loss: 126.1534 / 163.7073
[2017-12-14 22:46:58] Epoch 0139 mean train/dev loss: 126.4472 / 164.6647
[2017-12-14 22:47:39] Epoch 0140 mean train/dev loss: 126.2258 / 161.8242
[2017-12-14 22:47:39] Checkpointing model...
[2017-12-14 22:47:39] Model Checkpointing finished.
[2017-12-14 22:48:19] Epoch 0141 mean train/dev loss: 134.7050 / 162.4534
[2017-12-14 22:49:00] Epoch 0142 mean train/dev loss: 125.9815 / 164.0627
[2017-12-14 22:49:40] Epoch 0143 mean train/dev loss: 126.1548 / 162.5497
[2017-12-14 22:50:21] Epoch 0144 mean train/dev loss: 125.7207 / 163.7986
[2017-12-14 22:51:01] Epoch 0145 mean train/dev loss: 126.0633 / 165.3889
[2017-12-14 22:51:01] Checkpointing model...
[2017-12-14 22:51:01] Model Checkpointing finished.
[2017-12-14 22:51:41] Epoch 0146 mean train/dev loss: 125.8923 / 162.0841
[2017-12-14 22:52:21] Epoch 0147 mean train/dev loss: 126.2428 / 163.3002
[2017-12-14 22:53:02] Epoch 0148 mean train/dev loss: 125.9468 / 164.1170
[2017-12-14 22:53:42] Epoch 0149 mean train/dev loss: 125.6668 / 164.2601
[2017-12-14 22:54:23] Epoch 0150 mean train/dev loss: 125.5369 / 162.1520
[2017-12-14 22:54:23] Learning rate decayed by 0.5000
[2017-12-14 22:54:23] Checkpointing model...
[2017-12-14 22:54:23] Model Checkpointing finished.
[2017-12-14 22:55:04] Epoch 0151 mean train/dev loss: 125.8842 / 163.5084
[2017-12-14 22:55:44] Epoch 0152 mean train/dev loss: 125.7455 / 164.1651
[2017-12-14 22:56:25] Epoch 0153 mean train/dev loss: 125.8057 / 164.3022
[2017-12-14 22:57:05] Epoch 0154 mean train/dev loss: 125.6516 / 162.7874
[2017-12-14 22:57:46] Epoch 0155 mean train/dev loss: 125.6202 / 164.0128
[2017-12-14 22:57:46] Checkpointing model...
[2017-12-14 22:57:46] Model Checkpointing finished.
[2017-12-14 22:58:27] Epoch 0156 mean train/dev loss: 126.1380 / 164.3261
[2017-12-14 22:59:07] Epoch 0157 mean train/dev loss: 125.5134 / 163.2846
[2017-12-14 22:59:48] Epoch 0158 mean train/dev loss: 125.9108 / 163.1237
[2017-12-14 23:00:28] Epoch 0159 mean train/dev loss: 125.4362 / 163.5897
[2017-12-14 23:01:09] Epoch 0160 mean train/dev loss: 125.4403 / 163.1689
[2017-12-14 23:01:09] Checkpointing model...
[2017-12-14 23:01:09] Model Checkpointing finished.
[2017-12-14 23:01:50] Epoch 0161 mean train/dev loss: 125.7901 / 163.0962
[2017-12-14 23:02:30] Epoch 0162 mean train/dev loss: 125.5209 / 163.6807
[2017-12-14 23:03:11] Epoch 0163 mean train/dev loss: 133.6646 / 162.9511
[2017-12-14 23:03:51] Epoch 0164 mean train/dev loss: 125.5211 / 164.1761
[2017-12-14 23:04:32] Epoch 0165 mean train/dev loss: 125.5731 / 162.9872
[2017-12-14 23:04:32] Learning rate decayed by 0.5000
[2017-12-14 23:04:32] Checkpointing model...
[2017-12-14 23:04:32] Model Checkpointing finished.
[2017-12-14 23:05:13] Epoch 0166 mean train/dev loss: 125.2711 / 163.9667
[2017-12-14 23:05:54] Epoch 0167 mean train/dev loss: 125.5345 / 162.8191
[2017-12-14 23:06:35] Epoch 0168 mean train/dev loss: 125.3298 / 163.3585
[2017-12-14 23:07:16] Epoch 0169 mean train/dev loss: 125.1645 / 163.0767
[2017-12-14 23:07:57] Epoch 0170 mean train/dev loss: 125.2183 / 163.5635
[2017-12-14 23:07:57] Checkpointing model...
[2017-12-14 23:07:57] Model Checkpointing finished.
[2017-12-14 23:08:38] Epoch 0171 mean train/dev loss: 125.0180 / 163.2842
[2017-12-14 23:09:19] Epoch 0172 mean train/dev loss: 125.1081 / 163.2417
[2017-12-14 23:09:59] Epoch 0173 mean train/dev loss: 125.4019 / 163.9496
[2017-12-14 23:10:40] Epoch 0174 mean train/dev loss: 125.3177 / 163.8161
[2017-12-14 23:11:21] Epoch 0175 mean train/dev loss: 125.1079 / 164.1064
[2017-12-14 23:11:21] Checkpointing model...
[2017-12-14 23:11:21] Model Checkpointing finished.
[2017-12-14 23:12:02] Epoch 0176 mean train/dev loss: 125.6823 / 163.3739
[2017-12-14 23:12:42] Epoch 0177 mean train/dev loss: 125.1765 / 163.4548
[2017-12-14 23:13:23] Epoch 0178 mean train/dev loss: 125.4037 / 163.4768
[2017-12-14 23:14:03] Epoch 0179 mean train/dev loss: 125.1563 / 163.4556
[2017-12-14 23:14:43] Epoch 0180 mean train/dev loss: 125.1384 / 163.6077
[2017-12-14 23:14:43] Learning rate decayed by 0.5000
[2017-12-14 23:14:43] Checkpointing model...
[2017-12-14 23:14:43] Model Checkpointing finished.
[2017-12-14 23:15:24] Epoch 0181 mean train/dev loss: 125.2718 / 163.3803
[2017-12-14 23:15:24] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-14 23:15:24] 
                       *** Training finished *** 
[2017-12-14 23:15:27] Dev MSE: 163.3803
[2017-12-14 23:16:01] Training MSE: 125.3216
[2017-12-14 23:16:02] Experiment lstm.hs_100.nl_1.lr_0.01.wd_0.001.rl_40_40 logging ended.
