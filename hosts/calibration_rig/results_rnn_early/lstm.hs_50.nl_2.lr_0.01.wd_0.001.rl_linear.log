[2017-12-15 02:50:30] Experiment lstm.hs_50.nl_2.lr_0.01.wd_0.001.rl_linear logging started.
[2017-12-15 02:50:30] 
                       *** Starting Experiment lstm.hs_50.nl_2.lr_0.01.wd_0.001.rl_linear ***
                      
[2017-12-15 02:50:30] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 50  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 300  
                      [               num_layers] 2  
                      [        regression_layers] None  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 02:50:33] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 50, num_layers=2, batch_first=True)
                        (final): Linear (50 -> 1)
                      )
[2017-12-15 02:50:33]  *** Training on GPU ***
[2017-12-15 02:51:33] Epoch 0001 mean train/dev loss: 318214.8450 / 307675.1875
[2017-12-15 02:51:33] Checkpointing model...
[2017-12-15 02:51:33] Model Checkpointing finished.
[2017-12-15 02:52:34] Epoch 0002 mean train/dev loss: 293509.5226 / 284547.2812
[2017-12-15 02:52:34] Checkpointing model...
[2017-12-15 02:52:34] Model Checkpointing finished.
[2017-12-15 02:53:38] Epoch 0003 mean train/dev loss: 271028.9630 / 263174.8125
[2017-12-15 02:53:38] Checkpointing model...
[2017-12-15 02:53:38] Model Checkpointing finished.
[2017-12-15 02:54:44] Epoch 0004 mean train/dev loss: 250797.1132 / 243898.3594
[2017-12-15 02:54:44] Checkpointing model...
[2017-12-15 02:54:45] Model Checkpointing finished.
[2017-12-15 02:55:49] Epoch 0005 mean train/dev loss: 232176.5609 / 226152.1719
[2017-12-15 02:55:49] Checkpointing model...
[2017-12-15 02:55:49] Model Checkpointing finished.
[2017-12-15 02:56:50] Epoch 0006 mean train/dev loss: 216128.7950 / 209695.3281
[2017-12-15 02:57:55] Epoch 0007 mean train/dev loss: 199861.4882 / 194657.3594
[2017-12-15 02:58:56] Epoch 0008 mean train/dev loss: 184866.7009 / 180607.3438
[2017-12-15 02:59:59] Epoch 0009 mean train/dev loss: 171910.6260 / 167849.8281
[2017-12-15 03:01:02] Epoch 0010 mean train/dev loss: 159524.7497 / 156106.5625
[2017-12-15 03:01:02] Checkpointing model...
[2017-12-15 03:01:03] Model Checkpointing finished.
[2017-12-15 03:02:05] Epoch 0011 mean train/dev loss: 148087.3665 / 145298.9844
[2017-12-15 03:03:09] Epoch 0012 mean train/dev loss: 137892.5319 / 135385.2656
[2017-12-15 03:04:14] Epoch 0013 mean train/dev loss: 128266.6148 / 125882.1953
[2017-12-15 03:05:16] Epoch 0014 mean train/dev loss: 118929.1939 / 116626.2344
[2017-12-15 03:06:19] Epoch 0015 mean train/dev loss: 109688.3481 / 108428.5312
[2017-12-15 03:06:19] Learning rate decayed by 0.5000
[2017-12-15 03:06:19] Checkpointing model...
[2017-12-15 03:06:19] Model Checkpointing finished.
[2017-12-15 03:07:21] Epoch 0016 mean train/dev loss: 102863.2333 / 102853.0703
[2017-12-15 03:08:25] Epoch 0017 mean train/dev loss: 98121.2679 / 98684.8125
[2017-12-15 03:09:26] Epoch 0018 mean train/dev loss: 94096.7275 / 94830.4922
[2017-12-15 03:10:28] Epoch 0019 mean train/dev loss: 90275.2382 / 91082.3594
[2017-12-15 03:11:29] Epoch 0020 mean train/dev loss: 86990.6129 / 87477.0000
[2017-12-15 03:11:29] Checkpointing model...
[2017-12-15 03:11:29] Model Checkpointing finished.
[2017-12-15 03:12:32] Epoch 0021 mean train/dev loss: 83476.6953 / 84022.9141
[2017-12-15 03:13:33] Epoch 0022 mean train/dev loss: 79915.0961 / 80636.5625
[2017-12-15 03:14:38] Epoch 0023 mean train/dev loss: 76645.6618 / 77462.7891
[2017-12-15 03:15:39] Epoch 0024 mean train/dev loss: 73592.3871 / 74382.0078
[2017-12-15 03:16:44] Epoch 0025 mean train/dev loss: 70583.9074 / 71341.8438
[2017-12-15 03:16:44] Checkpointing model...
[2017-12-15 03:16:45] Model Checkpointing finished.
[2017-12-15 03:17:45] Epoch 0026 mean train/dev loss: 67526.2957 / 68650.8906
[2017-12-15 03:18:46] Epoch 0027 mean train/dev loss: 64854.8151 / 65609.0000
[2017-12-15 03:19:48] Epoch 0028 mean train/dev loss: 62287.2582 / 62936.8984
[2017-12-15 03:20:49] Epoch 0029 mean train/dev loss: 59354.0914 / 60325.0039
[2017-12-15 03:21:50] Epoch 0030 mean train/dev loss: 57009.0914 / 57806.9297
[2017-12-15 03:21:50] Learning rate decayed by 0.5000
[2017-12-15 03:21:50] Checkpointing model...
[2017-12-15 03:21:50] Model Checkpointing finished.
[2017-12-15 03:22:52] Epoch 0031 mean train/dev loss: 55121.1753 / 56608.1445
[2017-12-15 03:23:54] Epoch 0032 mean train/dev loss: 53949.4985 / 55400.4062
[2017-12-15 03:24:53] Epoch 0033 mean train/dev loss: 52888.8645 / 54218.4023
[2017-12-15 03:25:57] Epoch 0034 mean train/dev loss: 51652.7591 / 53030.8906
[2017-12-15 03:27:00] Epoch 0035 mean train/dev loss: 50533.6134 / 51873.5781
[2017-12-15 03:27:00] Checkpointing model...
[2017-12-15 03:27:00] Model Checkpointing finished.
[2017-12-15 03:28:03] Epoch 0036 mean train/dev loss: 49433.4015 / 50702.2109
[2017-12-15 03:29:07] Epoch 0037 mean train/dev loss: 48236.4699 / 49583.2344
[2017-12-15 03:30:07] Epoch 0038 mean train/dev loss: 47135.6968 / 48457.8750
[2017-12-15 03:31:08] Epoch 0039 mean train/dev loss: 46080.9847 / 47360.9961
[2017-12-15 03:32:07] Epoch 0040 mean train/dev loss: 45000.1029 / 46288.5508
[2017-12-15 03:32:07] Checkpointing model...
[2017-12-15 03:32:07] Model Checkpointing finished.
[2017-12-15 03:33:12] Epoch 0041 mean train/dev loss: 43963.1517 / 45195.4922
[2017-12-15 03:34:13] Epoch 0042 mean train/dev loss: 42881.9835 / 44134.1836
[2017-12-15 03:35:15] Epoch 0043 mean train/dev loss: 41967.7992 / 43117.7188
[2017-12-15 03:36:17] Epoch 0044 mean train/dev loss: 40898.0994 / 42043.1914
[2017-12-15 03:37:18] Epoch 0045 mean train/dev loss: 39941.4985 / 40977.9219
[2017-12-15 03:37:18] Learning rate decayed by 0.5000
[2017-12-15 03:37:18] Checkpointing model...
[2017-12-15 03:37:18] Model Checkpointing finished.
[2017-12-15 03:38:19] Epoch 0046 mean train/dev loss: 39031.2818 / 40471.7695
[2017-12-15 03:39:24] Epoch 0047 mean train/dev loss: 38596.1670 / 39950.9023
[2017-12-15 03:40:24] Epoch 0048 mean train/dev loss: 37999.9092 / 39455.0273
[2017-12-15 03:41:28] Epoch 0049 mean train/dev loss: 37569.3329 / 38922.6719
[2017-12-15 03:42:29] Epoch 0050 mean train/dev loss: 37055.9495 / 38417.9648
[2017-12-15 03:42:29] Checkpointing model...
[2017-12-15 03:42:29] Model Checkpointing finished.
[2017-12-15 03:43:33] Epoch 0051 mean train/dev loss: 36634.1906 / 37910.8594
[2017-12-15 03:44:35] Epoch 0052 mean train/dev loss: 36023.4797 / 37407.5898
[2017-12-15 03:45:37] Epoch 0053 mean train/dev loss: 35679.7442 / 36910.2578
[2017-12-15 03:46:38] Epoch 0054 mean train/dev loss: 35055.7859 / 36409.6133
[2017-12-15 03:47:39] Epoch 0055 mean train/dev loss: 34607.1490 / 35885.1953
[2017-12-15 03:47:39] Checkpointing model...
[2017-12-15 03:47:39] Model Checkpointing finished.
[2017-12-15 03:48:39] Epoch 0056 mean train/dev loss: 34222.2324 / 35412.0312
[2017-12-15 03:49:43] Epoch 0057 mean train/dev loss: 33688.7428 / 34926.8086
[2017-12-15 03:50:44] Epoch 0058 mean train/dev loss: 33293.9516 / 34438.5977
[2017-12-15 03:51:45] Epoch 0059 mean train/dev loss: 32872.2348 / 33961.8672
[2017-12-15 03:52:48] Epoch 0060 mean train/dev loss: 32286.8072 / 33471.0547
[2017-12-15 03:52:48] Learning rate decayed by 0.5000
[2017-12-15 03:52:48] Checkpointing model...
[2017-12-15 03:52:49] Model Checkpointing finished.
[2017-12-15 03:53:50] Epoch 0061 mean train/dev loss: 31919.8060 / 33231.9492
[2017-12-15 03:54:51] Epoch 0062 mean train/dev loss: 31668.7905 / 33037.1406
[2017-12-15 03:55:54] Epoch 0063 mean train/dev loss: 31578.9438 / 32754.2109
[2017-12-15 03:56:52] Epoch 0064 mean train/dev loss: 31229.7758 / 32518.5000
[2017-12-15 03:57:56] Epoch 0065 mean train/dev loss: 31042.7069 / 32273.6816
[2017-12-15 03:57:56] Checkpointing model...
[2017-12-15 03:57:56] Model Checkpointing finished.
[2017-12-15 03:58:55] Epoch 0066 mean train/dev loss: 30822.6783 / 32031.8242
[2017-12-15 03:59:56] Epoch 0067 mean train/dev loss: 30543.1117 / 31783.0176
[2017-12-15 04:00:58] Epoch 0068 mean train/dev loss: 30280.6081 / 31548.0625
[2017-12-15 04:02:03] Epoch 0069 mean train/dev loss: 30145.2983 / 31302.5879
[2017-12-15 04:03:06] Epoch 0070 mean train/dev loss: 29803.5618 / 31059.7891
[2017-12-15 04:03:06] Checkpointing model...
[2017-12-15 04:03:06] Model Checkpointing finished.
[2017-12-15 04:04:07] Epoch 0071 mean train/dev loss: 29622.6835 / 30816.5879
[2017-12-15 04:05:12] Epoch 0072 mean train/dev loss: 29423.7649 / 30573.7754
[2017-12-15 04:06:16] Epoch 0073 mean train/dev loss: 29249.3104 / 30333.9590
[2017-12-15 04:07:19] Epoch 0074 mean train/dev loss: 28850.9044 / 30087.7480
[2017-12-15 04:08:19] Epoch 0075 mean train/dev loss: 28611.2714 / 29846.9668
[2017-12-15 04:08:19] Learning rate decayed by 0.5000
[2017-12-15 04:08:19] Checkpointing model...
[2017-12-15 04:08:20] Model Checkpointing finished.
[2017-12-15 04:09:20] Epoch 0076 mean train/dev loss: 28369.2500 / 29721.5566
[2017-12-15 04:10:21] Epoch 0077 mean train/dev loss: 28348.7388 / 29600.0156
[2017-12-15 04:11:23] Epoch 0078 mean train/dev loss: 28234.2636 / 29477.6875
[2017-12-15 04:12:23] Epoch 0079 mean train/dev loss: 28067.8900 / 29354.3633
[2017-12-15 04:13:23] Epoch 0080 mean train/dev loss: 27936.8988 / 29233.8613
[2017-12-15 04:13:23] Checkpointing model...
[2017-12-15 04:13:24] Model Checkpointing finished.
[2017-12-15 04:14:25] Epoch 0081 mean train/dev loss: 27853.4796 / 29110.2852
[2017-12-15 04:15:25] Epoch 0082 mean train/dev loss: 27694.3914 / 28982.2578
[2017-12-15 04:16:27] Epoch 0083 mean train/dev loss: 27611.3336 / 28859.1113
[2017-12-15 04:17:28] Epoch 0084 mean train/dev loss: 27486.9572 / 28732.5801
[2017-12-15 04:18:27] Epoch 0085 mean train/dev loss: 27410.5993 / 28608.3691
[2017-12-15 04:18:27] Checkpointing model...
[2017-12-15 04:18:28] Model Checkpointing finished.
[2017-12-15 04:19:27] Epoch 0086 mean train/dev loss: 27430.7825 / 28484.1836
[2017-12-15 04:20:28] Epoch 0087 mean train/dev loss: 27182.8626 / 28356.5625
[2017-12-15 04:21:28] Epoch 0088 mean train/dev loss: 27034.2047 / 28230.3828
[2017-12-15 04:22:27] Epoch 0089 mean train/dev loss: 26828.3542 / 28109.0469
[2017-12-15 04:23:27] Epoch 0090 mean train/dev loss: 26817.5755 / 27990.2441
[2017-12-15 04:23:27] Learning rate decayed by 0.5000
[2017-12-15 04:23:27] Checkpointing model...
[2017-12-15 04:23:27] Model Checkpointing finished.
[2017-12-15 04:24:25] Epoch 0091 mean train/dev loss: 26744.0014 / 27918.4766
[2017-12-15 04:25:25] Epoch 0092 mean train/dev loss: 26646.6835 / 27854.5332
[2017-12-15 04:26:24] Epoch 0093 mean train/dev loss: 26629.6200 / 27788.8516
[2017-12-15 04:27:22] Epoch 0094 mean train/dev loss: 26574.4920 / 27725.3164
[2017-12-15 04:28:25] Epoch 0095 mean train/dev loss: 26464.8456 / 27658.9297
[2017-12-15 04:28:25] Checkpointing model...
[2017-12-15 04:28:25] Model Checkpointing finished.
[2017-12-15 04:29:24] Epoch 0096 mean train/dev loss: 26378.0768 / 27595.1309
[2017-12-15 04:30:25] Epoch 0097 mean train/dev loss: 26325.0373 / 27534.3672
[2017-12-15 04:31:27] Epoch 0098 mean train/dev loss: 26292.4130 / 27467.8047
[2017-12-15 04:32:28] Epoch 0099 mean train/dev loss: 26258.4092 / 27398.9219
[2017-12-15 04:33:27] Epoch 0100 mean train/dev loss: 26283.3831 / 27337.0938
[2017-12-15 04:33:27] Checkpointing model...
[2017-12-15 04:33:27] Model Checkpointing finished.
[2017-12-15 04:34:27] Epoch 0101 mean train/dev loss: 26024.4910 / 27268.3984
[2017-12-15 04:35:27] Epoch 0102 mean train/dev loss: 26046.4052 / 27203.4023
[2017-12-15 04:36:27] Epoch 0103 mean train/dev loss: 26089.0761 / 27138.8613
[2017-12-15 04:37:27] Epoch 0104 mean train/dev loss: 25945.4900 / 27070.5898
[2017-12-15 04:38:28] Epoch 0105 mean train/dev loss: 25835.8805 / 27005.0020
[2017-12-15 04:38:28] Learning rate decayed by 0.5000
[2017-12-15 04:38:28] Checkpointing model...
[2017-12-15 04:38:28] Model Checkpointing finished.
[2017-12-15 04:39:26] Epoch 0106 mean train/dev loss: 25683.6564 / 26970.6797
[2017-12-15 04:40:25] Epoch 0107 mean train/dev loss: 25776.9200 / 26937.0703
[2017-12-15 04:41:23] Epoch 0108 mean train/dev loss: 25670.9762 / 26905.4258
[2017-12-15 04:42:23] Epoch 0109 mean train/dev loss: 25614.0222 / 26870.6289
[2017-12-15 04:43:23] Epoch 0110 mean train/dev loss: 25727.3147 / 26840.2500
[2017-12-15 04:43:23] Checkpointing model...
[2017-12-15 04:43:23] Model Checkpointing finished.
[2017-12-15 04:44:23] Epoch 0111 mean train/dev loss: 25597.3577 / 26804.7051
[2017-12-15 04:45:24] Epoch 0112 mean train/dev loss: 25546.8142 / 26770.3438
[2017-12-15 04:46:23] Epoch 0113 mean train/dev loss: 25497.9396 / 26737.0410
[2017-12-15 04:47:23] Epoch 0114 mean train/dev loss: 25542.8184 / 26703.2988
[2017-12-15 04:48:25] Epoch 0115 mean train/dev loss: 25509.6303 / 26669.1230
[2017-12-15 04:48:25] Checkpointing model...
[2017-12-15 04:48:25] Model Checkpointing finished.
[2017-12-15 04:49:26] Epoch 0116 mean train/dev loss: 25499.8151 / 26638.3906
[2017-12-15 04:50:29] Epoch 0117 mean train/dev loss: 25381.5539 / 26602.6504
[2017-12-15 04:51:30] Epoch 0118 mean train/dev loss: 25448.9199 / 26568.4668
[2017-12-15 04:52:32] Epoch 0119 mean train/dev loss: 25292.5535 / 26534.5547
[2017-12-15 04:53:34] Epoch 0120 mean train/dev loss: 25252.5438 / 26500.5137
[2017-12-15 04:53:34] Learning rate decayed by 0.5000
[2017-12-15 04:53:34] Checkpointing model...
[2017-12-15 04:53:34] Model Checkpointing finished.
[2017-12-15 04:54:34] Epoch 0121 mean train/dev loss: 25337.6751 / 26483.8301
[2017-12-15 04:55:34] Epoch 0122 mean train/dev loss: 25295.6724 / 26466.8828
[2017-12-15 04:56:32] Epoch 0123 mean train/dev loss: 25277.3361 / 26449.5000
[2017-12-15 04:57:30] Epoch 0124 mean train/dev loss: 25269.8648 / 26432.5488
[2017-12-15 04:58:32] Epoch 0125 mean train/dev loss: 25278.3891 / 26415.7617
[2017-12-15 04:58:32] Checkpointing model...
[2017-12-15 04:58:32] Model Checkpointing finished.
[2017-12-15 04:59:32] Epoch 0126 mean train/dev loss: 25150.4538 / 26398.2910
[2017-12-15 05:00:30] Epoch 0127 mean train/dev loss: 25254.7468 / 26380.4902
[2017-12-15 05:01:28] Epoch 0128 mean train/dev loss: 25165.1824 / 26363.9531
[2017-12-15 05:02:26] Epoch 0129 mean train/dev loss: 25253.0188 / 26346.2656
[2017-12-15 05:03:25] Epoch 0130 mean train/dev loss: 25229.9636 / 26329.5117
[2017-12-15 05:03:25] Checkpointing model...
[2017-12-15 05:03:26] Model Checkpointing finished.
[2017-12-15 05:04:24] Epoch 0131 mean train/dev loss: 25077.6335 / 26312.3047
[2017-12-15 05:05:23] Epoch 0132 mean train/dev loss: 25199.3643 / 26294.8691
[2017-12-15 05:06:24] Epoch 0133 mean train/dev loss: 25021.8685 / 26277.6562
[2017-12-15 05:07:24] Epoch 0134 mean train/dev loss: 25055.9300 / 26259.6328
[2017-12-15 05:08:24] Epoch 0135 mean train/dev loss: 25138.1930 / 26242.3125
[2017-12-15 05:08:24] Learning rate decayed by 0.5000
[2017-12-15 05:08:24] Checkpointing model...
[2017-12-15 05:08:24] Model Checkpointing finished.
[2017-12-15 05:09:24] Epoch 0136 mean train/dev loss: 25012.3246 / 26234.0977
[2017-12-15 05:10:26] Epoch 0137 mean train/dev loss: 25002.9288 / 26225.1016
[2017-12-15 05:11:24] Epoch 0138 mean train/dev loss: 25056.2516 / 26216.6230
[2017-12-15 05:12:23] Epoch 0139 mean train/dev loss: 24994.5751 / 26207.6465
[2017-12-15 05:13:23] Epoch 0140 mean train/dev loss: 24992.6003 / 26199.1387
[2017-12-15 05:13:23] Checkpointing model...
[2017-12-15 05:13:23] Model Checkpointing finished.
[2017-12-15 05:14:23] Epoch 0141 mean train/dev loss: 25009.7430 / 26190.2715
[2017-12-15 05:15:22] Epoch 0142 mean train/dev loss: 24980.5350 / 26181.8398
[2017-12-15 05:16:21] Epoch 0143 mean train/dev loss: 24992.1679 / 26173.0898
[2017-12-15 05:17:21] Epoch 0144 mean train/dev loss: 25023.8231 / 26164.3301
[2017-12-15 05:18:18] Epoch 0145 mean train/dev loss: 24931.5127 / 26155.2715
[2017-12-15 05:18:18] Checkpointing model...
[2017-12-15 05:18:19] Model Checkpointing finished.
[2017-12-15 05:19:17] Epoch 0146 mean train/dev loss: 24958.9699 / 26146.9668
[2017-12-15 05:20:15] Epoch 0147 mean train/dev loss: 24971.7617 / 26137.8066
[2017-12-15 05:21:12] Epoch 0148 mean train/dev loss: 25009.4360 / 26129.1836
[2017-12-15 05:22:17] Epoch 0149 mean train/dev loss: 25000.7737 / 26120.5020
[2017-12-15 05:23:15] Epoch 0150 mean train/dev loss: 24932.8457 / 26111.4785
[2017-12-15 05:23:15] Learning rate decayed by 0.5000
[2017-12-15 05:23:15] Checkpointing model...
[2017-12-15 05:23:15] Model Checkpointing finished.
[2017-12-15 05:24:15] Epoch 0151 mean train/dev loss: 24925.0916 / 26107.2305
[2017-12-15 05:25:14] Epoch 0152 mean train/dev loss: 24885.7209 / 26102.8887
[2017-12-15 05:26:13] Epoch 0153 mean train/dev loss: 24880.9361 / 26098.4648
[2017-12-15 05:27:12] Epoch 0154 mean train/dev loss: 25008.7539 / 26094.1855
[2017-12-15 05:28:12] Epoch 0155 mean train/dev loss: 24860.2674 / 26089.6934
[2017-12-15 05:28:12] Checkpointing model...
[2017-12-15 05:28:12] Model Checkpointing finished.
[2017-12-15 05:29:12] Epoch 0156 mean train/dev loss: 24981.4526 / 26085.1543
[2017-12-15 05:30:12] Epoch 0157 mean train/dev loss: 24881.9856 / 26080.8730
[2017-12-15 05:31:14] Epoch 0158 mean train/dev loss: 24882.9766 / 26076.4062
[2017-12-15 05:32:14] Epoch 0159 mean train/dev loss: 24911.8685 / 26071.9785
[2017-12-15 05:33:11] Epoch 0160 mean train/dev loss: 24848.8034 / 26067.6113
[2017-12-15 05:33:11] Checkpointing model...
[2017-12-15 05:33:11] Model Checkpointing finished.
[2017-12-15 05:34:06] Epoch 0161 mean train/dev loss: 24817.4238 / 26063.1836
[2017-12-15 05:35:05] Epoch 0162 mean train/dev loss: 24821.6849 / 26058.6660
[2017-12-15 05:36:04] Epoch 0163 mean train/dev loss: 24924.0694 / 26054.3301
[2017-12-15 05:37:06] Epoch 0164 mean train/dev loss: 24883.8727 / 26049.8066
[2017-12-15 05:38:05] Epoch 0165 mean train/dev loss: 24841.2894 / 26045.4180
[2017-12-15 05:38:05] Learning rate decayed by 0.5000
[2017-12-15 05:38:05] Checkpointing model...
[2017-12-15 05:38:05] Model Checkpointing finished.
[2017-12-15 05:39:04] Epoch 0166 mean train/dev loss: 24813.5758 / 26043.3555
[2017-12-15 05:40:05] Epoch 0167 mean train/dev loss: 24872.9632 / 26041.1250
[2017-12-15 05:41:05] Epoch 0168 mean train/dev loss: 24871.5570 / 26038.7793
[2017-12-15 05:42:04] Epoch 0169 mean train/dev loss: 24828.3063 / 26036.6191
[2017-12-15 05:43:04] Epoch 0170 mean train/dev loss: 24845.2300 / 26034.4629
[2017-12-15 05:43:04] Checkpointing model...
[2017-12-15 05:43:04] Model Checkpointing finished.
[2017-12-15 05:44:06] Epoch 0171 mean train/dev loss: 24823.2649 / 26032.2793
[2017-12-15 05:45:05] Epoch 0172 mean train/dev loss: 24887.7766 / 26030.0254
[2017-12-15 05:46:06] Epoch 0173 mean train/dev loss: 24802.3913 / 26027.8867
[2017-12-15 05:47:05] Epoch 0174 mean train/dev loss: 24762.3682 / 26025.5469
[2017-12-15 05:48:05] Epoch 0175 mean train/dev loss: 24920.4046 / 26023.3438
[2017-12-15 05:48:05] Checkpointing model...
[2017-12-15 05:48:05] Model Checkpointing finished.
[2017-12-15 05:49:04] Epoch 0176 mean train/dev loss: 24774.0767 / 26021.2285
[2017-12-15 05:50:03] Epoch 0177 mean train/dev loss: 24922.0585 / 26019.0605
[2017-12-15 05:51:02] Epoch 0178 mean train/dev loss: 24835.3590 / 26016.8145
[2017-12-15 05:52:03] Epoch 0179 mean train/dev loss: 24956.3099 / 26014.6172
[2017-12-15 05:53:02] Epoch 0180 mean train/dev loss: 24811.5776 / 26012.4199
[2017-12-15 05:53:02] Learning rate decayed by 0.5000
[2017-12-15 05:53:02] Checkpointing model...
[2017-12-15 05:53:02] Model Checkpointing finished.
[2017-12-15 05:54:03] Epoch 0181 mean train/dev loss: 24791.6304 / 26011.2598
[2017-12-15 05:55:01] Epoch 0182 mean train/dev loss: 24899.1529 / 26010.1133
[2017-12-15 05:55:56] Epoch 0183 mean train/dev loss: 24882.7640 / 26008.9414
[2017-12-15 05:56:56] Epoch 0184 mean train/dev loss: 24753.8267 / 26007.7480
[2017-12-15 05:57:53] Epoch 0185 mean train/dev loss: 24970.9650 / 26006.6074
[2017-12-15 05:57:53] Checkpointing model...
[2017-12-15 05:57:54] Model Checkpointing finished.
[2017-12-15 05:58:53] Epoch 0186 mean train/dev loss: 24910.3996 / 26005.3789
[2017-12-15 05:59:53] Epoch 0187 mean train/dev loss: 24883.9019 / 26004.2051
[2017-12-15 06:00:52] Epoch 0188 mean train/dev loss: 24790.2138 / 26003.0371
[2017-12-15 06:01:53] Epoch 0189 mean train/dev loss: 24851.7460 / 26001.8691
[2017-12-15 06:02:51] Epoch 0190 mean train/dev loss: 24791.1947 / 26000.7363
[2017-12-15 06:02:51] Checkpointing model...
[2017-12-15 06:02:51] Model Checkpointing finished.
[2017-12-15 06:03:50] Epoch 0191 mean train/dev loss: 24837.1620 / 25999.4629
[2017-12-15 06:04:52] Epoch 0192 mean train/dev loss: 24825.2956 / 25998.3457
[2017-12-15 06:05:53] Epoch 0193 mean train/dev loss: 24800.7535 / 25997.2402
[2017-12-15 06:06:53] Epoch 0194 mean train/dev loss: 24784.9578 / 25996.1016
[2017-12-15 06:07:56] Epoch 0195 mean train/dev loss: 24809.8603 / 25994.8945
[2017-12-15 06:07:56] Learning rate decayed by 0.5000
[2017-12-15 06:07:56] Checkpointing model...
[2017-12-15 06:07:56] Model Checkpointing finished.
[2017-12-15 06:08:59] Epoch 0196 mean train/dev loss: 24876.8672 / 25994.4336
[2017-12-15 06:10:04] Epoch 0197 mean train/dev loss: 24844.3727 / 25993.9922
[2017-12-15 06:11:05] Epoch 0198 mean train/dev loss: 24848.2769 / 25993.5391
[2017-12-15 06:12:05] Epoch 0199 mean train/dev loss: 24874.9700 / 25993.1152
[2017-12-15 06:13:06] Epoch 0200 mean train/dev loss: 24810.4501 / 25992.6855
[2017-12-15 06:13:06] Checkpointing model...
[2017-12-15 06:13:07] Model Checkpointing finished.
[2017-12-15 06:14:10] Epoch 0201 mean train/dev loss: 24815.1863 / 25992.2207
[2017-12-15 06:15:12] Epoch 0202 mean train/dev loss: 24809.8978 / 25991.7910
[2017-12-15 06:16:14] Epoch 0203 mean train/dev loss: 24753.7041 / 25991.3770
[2017-12-15 06:17:16] Epoch 0204 mean train/dev loss: 24795.7176 / 25990.9219
[2017-12-15 06:18:17] Epoch 0205 mean train/dev loss: 24827.5156 / 25990.4570
[2017-12-15 06:18:17] Checkpointing model...
[2017-12-15 06:18:18] Model Checkpointing finished.
[2017-12-15 06:19:17] Epoch 0206 mean train/dev loss: 24876.8020 / 25990.0391
[2017-12-15 06:20:21] Epoch 0207 mean train/dev loss: 24837.0283 / 25989.5156
[2017-12-15 06:21:23] Epoch 0208 mean train/dev loss: 24747.7419 / 25989.0625
[2017-12-15 06:22:23] Epoch 0209 mean train/dev loss: 24828.4879 / 25988.6660
[2017-12-15 06:23:23] Epoch 0210 mean train/dev loss: 24867.1247 / 25988.1953
[2017-12-15 06:23:23] Learning rate decayed by 0.5000
[2017-12-15 06:23:23] Checkpointing model...
[2017-12-15 06:23:23] Model Checkpointing finished.
[2017-12-15 06:24:29] Epoch 0211 mean train/dev loss: 24766.8073 / 25987.7617
[2017-12-15 06:25:29] Epoch 0212 mean train/dev loss: 24949.3262 / 25987.3340
[2017-12-15 06:26:32] Epoch 0213 mean train/dev loss: 24743.7726 / 25986.8906
[2017-12-15 06:27:36] Epoch 0214 mean train/dev loss: 24809.2184 / 25986.4629
[2017-12-15 06:28:42] Epoch 0215 mean train/dev loss: 24826.3064 / 25986.0078
[2017-12-15 06:28:42] Checkpointing model...
[2017-12-15 06:28:42] Model Checkpointing finished.
[2017-12-15 06:29:41] Epoch 0216 mean train/dev loss: 24807.7132 / 25985.5977
[2017-12-15 06:30:46] Epoch 0217 mean train/dev loss: 24784.7362 / 25985.1562
[2017-12-15 06:31:47] Epoch 0218 mean train/dev loss: 24773.6165 / 25984.7090
[2017-12-15 06:32:50] Epoch 0219 mean train/dev loss: 24814.4750 / 25984.2832
[2017-12-15 06:33:51] Epoch 0220 mean train/dev loss: 24747.5516 / 25983.8418
[2017-12-15 06:33:51] Checkpointing model...
[2017-12-15 06:33:51] Model Checkpointing finished.
[2017-12-15 06:34:53] Epoch 0221 mean train/dev loss: 24755.9597 / 25983.4102
[2017-12-15 06:35:57] Epoch 0222 mean train/dev loss: 24785.1052 / 25982.9629
[2017-12-15 06:36:54] Epoch 0223 mean train/dev loss: 24820.0230 / 25982.5273
[2017-12-15 06:37:50] Epoch 0224 mean train/dev loss: 24683.8670 / 25982.0762
[2017-12-15 06:38:49] Epoch 0225 mean train/dev loss: 24842.3458 / 25981.6445
[2017-12-15 06:38:49] Learning rate decayed by 0.5000
[2017-12-15 06:38:49] Checkpointing model...
[2017-12-15 06:38:50] Model Checkpointing finished.
[2017-12-15 06:39:48] Epoch 0226 mean train/dev loss: 24825.8663 / 25981.6465
[2017-12-15 06:40:48] Epoch 0227 mean train/dev loss: 24792.0594 / 25981.6445
[2017-12-15 06:41:46] Epoch 0228 mean train/dev loss: 24750.1085 / 25981.6465
[2017-12-15 06:42:49] Epoch 0229 mean train/dev loss: 24821.8611 / 25981.6465
[2017-12-15 06:43:48] Epoch 0230 mean train/dev loss: 24820.6707 / 25981.6445
[2017-12-15 06:43:48] Checkpointing model...
[2017-12-15 06:43:49] Model Checkpointing finished.
[2017-12-15 06:44:49] Epoch 0231 mean train/dev loss: 24815.2303 / 25981.6484
[2017-12-15 06:45:49] Epoch 0232 mean train/dev loss: 24826.5472 / 25981.6445
[2017-12-15 06:46:50] Epoch 0233 mean train/dev loss: 24751.0071 / 25981.6465
[2017-12-15 06:47:50] Epoch 0234 mean train/dev loss: 24742.9835 / 25981.6445
[2017-12-15 06:48:50] Epoch 0235 mean train/dev loss: 24750.9689 / 25981.6484
[2017-12-15 06:48:50] Checkpointing model...
[2017-12-15 06:48:50] Model Checkpointing finished.
[2017-12-15 06:49:49] Epoch 0236 mean train/dev loss: 24885.2453 / 25981.6484
[2017-12-15 06:50:52] Epoch 0237 mean train/dev loss: 24833.8837 / 25981.6445
[2017-12-15 06:51:52] Epoch 0238 mean train/dev loss: 24817.4793 / 25981.6504
[2017-12-15 06:52:55] Epoch 0239 mean train/dev loss: 24786.2301 / 25981.6523
[2017-12-15 06:53:56] Epoch 0240 mean train/dev loss: 24799.3535 / 25981.6523
[2017-12-15 06:53:56] Learning rate decayed by 0.5000
[2017-12-15 06:53:56] Checkpointing model...
[2017-12-15 06:53:57] Model Checkpointing finished.
[2017-12-15 06:54:55] Epoch 0241 mean train/dev loss: 24779.1798 / 25981.6543
[2017-12-15 06:55:56] Epoch 0242 mean train/dev loss: 24867.5303 / 25981.6543
[2017-12-15 06:56:56] Epoch 0243 mean train/dev loss: 24786.6107 / 25981.6523
[2017-12-15 06:57:58] Epoch 0244 mean train/dev loss: 24836.0808 / 25981.6523
[2017-12-15 06:58:56] Epoch 0245 mean train/dev loss: 24821.5029 / 25981.6562
[2017-12-15 06:58:56] Checkpointing model...
[2017-12-15 06:58:56] Model Checkpointing finished.
[2017-12-15 06:59:56] Epoch 0246 mean train/dev loss: 24771.4672 / 25981.6562
[2017-12-15 07:00:54] Epoch 0247 mean train/dev loss: 24790.5001 / 25981.6523
[2017-12-15 07:01:57] Epoch 0248 mean train/dev loss: 24724.7259 / 25981.6543
[2017-12-15 07:02:55] Epoch 0249 mean train/dev loss: 24799.3381 / 25981.6543
[2017-12-15 07:03:55] Epoch 0250 mean train/dev loss: 24816.4188 / 25981.6523
[2017-12-15 07:03:55] Checkpointing model...
[2017-12-15 07:03:56] Model Checkpointing finished.
[2017-12-15 07:04:54] Epoch 0251 mean train/dev loss: 24800.8538 / 25981.6543
[2017-12-15 07:05:55] Epoch 0252 mean train/dev loss: 24769.0880 / 25981.6582
[2017-12-15 07:06:54] Epoch 0253 mean train/dev loss: 24806.6533 / 25981.6523
[2017-12-15 07:07:52] Epoch 0254 mean train/dev loss: 24832.4854 / 25981.6523
[2017-12-15 07:08:52] Epoch 0255 mean train/dev loss: 24850.3270 / 25981.6582
[2017-12-15 07:08:52] Learning rate decayed by 0.5000
[2017-12-15 07:08:52] Checkpointing model...
[2017-12-15 07:08:53] Model Checkpointing finished.
[2017-12-15 07:09:54] Epoch 0256 mean train/dev loss: 24705.1421 / 25981.6582
[2017-12-15 07:10:53] Epoch 0257 mean train/dev loss: 24779.1817 / 25981.6582
[2017-12-15 07:11:55] Epoch 0258 mean train/dev loss: 24741.0759 / 25981.6582
[2017-12-15 07:12:55] Epoch 0259 mean train/dev loss: 24879.9845 / 25981.6582
[2017-12-15 07:13:56] Epoch 0260 mean train/dev loss: 24911.3001 / 25981.6582
[2017-12-15 07:13:56] Checkpointing model...
[2017-12-15 07:13:56] Model Checkpointing finished.
[2017-12-15 07:14:56] Epoch 0261 mean train/dev loss: 24751.5779 / 25981.6582
[2017-12-15 07:15:55] Epoch 0262 mean train/dev loss: 24752.7664 / 25981.6602
[2017-12-15 07:16:56] Epoch 0263 mean train/dev loss: 24770.9039 / 25981.6582
[2017-12-15 07:17:58] Epoch 0264 mean train/dev loss: 24816.6288 / 25981.6602
[2017-12-15 07:18:58] Epoch 0265 mean train/dev loss: 24907.9438 / 25981.6602
[2017-12-15 07:18:58] Checkpointing model...
[2017-12-15 07:18:58] Model Checkpointing finished.
[2017-12-15 07:19:59] Epoch 0266 mean train/dev loss: 24884.4322 / 25981.6582
[2017-12-15 07:19:59] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 07:19:59] 
                       *** Training finished *** 
[2017-12-15 07:20:05] Dev MSE: 25981.6582
[2017-12-15 07:20:55] Training MSE: 24807.3887
[2017-12-15 07:20:57] Experiment lstm.hs_50.nl_2.lr_0.01.wd_0.001.rl_linear logging ended.
