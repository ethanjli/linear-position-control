[2017-12-14 18:39:51] Experiment lstm.hs_50.nl_1.lr_0.01.wd_0.001.rl_40_40 logging started.
[2017-12-14 18:39:51] 
                       *** Starting Experiment lstm.hs_50.nl_1.lr_0.01.wd_0.001.rl_40_40 ***
                      
[2017-12-14 18:39:51] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 50  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 300  
                      [               num_layers] 1  
                      [        regression_layers] [40, 40]  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-14 18:39:51] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 50, batch_first=True)
                        (linear1): Linear (50 -> 40)
                        (linear2): Linear (40 -> 40)
                        (final): Linear (40 -> 1)
                      )
[2017-12-14 18:39:51]  *** Training on GPU ***
[2017-12-14 18:40:55] Epoch 0001 mean train/dev loss: 131415.6688 / 48771.1367
[2017-12-14 18:40:55] Checkpointing model...
[2017-12-14 18:40:55] Model Checkpointing finished.
[2017-12-14 18:41:57] Epoch 0002 mean train/dev loss: 44597.4536 / 33253.8320
[2017-12-14 18:41:57] Checkpointing model...
[2017-12-14 18:41:58] Model Checkpointing finished.
[2017-12-14 18:43:00] Epoch 0003 mean train/dev loss: 15483.9029 / 1630.0089
[2017-12-14 18:43:00] Checkpointing model...
[2017-12-14 18:43:00] Model Checkpointing finished.
[2017-12-14 18:44:02] Epoch 0004 mean train/dev loss: 1040.7377 / 793.3392
[2017-12-14 18:44:02] Checkpointing model...
[2017-12-14 18:44:03] Model Checkpointing finished.
[2017-12-14 18:45:04] Epoch 0005 mean train/dev loss: 670.7645 / 757.7232
[2017-12-14 18:45:04] Checkpointing model...
[2017-12-14 18:45:04] Model Checkpointing finished.
[2017-12-14 18:46:07] Epoch 0006 mean train/dev loss: 602.4598 / 722.7193
[2017-12-14 18:47:11] Epoch 0007 mean train/dev loss: 539.0480 / 733.6940
[2017-12-14 18:48:13] Epoch 0008 mean train/dev loss: 470.6759 / 647.7210
[2017-12-14 18:49:13] Epoch 0009 mean train/dev loss: 404.4533 / 589.5984
[2017-12-14 18:50:17] Epoch 0010 mean train/dev loss: 412.2495 / 780.5624
[2017-12-14 18:50:17] Checkpointing model...
[2017-12-14 18:50:17] Model Checkpointing finished.
[2017-12-14 18:51:20] Epoch 0011 mean train/dev loss: 363.6435 / 490.5340
[2017-12-14 18:52:21] Epoch 0012 mean train/dev loss: 337.7135 / 536.6144
[2017-12-14 18:53:21] Epoch 0013 mean train/dev loss: 445.2759 / 375.1595
[2017-12-14 18:54:20] Epoch 0014 mean train/dev loss: 245.6218 / 271.3679
[2017-12-14 18:55:24] Epoch 0015 mean train/dev loss: 217.0226 / 302.7463
[2017-12-14 18:55:24] Learning rate decayed by 0.5000
[2017-12-14 18:55:24] Checkpointing model...
[2017-12-14 18:55:24] Model Checkpointing finished.
[2017-12-14 18:56:25] Epoch 0016 mean train/dev loss: 196.2150 / 259.1317
[2017-12-14 18:57:28] Epoch 0017 mean train/dev loss: 171.6861 / 241.2018
[2017-12-14 18:58:28] Epoch 0018 mean train/dev loss: 168.1592 / 280.2423
[2017-12-14 18:59:33] Epoch 0019 mean train/dev loss: 157.6119 / 255.9382
[2017-12-14 19:00:33] Epoch 0020 mean train/dev loss: 151.2783 / 254.1433
[2017-12-14 19:00:33] Checkpointing model...
[2017-12-14 19:00:34] Model Checkpointing finished.
[2017-12-14 19:01:33] Epoch 0021 mean train/dev loss: 148.7028 / 228.6774
[2017-12-14 19:02:34] Epoch 0022 mean train/dev loss: 137.4729 / 256.7200
[2017-12-14 19:03:33] Epoch 0023 mean train/dev loss: 145.9572 / 220.8002
[2017-12-14 19:04:34] Epoch 0024 mean train/dev loss: 137.1765 / 209.4375
[2017-12-14 19:05:39] Epoch 0025 mean train/dev loss: 142.4991 / 210.7576
[2017-12-14 19:05:39] Checkpointing model...
[2017-12-14 19:05:39] Model Checkpointing finished.
[2017-12-14 19:06:42] Epoch 0026 mean train/dev loss: 132.3383 / 206.3696
[2017-12-14 19:07:43] Epoch 0027 mean train/dev loss: 139.8344 / 201.5508
[2017-12-14 19:08:44] Epoch 0028 mean train/dev loss: 126.8934 / 196.1124
[2017-12-14 19:09:46] Epoch 0029 mean train/dev loss: 128.9249 / 225.6184
[2017-12-14 19:10:50] Epoch 0030 mean train/dev loss: 144.2376 / 232.0437
[2017-12-14 19:10:50] Learning rate decayed by 0.5000
[2017-12-14 19:10:50] Checkpointing model...
[2017-12-14 19:10:51] Model Checkpointing finished.
[2017-12-14 19:11:54] Epoch 0031 mean train/dev loss: 113.9560 / 208.4064
[2017-12-14 19:12:56] Epoch 0032 mean train/dev loss: 107.4820 / 215.0396
[2017-12-14 19:13:59] Epoch 0033 mean train/dev loss: 108.5120 / 201.2459
[2017-12-14 19:15:01] Epoch 0034 mean train/dev loss: 111.8002 / 201.9646
[2017-12-14 19:16:11] Epoch 0035 mean train/dev loss: 111.6930 / 206.5322
[2017-12-14 19:16:11] Checkpointing model...
[2017-12-14 19:16:11] Model Checkpointing finished.
[2017-12-14 19:17:20] Epoch 0036 mean train/dev loss: 109.9344 / 192.7385
[2017-12-14 19:18:31] Epoch 0037 mean train/dev loss: 103.3300 / 213.3003
[2017-12-14 19:19:38] Epoch 0038 mean train/dev loss: 103.8178 / 202.0078
[2017-12-14 19:20:46] Epoch 0039 mean train/dev loss: 102.8629 / 215.2504
[2017-12-14 19:21:50] Epoch 0040 mean train/dev loss: 102.5803 / 191.8203
[2017-12-14 19:21:50] Checkpointing model...
[2017-12-14 19:21:51] Model Checkpointing finished.
[2017-12-14 19:22:56] Epoch 0041 mean train/dev loss: 101.9349 / 207.6613
[2017-12-14 19:24:06] Epoch 0042 mean train/dev loss: 100.8128 / 172.7875
[2017-12-14 19:25:15] Epoch 0043 mean train/dev loss: 100.2221 / 169.7023
[2017-12-14 19:26:21] Epoch 0044 mean train/dev loss: 110.4952 / 184.4351
[2017-12-14 19:27:26] Epoch 0045 mean train/dev loss: 103.1604 / 165.8438
[2017-12-14 19:27:26] Learning rate decayed by 0.5000
[2017-12-14 19:27:26] Checkpointing model...
[2017-12-14 19:27:26] Model Checkpointing finished.
[2017-12-14 19:28:31] Epoch 0046 mean train/dev loss: 97.4675 / 179.3266
[2017-12-14 19:29:37] Epoch 0047 mean train/dev loss: 98.3754 / 165.8140
[2017-12-14 19:30:41] Epoch 0048 mean train/dev loss: 97.2252 / 169.7945
[2017-12-14 19:31:49] Epoch 0049 mean train/dev loss: 94.8291 / 161.8060
[2017-12-14 19:32:53] Epoch 0050 mean train/dev loss: 96.1517 / 163.3649
[2017-12-14 19:32:53] Checkpointing model...
[2017-12-14 19:32:53] Model Checkpointing finished.
[2017-12-14 19:34:00] Epoch 0051 mean train/dev loss: 95.7164 / 158.4406
[2017-12-14 19:35:06] Epoch 0052 mean train/dev loss: 97.3932 / 162.5985
[2017-12-14 19:36:13] Epoch 0053 mean train/dev loss: 92.6071 / 166.4478
[2017-12-14 19:37:18] Epoch 0054 mean train/dev loss: 94.0693 / 165.3283
[2017-12-14 19:38:21] Epoch 0055 mean train/dev loss: 95.3087 / 159.6055
[2017-12-14 19:38:21] Checkpointing model...
[2017-12-14 19:38:21] Model Checkpointing finished.
[2017-12-14 19:39:25] Epoch 0056 mean train/dev loss: 92.1854 / 158.8619
[2017-12-14 19:40:27] Epoch 0057 mean train/dev loss: 93.4370 / 164.1986
[2017-12-14 19:41:32] Epoch 0058 mean train/dev loss: 93.4229 / 156.9561
[2017-12-14 19:42:36] Epoch 0059 mean train/dev loss: 93.0911 / 173.5399
[2017-12-14 19:43:35] Epoch 0060 mean train/dev loss: 92.9848 / 161.4452
[2017-12-14 19:43:35] Learning rate decayed by 0.5000
[2017-12-14 19:43:35] Checkpointing model...
[2017-12-14 19:43:35] Model Checkpointing finished.
[2017-12-14 19:44:36] Epoch 0061 mean train/dev loss: 90.3201 / 163.8649
[2017-12-14 19:45:38] Epoch 0062 mean train/dev loss: 90.0142 / 154.5952
[2017-12-14 19:46:40] Epoch 0063 mean train/dev loss: 90.1853 / 156.8071
[2017-12-14 19:47:42] Epoch 0064 mean train/dev loss: 90.0316 / 169.8794
[2017-12-14 19:48:43] Epoch 0065 mean train/dev loss: 89.3077 / 162.9695
[2017-12-14 19:48:43] Checkpointing model...
[2017-12-14 19:48:44] Model Checkpointing finished.
[2017-12-14 19:49:45] Epoch 0066 mean train/dev loss: 89.6678 / 159.6964
[2017-12-14 19:50:48] Epoch 0067 mean train/dev loss: 89.5498 / 158.2189
[2017-12-14 19:51:47] Epoch 0068 mean train/dev loss: 89.7797 / 161.8171
[2017-12-14 19:52:52] Epoch 0069 mean train/dev loss: 89.8910 / 170.5466
[2017-12-14 19:53:51] Epoch 0070 mean train/dev loss: 88.1444 / 158.3557
[2017-12-14 19:53:51] Checkpointing model...
[2017-12-14 19:53:51] Model Checkpointing finished.
[2017-12-14 19:54:51] Epoch 0071 mean train/dev loss: 90.6848 / 167.9713
[2017-12-14 19:55:51] Epoch 0072 mean train/dev loss: 91.2297 / 163.3551
[2017-12-14 19:56:52] Epoch 0073 mean train/dev loss: 89.4388 / 157.8034
[2017-12-14 19:57:54] Epoch 0074 mean train/dev loss: 90.0864 / 166.1428
[2017-12-14 19:58:56] Epoch 0075 mean train/dev loss: 88.2755 / 162.1655
[2017-12-14 19:58:56] Learning rate decayed by 0.5000
[2017-12-14 19:58:56] Checkpointing model...
[2017-12-14 19:58:56] Model Checkpointing finished.
[2017-12-14 20:00:01] Epoch 0076 mean train/dev loss: 87.6207 / 161.3174
[2017-12-14 20:01:01] Epoch 0077 mean train/dev loss: 86.8703 / 161.1863
[2017-12-14 20:02:04] Epoch 0078 mean train/dev loss: 87.4798 / 161.3353
[2017-12-14 20:03:09] Epoch 0079 mean train/dev loss: 86.2207 / 160.5842
[2017-12-14 20:04:10] Epoch 0080 mean train/dev loss: 86.0779 / 163.4876
[2017-12-14 20:04:10] Checkpointing model...
[2017-12-14 20:04:10] Model Checkpointing finished.
[2017-12-14 20:05:12] Epoch 0081 mean train/dev loss: 86.7675 / 159.3978
[2017-12-14 20:06:15] Epoch 0082 mean train/dev loss: 87.0906 / 159.9153
[2017-12-14 20:07:16] Epoch 0083 mean train/dev loss: 85.9422 / 164.4662
[2017-12-14 20:08:18] Epoch 0084 mean train/dev loss: 86.4025 / 164.7263
[2017-12-14 20:09:21] Epoch 0085 mean train/dev loss: 85.9963 / 168.3446
[2017-12-14 20:09:21] Checkpointing model...
[2017-12-14 20:09:22] Model Checkpointing finished.
[2017-12-14 20:10:23] Epoch 0086 mean train/dev loss: 87.0262 / 155.6708
[2017-12-14 20:11:24] Epoch 0087 mean train/dev loss: 86.0811 / 158.9969
[2017-12-14 20:12:28] Epoch 0088 mean train/dev loss: 87.1549 / 161.4498
[2017-12-14 20:13:28] Epoch 0089 mean train/dev loss: 86.7411 / 160.2779
[2017-12-14 20:14:30] Epoch 0090 mean train/dev loss: 85.5641 / 168.2052
[2017-12-14 20:14:30] Learning rate decayed by 0.5000
[2017-12-14 20:14:30] Checkpointing model...
[2017-12-14 20:14:30] Model Checkpointing finished.
[2017-12-14 20:15:34] Epoch 0091 mean train/dev loss: 86.3319 / 164.3296
[2017-12-14 20:16:39] Epoch 0092 mean train/dev loss: 84.6479 / 160.6338
[2017-12-14 20:17:42] Epoch 0093 mean train/dev loss: 84.9705 / 159.1479
[2017-12-14 20:18:46] Epoch 0094 mean train/dev loss: 84.3346 / 160.1638
[2017-12-14 20:19:48] Epoch 0095 mean train/dev loss: 84.8741 / 158.7028
[2017-12-14 20:19:48] Checkpointing model...
[2017-12-14 20:19:49] Model Checkpointing finished.
[2017-12-14 20:20:52] Epoch 0096 mean train/dev loss: 84.4490 / 158.6932
[2017-12-14 20:21:56] Epoch 0097 mean train/dev loss: 84.4361 / 159.8681
[2017-12-14 20:23:00] Epoch 0098 mean train/dev loss: 84.7805 / 158.3281
[2017-12-14 20:24:05] Epoch 0099 mean train/dev loss: 84.5714 / 161.4254
[2017-12-14 20:25:06] Epoch 0100 mean train/dev loss: 84.5318 / 160.0581
[2017-12-14 20:25:06] Checkpointing model...
[2017-12-14 20:25:06] Model Checkpointing finished.
[2017-12-14 20:26:11] Epoch 0101 mean train/dev loss: 84.7432 / 156.9378
[2017-12-14 20:27:13] Epoch 0102 mean train/dev loss: 84.2386 / 155.6501
[2017-12-14 20:28:15] Epoch 0103 mean train/dev loss: 84.1173 / 160.1819
[2017-12-14 20:28:15] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-14 20:28:15] 
                       *** Training finished *** 
[2017-12-14 20:28:21] Dev MSE: 160.1819
[2017-12-14 20:29:15] Training MSE: 85.6112
[2017-12-14 20:29:17] Experiment lstm.hs_50.nl_1.lr_0.01.wd_0.001.rl_40_40 logging ended.
