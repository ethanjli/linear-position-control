[2017-12-15 05:26:51] Experiment lstm.hs_50.nl_2.lr_0.01.wd_0.001.rl_40_40 logging started.
[2017-12-15 05:26:51] 
                       *** Starting Experiment lstm.hs_50.nl_2.lr_0.01.wd_0.001.rl_40_40 ***
                      
[2017-12-15 05:26:51] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 50  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 300  
                      [               num_layers] 2  
                      [        regression_layers] [40, 40]  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 05:26:51] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 50, num_layers=2, batch_first=True)
                        (linear1): Linear (50 -> 40)
                        (linear2): Linear (40 -> 40)
                        (final): Linear (40 -> 1)
                      )
[2017-12-15 05:26:51]  *** Training on GPU ***
[2017-12-15 05:27:48] Epoch 0001 mean train/dev loss: 148586.0863 / 59899.6484
[2017-12-15 05:27:48] Checkpointing model...
[2017-12-15 05:27:48] Model Checkpointing finished.
[2017-12-15 05:28:48] Epoch 0002 mean train/dev loss: 51227.9817 / 43051.0742
[2017-12-15 05:28:48] Checkpointing model...
[2017-12-15 05:28:48] Model Checkpointing finished.
[2017-12-15 05:29:46] Epoch 0003 mean train/dev loss: 42867.3161 / 41282.7773
[2017-12-15 05:29:46] Checkpointing model...
[2017-12-15 05:29:46] Model Checkpointing finished.
[2017-12-15 05:30:42] Epoch 0004 mean train/dev loss: 42338.1359 / 41413.9336
[2017-12-15 05:30:42] Checkpointing model...
[2017-12-15 05:30:43] Model Checkpointing finished.
[2017-12-15 05:31:43] Epoch 0005 mean train/dev loss: 39192.3745 / 36393.6523
[2017-12-15 05:31:43] Checkpointing model...
[2017-12-15 05:31:43] Model Checkpointing finished.
[2017-12-15 05:32:43] Epoch 0006 mean train/dev loss: 36804.7838 / 35433.3477
[2017-12-15 05:33:43] Epoch 0007 mean train/dev loss: 35946.0362 / 35055.5391
[2017-12-15 05:34:44] Epoch 0008 mean train/dev loss: 33868.4018 / 28484.1387
[2017-12-15 05:35:41] Epoch 0009 mean train/dev loss: 20699.1369 / 7958.8804
[2017-12-15 05:36:42] Epoch 0010 mean train/dev loss: 3536.2583 / 552.0986
[2017-12-15 05:36:42] Checkpointing model...
[2017-12-15 05:36:43] Model Checkpointing finished.
[2017-12-15 05:37:43] Epoch 0011 mean train/dev loss: 584.8103 / 323.7507
[2017-12-15 05:38:42] Epoch 0012 mean train/dev loss: 479.5104 / 312.7520
[2017-12-15 05:39:39] Epoch 0013 mean train/dev loss: 391.3159 / 287.1045
[2017-12-15 05:40:37] Epoch 0014 mean train/dev loss: 403.4622 / 379.7335
[2017-12-15 05:41:34] Epoch 0015 mean train/dev loss: 293.4751 / 264.4844
[2017-12-15 05:41:34] Learning rate decayed by 0.5000
[2017-12-15 05:41:34] Checkpointing model...
[2017-12-15 05:41:35] Model Checkpointing finished.
[2017-12-15 05:42:32] Epoch 0016 mean train/dev loss: 290.0545 / 231.2803
[2017-12-15 05:43:30] Epoch 0017 mean train/dev loss: 228.6745 / 211.1510
[2017-12-15 05:44:24] Epoch 0018 mean train/dev loss: 206.7484 / 236.1556
[2017-12-15 05:45:21] Epoch 0019 mean train/dev loss: 202.8274 / 258.1671
[2017-12-15 05:46:18] Epoch 0020 mean train/dev loss: 202.7767 / 219.6383
[2017-12-15 05:46:18] Checkpointing model...
[2017-12-15 05:46:19] Model Checkpointing finished.
[2017-12-15 05:47:17] Epoch 0021 mean train/dev loss: 190.6498 / 272.6732
[2017-12-15 05:48:17] Epoch 0022 mean train/dev loss: 185.4809 / 212.2773
[2017-12-15 05:49:15] Epoch 0023 mean train/dev loss: 222.6056 / 268.1591
[2017-12-15 05:50:11] Epoch 0024 mean train/dev loss: 209.2209 / 202.5746
[2017-12-15 05:51:08] Epoch 0025 mean train/dev loss: 169.9510 / 174.4942
[2017-12-15 05:51:08] Checkpointing model...
[2017-12-15 05:51:08] Model Checkpointing finished.
[2017-12-15 05:52:07] Epoch 0026 mean train/dev loss: 163.4249 / 189.9434
[2017-12-15 05:53:06] Epoch 0027 mean train/dev loss: 159.4550 / 257.3376
[2017-12-15 05:54:04] Epoch 0028 mean train/dev loss: 164.6338 / 294.2344
[2017-12-15 05:55:03] Epoch 0029 mean train/dev loss: 160.7866 / 267.0088
[2017-12-15 05:56:00] Epoch 0030 mean train/dev loss: 151.4118 / 170.2139
[2017-12-15 05:56:00] Learning rate decayed by 0.5000
[2017-12-15 05:56:00] Checkpointing model...
[2017-12-15 05:56:00] Model Checkpointing finished.
[2017-12-15 05:56:59] Epoch 0031 mean train/dev loss: 140.8402 / 241.7176
[2017-12-15 05:57:56] Epoch 0032 mean train/dev loss: 132.0510 / 249.2011
[2017-12-15 05:58:54] Epoch 0033 mean train/dev loss: 139.9091 / 264.0413
[2017-12-15 05:59:53] Epoch 0034 mean train/dev loss: 138.7082 / 255.5582
[2017-12-15 06:00:54] Epoch 0035 mean train/dev loss: 133.5479 / 244.2341
[2017-12-15 06:00:54] Checkpointing model...
[2017-12-15 06:00:54] Model Checkpointing finished.
[2017-12-15 06:01:52] Epoch 0036 mean train/dev loss: 134.7958 / 245.5920
[2017-12-15 06:02:50] Epoch 0037 mean train/dev loss: 136.6466 / 264.9300
[2017-12-15 06:03:50] Epoch 0038 mean train/dev loss: 140.1685 / 247.6573
[2017-12-15 06:04:51] Epoch 0039 mean train/dev loss: 133.2412 / 252.0956
[2017-12-15 06:05:50] Epoch 0040 mean train/dev loss: 144.5044 / 246.1111
[2017-12-15 06:05:50] Checkpointing model...
[2017-12-15 06:05:50] Model Checkpointing finished.
[2017-12-15 06:06:54] Epoch 0041 mean train/dev loss: 139.7099 / 282.0479
[2017-12-15 06:07:57] Epoch 0042 mean train/dev loss: 134.4826 / 252.0815
[2017-12-15 06:08:59] Epoch 0043 mean train/dev loss: 131.0193 / 247.9630
[2017-12-15 06:10:02] Epoch 0044 mean train/dev loss: 128.4559 / 240.6640
[2017-12-15 06:11:05] Epoch 0045 mean train/dev loss: 133.3588 / 244.5574
[2017-12-15 06:11:05] Learning rate decayed by 0.5000
[2017-12-15 06:11:05] Checkpointing model...
[2017-12-15 06:11:05] Model Checkpointing finished.
[2017-12-15 06:12:06] Epoch 0046 mean train/dev loss: 129.4774 / 258.6127
[2017-12-15 06:13:09] Epoch 0047 mean train/dev loss: 125.0206 / 237.1595
[2017-12-15 06:14:11] Epoch 0048 mean train/dev loss: 119.7086 / 238.6793
[2017-12-15 06:15:15] Epoch 0049 mean train/dev loss: 119.3159 / 244.3211
[2017-12-15 06:16:16] Epoch 0050 mean train/dev loss: 119.5774 / 246.4627
[2017-12-15 06:16:16] Checkpointing model...
[2017-12-15 06:16:16] Model Checkpointing finished.
[2017-12-15 06:17:20] Epoch 0051 mean train/dev loss: 116.7383 / 232.3585
[2017-12-15 06:18:22] Epoch 0052 mean train/dev loss: 117.1029 / 233.9705
[2017-12-15 06:19:24] Epoch 0053 mean train/dev loss: 115.6880 / 229.7888
[2017-12-15 06:20:27] Epoch 0054 mean train/dev loss: 115.0913 / 232.8794
[2017-12-15 06:21:29] Epoch 0055 mean train/dev loss: 116.3145 / 221.3567
[2017-12-15 06:21:29] Checkpointing model...
[2017-12-15 06:21:30] Model Checkpointing finished.
[2017-12-15 06:22:33] Epoch 0056 mean train/dev loss: 116.8118 / 223.5327
[2017-12-15 06:23:37] Epoch 0057 mean train/dev loss: 115.0662 / 233.6468
[2017-12-15 06:24:39] Epoch 0058 mean train/dev loss: 113.9376 / 216.5118
[2017-12-15 06:25:42] Epoch 0059 mean train/dev loss: 120.1402 / 232.7154
[2017-12-15 06:26:44] Epoch 0060 mean train/dev loss: 118.0426 / 268.1241
[2017-12-15 06:26:44] Learning rate decayed by 0.5000
[2017-12-15 06:26:44] Checkpointing model...
[2017-12-15 06:26:44] Model Checkpointing finished.
[2017-12-15 06:27:45] Epoch 0061 mean train/dev loss: 118.1117 / 228.5908
[2017-12-15 06:28:48] Epoch 0062 mean train/dev loss: 108.8254 / 227.4516
[2017-12-15 06:29:53] Epoch 0063 mean train/dev loss: 106.9662 / 226.4001
[2017-12-15 06:30:54] Epoch 0064 mean train/dev loss: 108.9531 / 224.5382
[2017-12-15 06:31:56] Epoch 0065 mean train/dev loss: 106.6899 / 227.0438
[2017-12-15 06:31:56] Checkpointing model...
[2017-12-15 06:31:56] Model Checkpointing finished.
[2017-12-15 06:32:57] Epoch 0066 mean train/dev loss: 105.5318 / 235.6184
[2017-12-15 06:34:00] Epoch 0067 mean train/dev loss: 107.0330 / 219.0533
[2017-12-15 06:35:03] Epoch 0068 mean train/dev loss: 106.1730 / 228.0618
[2017-12-15 06:36:02] Epoch 0069 mean train/dev loss: 105.2451 / 220.1150
[2017-12-15 06:37:02] Epoch 0070 mean train/dev loss: 110.3498 / 225.7571
[2017-12-15 06:37:02] Checkpointing model...
[2017-12-15 06:37:02] Model Checkpointing finished.
[2017-12-15 06:38:04] Epoch 0071 mean train/dev loss: 108.8595 / 220.3204
[2017-12-15 06:38:04] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 06:38:04] 
                       *** Training finished *** 
[2017-12-15 06:38:10] Dev MSE: 220.3204
[2017-12-15 06:38:58] Training MSE: 101.1827
[2017-12-15 06:39:00] Experiment lstm.hs_50.nl_2.lr_0.01.wd_0.001.rl_40_40 logging ended.
