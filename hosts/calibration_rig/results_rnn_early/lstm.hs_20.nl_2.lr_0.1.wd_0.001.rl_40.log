[2017-12-14 05:10:13] Experiment lstm.hs_20.nl_2.lr_0.1.wd_0.001.rl_40 logging started.
[2017-12-14 05:10:13] 
                       *** Starting Experiment lstm.hs_20.nl_2.lr_0.1.wd_0.001.rl_40 ***
                      
[2017-12-14 05:10:13] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 20  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.1  
                      [               num_epochs] 300  
                      [               num_layers] 2  
                      [        regression_layers] [40]  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-14 05:10:13] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 20, num_layers=2, batch_first=True)
                        (linear1): Linear (20 -> 40)
                        (final): Linear (40 -> 1)
                      )
[2017-12-14 05:10:13]  *** Training on GPU ***
[2017-12-14 05:11:25] Epoch 0001 mean train/dev loss: 92068.5328 / 28382.0312
[2017-12-14 05:11:25] Checkpointing model...
[2017-12-14 05:11:25] Model Checkpointing finished.
[2017-12-14 05:12:37] Epoch 0002 mean train/dev loss: 7788.4717 / 3093.1572
[2017-12-14 05:12:37] Checkpointing model...
[2017-12-14 05:12:38] Model Checkpointing finished.
[2017-12-14 05:13:49] Epoch 0003 mean train/dev loss: 1851.4413 / 1291.3453
[2017-12-14 05:13:49] Checkpointing model...
[2017-12-14 05:13:49] Model Checkpointing finished.
[2017-12-14 05:15:01] Epoch 0004 mean train/dev loss: 1006.5529 / 960.7833
[2017-12-14 05:15:01] Checkpointing model...
[2017-12-14 05:15:02] Model Checkpointing finished.
[2017-12-14 05:16:14] Epoch 0005 mean train/dev loss: 1314.7656 / 939.0525
[2017-12-14 05:16:14] Checkpointing model...
[2017-12-14 05:16:14] Model Checkpointing finished.
[2017-12-14 05:17:26] Epoch 0006 mean train/dev loss: 892.6651 / 926.1294
[2017-12-14 05:18:37] Epoch 0007 mean train/dev loss: 717.3688 / 681.0709
[2017-12-14 05:19:49] Epoch 0008 mean train/dev loss: 672.3784 / 1167.1115
[2017-12-14 05:21:01] Epoch 0009 mean train/dev loss: 879.3881 / 2897.0344
[2017-12-14 05:22:12] Epoch 0010 mean train/dev loss: 1938.8444 / 2243.2202
[2017-12-14 05:22:12] Checkpointing model...
[2017-12-14 05:22:13] Model Checkpointing finished.
[2017-12-14 05:23:24] Epoch 0011 mean train/dev loss: 1697.1243 / 1745.1719
[2017-12-14 05:24:36] Epoch 0012 mean train/dev loss: 1139.5953 / 1144.4563
[2017-12-14 05:25:47] Epoch 0013 mean train/dev loss: 776.1104 / 859.2357
[2017-12-14 05:26:59] Epoch 0014 mean train/dev loss: 1049.1540 / 1148.6479
[2017-12-14 05:28:10] Epoch 0015 mean train/dev loss: 6875.9100 / 5909.2544
[2017-12-14 05:28:10] Learning rate decayed by 0.5000
[2017-12-14 05:28:10] Checkpointing model...
[2017-12-14 05:28:11] Model Checkpointing finished.
[2017-12-14 05:29:22] Epoch 0016 mean train/dev loss: 11791.2323 / 11253.9219
[2017-12-14 05:30:34] Epoch 0017 mean train/dev loss: 7512.5692 / 4992.7598
[2017-12-14 05:31:46] Epoch 0018 mean train/dev loss: 4296.5896 / 4715.5972
[2017-12-14 05:32:57] Epoch 0019 mean train/dev loss: 4178.7821 / 4169.1963
[2017-12-14 05:34:08] Epoch 0020 mean train/dev loss: 2972.2693 / 2864.6501
[2017-12-14 05:34:08] Checkpointing model...
[2017-12-14 05:34:09] Model Checkpointing finished.
[2017-12-14 05:35:20] Epoch 0021 mean train/dev loss: 1981.4447 / 1940.9335
[2017-12-14 05:36:31] Epoch 0022 mean train/dev loss: 1659.5560 / 1620.0187
[2017-12-14 05:37:43] Epoch 0023 mean train/dev loss: 3753.5960 / 3353.7593
[2017-12-14 05:38:54] Epoch 0024 mean train/dev loss: 2318.6245 / 1794.5028
[2017-12-14 05:40:05] Epoch 0025 mean train/dev loss: 1457.8519 / 1497.8383
[2017-12-14 05:40:05] Checkpointing model...
[2017-12-14 05:40:06] Model Checkpointing finished.
[2017-12-14 05:41:18] Epoch 0026 mean train/dev loss: 1337.2507 / 1350.3173
[2017-12-14 05:42:30] Epoch 0027 mean train/dev loss: 1191.8288 / 1383.7227
[2017-12-14 05:43:41] Epoch 0028 mean train/dev loss: 1188.5814 / 1309.9154
[2017-12-14 05:44:53] Epoch 0029 mean train/dev loss: 1158.0599 / 1170.2271
[2017-12-14 05:46:04] Epoch 0030 mean train/dev loss: 982.7871 / 1094.5698
[2017-12-14 05:46:04] Learning rate decayed by 0.5000
[2017-12-14 05:46:04] Checkpointing model...
[2017-12-14 05:46:05] Model Checkpointing finished.
[2017-12-14 05:47:16] Epoch 0031 mean train/dev loss: 847.2931 / 988.0963
[2017-12-14 05:48:28] Epoch 0032 mean train/dev loss: 747.3213 / 945.3126
[2017-12-14 05:49:39] Epoch 0033 mean train/dev loss: 745.7119 / 1024.5007
[2017-12-14 05:50:51] Epoch 0034 mean train/dev loss: 709.9890 / 898.0974
[2017-12-14 05:52:03] Epoch 0035 mean train/dev loss: 663.5073 / 817.7341
[2017-12-14 05:52:03] Checkpointing model...
[2017-12-14 05:52:03] Model Checkpointing finished.
[2017-12-14 05:53:15] Epoch 0036 mean train/dev loss: 750.9359 / 804.1498
[2017-12-14 05:54:27] Epoch 0037 mean train/dev loss: 699.7511 / 916.4224
[2017-12-14 05:55:38] Epoch 0038 mean train/dev loss: 626.2227 / 875.7465
[2017-12-14 05:56:49] Epoch 0039 mean train/dev loss: 640.0328 / 818.3861
[2017-12-14 05:58:01] Epoch 0040 mean train/dev loss: 562.2521 / 837.0341
[2017-12-14 05:58:01] Checkpointing model...
[2017-12-14 05:58:02] Model Checkpointing finished.
[2017-12-14 05:59:13] Epoch 0041 mean train/dev loss: 581.2844 / 821.6783
[2017-12-14 06:00:25] Epoch 0042 mean train/dev loss: 535.2427 / 769.2591
[2017-12-14 06:01:37] Epoch 0043 mean train/dev loss: 505.6272 / 793.7560
[2017-12-14 06:02:49] Epoch 0044 mean train/dev loss: 501.2916 / 767.0797
[2017-12-14 06:04:00] Epoch 0045 mean train/dev loss: 533.3450 / 820.4659
[2017-12-14 06:04:00] Learning rate decayed by 0.5000
[2017-12-14 06:04:00] Checkpointing model...
[2017-12-14 06:04:01] Model Checkpointing finished.
[2017-12-14 06:05:12] Epoch 0046 mean train/dev loss: 489.0924 / 775.3611
[2017-12-14 06:06:23] Epoch 0047 mean train/dev loss: 475.1175 / 772.1191
[2017-12-14 06:07:34] Epoch 0048 mean train/dev loss: 456.4051 / 772.3598
[2017-12-14 06:07:34] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-14 06:07:34] 
                       *** Training finished *** 
[2017-12-14 06:07:40] Dev MSE: 772.3598
[2017-12-14 06:08:41] Training MSE: 451.3600
[2017-12-14 06:08:43] Experiment lstm.hs_20.nl_2.lr_0.1.wd_0.001.rl_40 logging ended.
