[2017-12-15 07:56:38] Experiment lstm.hs_100.nl_2.lr_0.01.wd_0.001.rl_40_40 logging started.
[2017-12-15 07:56:38] 
                       *** Starting Experiment lstm.hs_100.nl_2.lr_0.01.wd_0.001.rl_40_40 ***
                      
[2017-12-15 07:56:38] Hyper parameters
                      [               batch_size] 64  
                      [           dataset_prefix] 20171209.1220  
                      [                 dump_dir] results_rnn_early  
                      [               early_stop] 40  
                      [              hidden_size] 100  
                      [                input_dim] 12  
                      [                  loss_fn] MSELoss ()  
                      [                 lr_decay] 0.5  
                      [            lr_decay_freq] 15  
                      [                  lr_init] 0.01  
                      [               num_epochs] 300  
                      [               num_layers] 2  
                      [        regression_layers] [40, 40]  
                      [                 use_cuda] True  
                      [             weight_decay] 0.001  
[2017-12-15 07:56:40] Model architecture
                      SequentialRegression (
                        (lstm): LSTM(12, 100, num_layers=2, batch_first=True)
                        (linear1): Linear (100 -> 40)
                        (linear2): Linear (40 -> 40)
                        (final): Linear (40 -> 1)
                      )
[2017-12-15 07:56:40]  *** Training on GPU ***
[2017-12-15 07:57:19] Epoch 0001 mean train/dev loss: 146479.9254 / 59369.9258
[2017-12-15 07:57:19] Checkpointing model...
[2017-12-15 07:57:20] Model Checkpointing finished.
[2017-12-15 07:57:59] Epoch 0002 mean train/dev loss: 30476.5833 / 4093.4456
[2017-12-15 07:57:59] Checkpointing model...
[2017-12-15 07:57:59] Model Checkpointing finished.
[2017-12-15 07:58:39] Epoch 0003 mean train/dev loss: 1509.5514 / 1020.7216
[2017-12-15 07:58:39] Checkpointing model...
[2017-12-15 07:58:39] Model Checkpointing finished.
[2017-12-15 07:59:19] Epoch 0004 mean train/dev loss: 630.8196 / 847.6888
[2017-12-15 07:59:19] Checkpointing model...
[2017-12-15 07:59:19] Model Checkpointing finished.
[2017-12-15 07:59:59] Epoch 0005 mean train/dev loss: 529.5760 / 991.8801
[2017-12-15 07:59:59] Checkpointing model...
[2017-12-15 07:59:59] Model Checkpointing finished.
[2017-12-15 08:00:39] Epoch 0006 mean train/dev loss: 555.6793 / 846.4281
[2017-12-15 08:01:19] Epoch 0007 mean train/dev loss: 424.8915 / 846.8354
[2017-12-15 08:01:59] Epoch 0008 mean train/dev loss: 508.7114 / 725.2802
[2017-12-15 08:02:39] Epoch 0009 mean train/dev loss: 495.6151 / 668.7227
[2017-12-15 08:03:18] Epoch 0010 mean train/dev loss: 268.3614 / 434.8578
[2017-12-15 08:03:18] Checkpointing model...
[2017-12-15 08:03:19] Model Checkpointing finished.
[2017-12-15 08:03:58] Epoch 0011 mean train/dev loss: 257.3481 / 375.5019
[2017-12-15 08:04:38] Epoch 0012 mean train/dev loss: 202.3901 / 277.7668
[2017-12-15 08:05:18] Epoch 0013 mean train/dev loss: 247.1705 / 498.7394
[2017-12-15 08:05:57] Epoch 0014 mean train/dev loss: 258.7044 / 628.4155
[2017-12-15 08:06:37] Epoch 0015 mean train/dev loss: 411.2523 / 371.0471
[2017-12-15 08:06:37] Learning rate decayed by 0.5000
[2017-12-15 08:06:37] Checkpointing model...
[2017-12-15 08:06:37] Model Checkpointing finished.
[2017-12-15 08:07:17] Epoch 0016 mean train/dev loss: 183.5007 / 380.8902
[2017-12-15 08:07:57] Epoch 0017 mean train/dev loss: 134.1546 / 287.3080
[2017-12-15 08:08:36] Epoch 0018 mean train/dev loss: 135.3889 / 304.5616
[2017-12-15 08:09:16] Epoch 0019 mean train/dev loss: 123.6815 / 187.7267
[2017-12-15 08:09:55] Epoch 0020 mean train/dev loss: 111.5093 / 206.6096
[2017-12-15 08:09:55] Checkpointing model...
[2017-12-15 08:09:55] Model Checkpointing finished.
[2017-12-15 08:10:35] Epoch 0021 mean train/dev loss: 115.6923 / 161.5484
[2017-12-15 08:11:15] Epoch 0022 mean train/dev loss: 98.6423 / 184.9859
[2017-12-15 08:11:54] Epoch 0023 mean train/dev loss: 108.0557 / 162.6478
[2017-12-15 08:12:34] Epoch 0024 mean train/dev loss: 102.6145 / 159.3225
[2017-12-15 08:13:13] Epoch 0025 mean train/dev loss: 108.8385 / 177.8610
[2017-12-15 08:13:13] Checkpointing model...
[2017-12-15 08:13:13] Model Checkpointing finished.
[2017-12-15 08:13:53] Epoch 0026 mean train/dev loss: 98.0032 / 159.0266
[2017-12-15 08:14:33] Epoch 0027 mean train/dev loss: 99.4716 / 156.8674
[2017-12-15 08:15:13] Epoch 0028 mean train/dev loss: 116.9607 / 146.0842
[2017-12-15 08:15:52] Epoch 0029 mean train/dev loss: 103.2850 / 99.7265
[2017-12-15 08:16:32] Epoch 0030 mean train/dev loss: 92.1410 / 256.9968
[2017-12-15 08:16:32] Learning rate decayed by 0.5000
[2017-12-15 08:16:32] Checkpointing model...
[2017-12-15 08:16:32] Model Checkpointing finished.
[2017-12-15 08:17:12] Epoch 0031 mean train/dev loss: 90.2519 / 148.2764
[2017-12-15 08:17:52] Epoch 0032 mean train/dev loss: 85.5359 / 156.3746
[2017-12-15 08:18:31] Epoch 0033 mean train/dev loss: 80.9577 / 147.8086
[2017-12-15 08:19:11] Epoch 0034 mean train/dev loss: 81.5704 / 161.4801
[2017-12-15 08:19:51] Epoch 0035 mean train/dev loss: 87.1865 / 152.1461
[2017-12-15 08:19:51] Checkpointing model...
[2017-12-15 08:19:51] Model Checkpointing finished.
[2017-12-15 08:20:31] Epoch 0036 mean train/dev loss: 110.6062 / 178.1893
[2017-12-15 08:21:11] Epoch 0037 mean train/dev loss: 85.1223 / 146.4789
[2017-12-15 08:21:51] Epoch 0038 mean train/dev loss: 79.3337 / 145.7446
[2017-12-15 08:22:30] Epoch 0039 mean train/dev loss: 79.9029 / 165.9258
[2017-12-15 08:23:10] Epoch 0040 mean train/dev loss: 80.6291 / 139.3361
[2017-12-15 08:23:10] Checkpointing model...
[2017-12-15 08:23:10] Model Checkpointing finished.
[2017-12-15 08:23:50] Epoch 0041 mean train/dev loss: 90.3614 / 147.3643
[2017-12-15 08:24:29] Epoch 0042 mean train/dev loss: 80.8811 / 124.5001
[2017-12-15 08:25:09] Epoch 0043 mean train/dev loss: 78.7238 / 131.9386
[2017-12-15 08:25:49] Epoch 0044 mean train/dev loss: 83.6424 / 168.1881
[2017-12-15 08:26:29] Epoch 0045 mean train/dev loss: 76.0798 / 142.4983
[2017-12-15 08:26:29] Learning rate decayed by 0.5000
[2017-12-15 08:26:29] Checkpointing model...
[2017-12-15 08:26:29] Model Checkpointing finished.
[2017-12-15 08:27:08] Epoch 0046 mean train/dev loss: 69.3828 / 153.4295
[2017-12-15 08:27:48] Epoch 0047 mean train/dev loss: 69.7356 / 172.5687
[2017-12-15 08:28:27] Epoch 0048 mean train/dev loss: 69.9040 / 139.8330
[2017-12-15 08:29:07] Epoch 0049 mean train/dev loss: 70.8551 / 140.5571
[2017-12-15 08:29:46] Epoch 0050 mean train/dev loss: 67.9440 / 145.6598
[2017-12-15 08:29:46] Checkpointing model...
[2017-12-15 08:29:46] Model Checkpointing finished.
[2017-12-15 08:30:26] Epoch 0051 mean train/dev loss: 66.8039 / 146.0120
[2017-12-15 08:31:05] Epoch 0052 mean train/dev loss: 66.8331 / 171.7552
[2017-12-15 08:31:45] Epoch 0053 mean train/dev loss: 69.4767 / 155.5547
[2017-12-15 08:32:24] Epoch 0054 mean train/dev loss: 67.3363 / 143.6004
[2017-12-15 08:33:03] Epoch 0055 mean train/dev loss: 65.1339 / 143.7481
[2017-12-15 08:33:03] Checkpointing model...
[2017-12-15 08:33:03] Model Checkpointing finished.
[2017-12-15 08:33:43] Epoch 0056 mean train/dev loss: 66.8769 / 154.9405
[2017-12-15 08:34:22] Epoch 0057 mean train/dev loss: 65.3073 / 174.2261
[2017-12-15 08:35:02] Epoch 0058 mean train/dev loss: 65.3136 / 147.9837
[2017-12-15 08:35:42] Epoch 0059 mean train/dev loss: 66.7055 / 144.8129
[2017-12-15 08:36:21] Epoch 0060 mean train/dev loss: 64.2847 / 146.3827
[2017-12-15 08:36:21] Learning rate decayed by 0.5000
[2017-12-15 08:36:21] Checkpointing model...
[2017-12-15 08:36:22] Model Checkpointing finished.
[2017-12-15 08:37:01] Epoch 0061 mean train/dev loss: 61.5158 / 150.3566
[2017-12-15 08:37:40] Epoch 0062 mean train/dev loss: 59.5454 / 142.1869
[2017-12-15 08:38:20] Epoch 0063 mean train/dev loss: 60.0240 / 139.4514
[2017-12-15 08:38:59] Epoch 0064 mean train/dev loss: 59.2621 / 140.7735
[2017-12-15 08:39:39] Epoch 0065 mean train/dev loss: 59.6866 / 140.9861
[2017-12-15 08:39:39] Checkpointing model...
[2017-12-15 08:39:39] Model Checkpointing finished.
[2017-12-15 08:40:19] Epoch 0066 mean train/dev loss: 58.4396 / 138.2914
[2017-12-15 08:40:58] Epoch 0067 mean train/dev loss: 58.9976 / 146.2230
[2017-12-15 08:41:38] Epoch 0068 mean train/dev loss: 60.9354 / 137.7889
[2017-12-15 08:42:17] Epoch 0069 mean train/dev loss: 60.7567 / 138.0982
[2017-12-15 08:42:57] Epoch 0070 mean train/dev loss: 60.4035 / 138.4624
[2017-12-15 08:42:57] Early stopping training because validation loss did not improve for 40 epochs!
[2017-12-15 08:42:57] 
                       *** Training finished *** 
[2017-12-15 08:43:00] Dev MSE: 138.4624
[2017-12-15 08:43:32] Training MSE: 56.3679
[2017-12-15 08:43:34] Experiment lstm.hs_100.nl_2.lr_0.01.wd_0.001.rl_40_40 logging ended.
